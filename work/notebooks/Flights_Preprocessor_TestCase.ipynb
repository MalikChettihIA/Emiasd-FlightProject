{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c94428-dbc0-4a56-a9c2-a79ea02d6a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of Emiasd-Flight-Data-Analysis.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of Emiasd-Flight-Data-Analysis.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e5025a-6a95-41ac-a937-a9e04f240204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "--> [FlightDataLoader] Flight Data Loading - Start ...\n",
      "--> 486133 loaded ...\n",
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- ARR_DELAY_NEW: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      "\n",
      "+----------+---------------------+-----------------+-----------------+---------------+------------+-------------+---------+--------+----------------+-------------+---------+\n",
      "|   FL_DATE|OP_CARRIER_AIRLINE_ID|OP_CARRIER_FL_NUM|ORIGIN_AIRPORT_ID|DEST_AIRPORT_ID|CRS_DEP_TIME|ARR_DELAY_NEW|CANCELLED|DIVERTED|CRS_ELAPSED_TIME|WEATHER_DELAY|NAS_DELAY|\n",
      "+----------+---------------------+-----------------+-----------------+---------------+------------+-------------+---------+--------+----------------+-------------+---------+\n",
      "|2012-01-01|                20366|             4426|            15370|          12266|         845|          0.0|      0.0|     0.0|            99.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4427|            12266|          15370|         858|          0.0|      0.0|     0.0|            88.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4427|            15370|          12266|        1051|          0.0|      0.0|     0.0|            89.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4428|            12266|          15370|        1125|         31.0|      0.0|     0.0|            87.0|          0.0|      2.0|\n",
      "|2012-01-01|                20366|             4428|            15370|          12266|        1319|         21.0|      0.0|     0.0|            96.0|          0.0|      0.0|\n",
      "|2012-01-01|                20366|             4429|            12266|          15370|        1328|          0.0|      0.0|     0.0|            94.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4429|            15370|          12266|        1527|          0.0|      0.0|     0.0|            97.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4430|            12266|          15370|        1449|         10.0|      0.0|     0.0|            89.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4430|            15370|          12266|        1643|          0.0|      0.0|     0.0|            98.0|         NULL|     NULL|\n",
      "|2012-01-01|                20366|             4431|            12266|          15370|        1920|         55.0|      0.0|     0.0|            91.0|          0.0|      0.0|\n",
      "+----------+---------------------+-----------------+-----------------+---------------+------------+-------------+---------+--------+----------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "--> [FlightDataLoader] Flight Data Loading - End ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "args = Array(local)\n",
       "spark = org.apache.spark.sql.SparkSession@276fa978\n",
       "session = org.apache.spark.sql.SparkSession@276fa978\n",
       "configuration = AppConfiguration(local,DataConfig(/data,FileConfig(/data/FLIGHT-3Y/Flights/201201.csv),FileConfig(/data/FLIGHT-3Y/Weather/201201hourly.txt),FileConfig(/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/output,FileConfig(/output/data),FileConfig(/output/model)))\n",
       "flightFilePath = ../data/FLIGHT-3Y/Flights/201201.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "flightData: org.apache.spark.sql.Dat...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "../data/FLIGHT-3Y/Flights/201201.csv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import com.flightdelay.config.{AppConfiguration, ConfigurationLoader}\n",
    "import com.flightdelay.data.loaders.FlightDataLoader\n",
    "\n",
    "//Env Configuration\n",
    "val args: Array[String] = Array(\"local\")\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(sc.getConf)\n",
    "  .getOrCreate()\n",
    "\n",
    "// Rendre la session Spark implicite\n",
    "implicit val session = spark\n",
    "implicit val configuration: AppConfiguration = ConfigurationLoader.loadConfiguration(args)\n",
    "\n",
    "// Cellule 4: Test\n",
    "val flightFilePath = \"../data/FLIGHT-3Y/Flights/201201.csv\"\n",
    "val flightData = FlightDataLoader.loadFromFilePath(flightFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50bf716-a979-4597-b1f0-2103d27473d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "--> [FlightPreprocessingPipeline] Flight Preprocessing Pipeline - Start ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "--> [FlightDataCleaner] Flight Data Cleaner - Start ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "Data Original Count: 486133\n",
      "\n",
      "Phase 1: Basic Cleaning - Remove Duplicates\n",
      "Current Count : 486133\n",
      "\n",
      "Phase 2: Filter Flights\n",
      "- Filter Cancelled and Diverted Flights\n",
      "Suppression des valeurs spécifiques: Map(CANCELLED -> List(1.0), DIVERTED -> List(1.0))\n",
      "Nombre de lignes avant: 486133\n",
      "Nombre de lignes après suppression des valeurs spécifiques: 478025\n",
      "- Filter Invalid departure time\n",
      "- Filter Invalid airports\n",
      "Current Count : 478025\n",
      "\n",
      "Phase 3: Types Conversion\n",
      "Conversion des types de données: NAS_DELAY, OP_CARRIER_AIRLINE_ID, CANCELLED, OP_CARRIER_FL_NUM, WEATHER_DELAY, DEST_AIRPORT_ID, ORIGIN_AIRPORT_ID, CRS_ELAPSED_TIME, DIVERTED, FL_DATE, CRS_DEP_TIME, ARR_DELAY_NEW\n",
      "- Filter Invalid flight date formats\n",
      "Current Count : 478025\n",
      "\n",
      "Phase 4: Filter Outliers\n",
      "- Filter delay > 10 hours = 600 minutes\n",
      "- Filter filght time > entre 10 minutes et 24 hours\n",
      "Current Count : 477960\n",
      "\n",
      "Phase 5: Final Validation\n",
      "Final Validation succeeded: 477960 flights\n",
      "\n",
      "=== Flight Cleaning Summary ===\n",
      "Original Count: 486133\n",
      "Final Count: 477960\n",
      "Cleaned: 8173\n",
      "Reduction Percentage: 2%\n",
      "\n",
      "--> [FlightDataCleaner] Flight Data Cleaner- End ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "--> [FlightDataGenerator] Flight Data Generator - Start ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "Original Column Counts: 12\n",
      "\n",
      "Phase 1: Add Temporal Features\n",
      "- Add flight_timestamp\n",
      "- Add flight_year\n",
      "- Add flight_month\n",
      "- Add flight_quarter\n",
      "- Add flight_day_of_month\n",
      "- Add flight_day_of_week\n",
      "- Add flight_day_of_year\n",
      "- Add flight_week_of_year\n",
      "- Add departure_hour\n",
      "- Add departure_minute\n",
      "- Add departure_hour_decimal\n",
      "- Add departure_quarter_day\n",
      "- Add departure_quarter_name\n",
      "- Add departure_time_period\n",
      "- Add minutes_since_midnight\n",
      "Temporal features added: 16\n",
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- ARR_DELAY_NEW: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- departure_minute: integer (nullable = true)\n",
      " |-- flight_quarter_name: string (nullable = false)\n",
      " |-- flight_year: integer (nullable = true)\n",
      " |-- flight_week_of_year: integer (nullable = true)\n",
      " |-- flight_quarter: integer (nullable = false)\n",
      " |-- minutes_since_midnight: double (nullable = true)\n",
      " |-- flight_day_of_year: integer (nullable = true)\n",
      " |-- flight_day_of_week: integer (nullable = true)\n",
      " |-- flight_month: integer (nullable = true)\n",
      " |-- departure_time_period: string (nullable = false)\n",
      " |-- departure_hour: integer (nullable = true)\n",
      " |-- flight_timestamp: timestamp (nullable = true)\n",
      " |-- departure_quarter_day: integer (nullable = false)\n",
      " |-- departure_hour_decimal: double (nullable = true)\n",
      " |-- departure_quarter_name: string (nullable = false)\n",
      " |-- flight_day_of_month: integer (nullable = true)\n",
      "\n",
      "\n",
      "Phase 2: Add Flight Characteristics\n",
      "- Add flight_unique_id \n",
      "- Add distance_category (short, medium, long, very_long) \n",
      "- Add distance_score \n",
      "- Add is_likely_domestic \n",
      "- Add carrier_hash \n",
      "- Add route_id \n",
      "- Add is_roundtrip_candidate \n",
      "Added Flight features: 7\n",
      "\n",
      "Phase 3: Add Period <indicator\n",
      "- Add is_weekend, is_friday, is_monday\n",
      "- Add is_summer, is_winter, is_spring, is_fall \n",
      "- Add is_holiday_season (approximative)\n",
      "- Add is_early_morning \n",
      "- Add is_morning_rush \n",
      "- Add is_business_hours \n",
      "- Add is_evening_rush \n",
      "- Add is_night_flight \n",
      "- Add is_month_start \n",
      "- Add is_month_end \n",
      "- Add is_extended_weekend \n",
      "Added Flight features: 16\n",
      "\n",
      "Phase 4: Add Geographical Features\n",
      "- Add origin_is_major_hub (10397, 11298, 12266, 13930, 14107, 14771, 15016  // Principaux hubs US)\n",
      "- Add dest_is_major_hub  (10397, 11298, 12266, 13930, 14107, 14771, 15016  // Principaux hubs US)\n",
      "- Add is_hub_to_hub\n",
      "- Add flight_quarter\n",
      "- Add origin_complexity_score\n",
      "- Add dest_complexity_score\n",
      "- Add timezone_diff_proxy\n",
      "- Add flight_week_of_year\n",
      "- Add is_eastbound\n",
      "- Add is_westbound\n",
      "Added Flight features: 8\n",
      "\n",
      "Phase 5 : Add Aggregated Features\n",
      "- Add flights_on_route\n",
      "- Add carrier_flight_count\n",
      "- Add origin_airport_traffic\n",
      "- Add route_popularity_score\n",
      "- Add carrier_size_category\n",
      "Added Flight features: 5\n",
      "\n",
      "Phase 6 : Features Normalization\n",
      "- Normalize CRS_ELAPSED_TIME\n",
      "- Normalize departure_hour_decimal\n",
      "- Normalize distance_score\n",
      "- Normalize origin_complexity_score\n",
      "- Normalize dest_complexity_score\n",
      "Normalized Columns: 5\n",
      "\n",
      "\n",
      "=== Enrichment Summary ===\n",
      "Original Columns: 12\n",
      "Columns after enrichment: 69\n",
      "Enriched Columns: 57\n",
      "Dataset size: 477960\n",
      "\n",
      "New Features Created : \n",
      "departure_minute,\n",
      "flight_quarter_name,\n",
      "flight_year,\n",
      "flight_week_of_year,\n",
      "flight_quarter,\n",
      "minutes_since_midnight,\n",
      "flight_day_of_year,\n",
      "flight_day_of_week,\n",
      "flight_month,\n",
      "departure_time_period,\n",
      "departure_hour,\n",
      "flight_timestamp,\n",
      "departure_quarter_day,\n",
      "departure_hour_decimal,\n",
      "departure_quarter_name,\n",
      "flight_day_of_month,\n",
      "carrier_hash,\n",
      "is_likely_domestic,\n",
      "route_id,\n",
      "is_roundtrip_candidate,\n",
      "flight_unique_id,\n",
      "distance_score,\n",
      "distance_category,\n",
      "is_early_morning,\n",
      "is_summer,\n",
      "is_evening_rush,\n",
      "is_monday,\n",
      "is_spring,\n",
      "is_fall,\n",
      "is_extended_weekend,\n",
      "is_winter,\n",
      "is_month_end,\n",
      "is_month_start,\n",
      "is_business_hours,\n",
      "is_night_flight,\n",
      "is_holiday_season,\n",
      "is_friday,\n",
      "is_weekend,\n",
      "is_morning_rush,\n",
      "origin_complexity_score,\n",
      "origin_is_major_hub,\n",
      "dest_is_major_hub,\n",
      "is_westbound,\n",
      "timezone_diff_proxy,\n",
      "is_eastbound,\n",
      "dest_complexity_score,\n",
      "is_hub_to_hub,\n",
      "flights_on_route,\n",
      "carrier_flight_count,\n",
      "origin_airport_traffic,\n",
      "route_popularity_score,\n",
      "carrier_size_category,\n",
      "CRS_ELAPSED_TIME_normalized,\n",
      "departure_hour_decimal_normalized,\n",
      "distance_score_normalized,\n",
      "origin_complexity_score_normalized,\n",
      "dest_complexity_score_normalized\n",
      "\n",
      "--> [FlightDataGenerator] Flight Data Generator- End ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "--> [FlightLabelGenerator] Flight Label Generator - Start ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Phase 1: Validate Required Columns\n",
      "- Validation of required columns: OK\n",
      "\n",
      "Phase 2: Handling missing values for delays\n",
      "\n",
      "Phase 2: Adding basic labels for different thresholds\n",
      "- Add is_delayed_15min\n",
      "- Add is_delayed_30min\n",
      "- Add is_delayed_45min\n",
      "- Add is_delayed_60min\n",
      "- Add is_delayed_90min\n",
      "- Add has_weather_delay\n",
      "- Add has_nas_delay\n",
      "- Add has_any_weather_nas_delay\n",
      "- Add total_weather_nas_delay\n",
      "- Add is_on_time\n",
      "- Add is_early\n",
      "\n",
      "Phase 3: Adding labels according to TIST strategies (D1, D2, D3, D4)\n",
      "- Add label_d1_15min\n",
      "- Add label_d1_60min\n",
      "- Add label_d2_15min\n",
      "- Add label_d2_60min\n",
      "- Add label_d3_15min\n",
      "- Add label_d3_60min\n",
      "- Add label_d4_15min\n",
      "- Add label_d4_60min\n",
      "- Add label_d1_30min\n",
      "- Add label_d2_30min\n",
      "- Add label_d3_30min\n",
      "- Add label_d4_30min\n",
      "Ajout des labels selon les stratégies TIST (D1, D2, D3, D4)\n",
      "\n",
      "Phase 4: Adding severity labels and categories\n",
      "- Add delay_category\n",
      "- Add delay_severity_score\n",
      "- Add weather_delay_category\n",
      "- Add nas_delay_category\n",
      "- Add weather_dominance_score\n",
      "- Add dominant_delay_type\n",
      "\n",
      "Phase 5: Adding composite and interaction labels\n",
      "- Add tist_strategy_15min\n",
      "- Add weather_risk_label\n",
      "- Add balanced_15min_candidate\n",
      "- Add predictable_delay_15min\n",
      "- Add temporal_split_group\n",
      "\n",
      "Phase 6: Validate And Log Label Statistics\n",
      "- Total of Flights: 477960\n",
      "- Delay ≥15min: 70843 (15%)\n",
      "- Delay ≥60min: 19458 (4%)\n",
      "- Delay météo: 4877 (1%)\n",
      "- Delay NAS: 40711 (9%)\n",
      "\n",
      "Phase 7: Log TIST Strategy Statistics\n",
      "Strategy D1 (15min): 42543 vols (9%)\n",
      "Strategy D1 (60min): 10703 vols (2%)\n",
      "Strategy D2 (15min): 28189 vols (6%)\n",
      "Strategy D2 (60min): 7580 vols (2%)\n",
      "Strategy D3 (15min): 42543 vols (9%)\n",
      "Strategy D3 (60min): 42543 vols (9%)\n",
      "Strategy D4 (15min): 70843 vols (15%)\n",
      "Strategy D4 (60min): 19458 vols (4%)\n",
      "\n",
      "Phase 8: Log Distribution by delay category\n",
      "minor_delay: 81170 vols (17%)\n",
      "on_time: 328989 vols (69%)\n",
      "moderate_delay: 48761 vols (10%)\n",
      "extreme_delay: 2312 vols (0%)\n",
      "major_delay: 16728 vols (3%)\n",
      "\n",
      "Phase 9: Validate Label Consistency\n",
      "Inconsistency in the hierarchy D1 ⊆ D2 ⊆ D3\n",
      "Consistency validation completed\n",
      "\n",
      "--> [FlightDataGenerator] Flight Data Generator- End ...\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "- Writing Csv /output/data\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 3 in stage 447.0 failed 1 times, most recent failure: Lost task 3.0 in stage 447.0 (TID 1093) (jupyter-spark executor driver): java.io.IOException: Mkdirs failed to create file:/output/data/_temporary/0/_temporary/attempt_202510010620043892692643308428300_0447_m_000003_1093 (exists=false, cwd=file:/home/jovyan/work/notebooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 447.0 failed 1 times, most recent failure: Lost task 3.0 in stage 447.0 (TID 1093) (jupyter-spark executor driver): java.io.IOException: Mkdirs failed to create file:/output/data/_temporary/0/_temporary/attempt_202510010620043892692643308428300_0447_m_000003_1093 (exists=false, cwd=file:/home/jovyan/work/notebooks)",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)",
      "  at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)",
      "  at com.flightdelay.utils.CsvWriter$.write(CsvWriter.scala:60)",
      "  at com.flightdelay.utils.CsvWriter$.write(CsvWriter.scala:38)",
      "  at com.flightdelay.data.preprocessing.FlightPreprocessingPipeline$.execute(FlightPreprocessingPipeline.scala:27)",
      "  ... 56 elided",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/output/data/_temporary/0/_temporary/attempt_202510010620043892692643308428300_0447_m_000003_1093 (exists=false, cwd=file:/home/jovyan/work/notebooks)",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)",
      "  at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)",
      "  at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)",
      "  at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)",
      "  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)",
      "  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)",
      "  at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)",
      "  at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "  ... 3 more"
     ]
    }
   ],
   "source": [
    "import com.flightdelay.data.preprocessing.FlightPreprocessingPipeline\n",
    "\n",
    "val flightPreproicessedData = FlightPreprocessingPipeline.execute(flightData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aba71b-fb55-44e1-9330-6870d4a0b298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_scala - Scala",
   "language": "scala",
   "name": "apache_toree_scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
