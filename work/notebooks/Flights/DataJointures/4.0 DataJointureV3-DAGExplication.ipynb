{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e159c5ae-37c7-43b7-a800-6349093ad707",
   "metadata": {},
   "source": [
    "# Data Jointure V4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e54813-1f4c-4ba8-ae6f-bd3f4571fb6a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b9013a-88ff-4c24-8b38-c69e9114ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar\n",
      "Finished download of Emiasd-Flight-Data-Analysis.jar\n",
      "Using cached version of Emiasd-Flight-Data-Analysis.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee6509b-16ae-49fc-b829-db9aed8c8820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "args = Array(jupyter)\n",
       "configuration = AppConfiguration(local,CommonConfig(42,true,debug,false,false,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output/data),FileConfig(/home/jovyan/work/output/model),None),MLFlowConfig(false,http://localhost:5555),/scripts),Stream(ExperimentConfig(Experience-local,Ba...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "AppConfiguration(local,CommonConfig(42,true,debug,false,false,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output/data),FileConfig(/home/jovyan/work/output/model),None),MLFlowConfig(false,http://localhost:5555),/scripts),Stream(ExperimentConfig(Experience-local,Ba..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import com.flightdelay.config.{AppConfiguration, ConfigurationLoader, ExperimentConfig}\n",
    "import com.flightdelay.data.loaders.FlightDataLoader\n",
    "\n",
    "// Env Configuration\n",
    "val args: Array[String] = Array(\"jupyter\")\n",
    "implicit val configuration: AppConfiguration = ConfigurationLoader.loadConfiguration(args)\n",
    "implicit val experimentConfig: ExperimentConfig = configuration.experiments(0)\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(sc.getConf)\n",
    "  .config(\"spark.eventLog.enabled\", \"true\")\n",
    "  .config(\"spark.eventLog.dir\", s\"${configuration.common.output.basePath}/spark-events\")  // ex: \"file:/tmp/spark-events\" ou \"hdfs:///spark-events\"\n",
    "  .getOrCreate()\n",
    "\n",
    "// Rendre la session Spark implicite\n",
    "implicit val session = spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dd5ff-6a58-4ec9-bd15-b0d629e468c6",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96dc104-1631-4d57-8b1a-6b6aeb896805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Flight DF Count: ,3908458)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDFPath = /home/jovyan/work/output/common/data/processed_flights.parquet\n",
       "flightData = [OP_CARRIER_AIRLINE_ID: int, DEST_AIRPORT_ID: int ... 32 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[OP_CARRIER_AIRLINE_ID: int, DEST_AIRPORT_ID: int ... 32 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDFPath = s\"${configuration.common.output.basePath}/common/data/processed_flights.parquet\"\n",
    "val flightData = spark.read.parquet(flightDFPath)\n",
    "\n",
    "println(\"Flight DF Count: \", flightData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccf286a-46b7-4aaf-881d-048ad866fd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherDFPath = /home/jovyan/work/output/common/data/processed_weather.parquet\n",
       "weatherData = [RelativeHumidity: double, feature_visibility_category: string ... 16 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Weather DF Count: ,1549320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RelativeHumidity: double, feature_visibility_category: string ... 16 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weatherDFPath = s\"${configuration.common.output.basePath}/common/data/processed_weather.parquet\"\n",
    "val weatherData = spark.read.parquet(weatherDFPath)\n",
    "\n",
    "println(\"Weather DF Count: \", weatherData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f7ddc-6864-4a71-9e7e-b373a3217f0b",
   "metadata": {},
   "source": [
    "## Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4a1331-95e9-48c1-bbc8-f094cabf4865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherOriginDepthHours = 3\n",
       "weatherDestinationDepthHours = 3\n",
       "labeledFlightData = [OP_CARRIER_AIRLINE_ID: int, DEST_AIRPORT_ID: int ... 33 more fields]\n",
       "flightFeaturesWithTarget = Some(Vector(OP_CARRIER_AIRLINE_ID, DEST_AIRPORT_ID, ORIGIN_AIRPORT_ID, feature_departure_hour_rounded_cos, CRS_ELAPSED_TIME, feature_departure_hour_rounded_sin, feature_arrival_time_period, feature_flight_week_of_year_cos, CRS_DEP_TIME, feature_flight_week_of_year_sin, feature_departure_time_period, is_delayed))\n",
       "weatherFeatures = Some(Vector(RelativeHumidity, feature_visibility_c...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 25/12/11 22:09:31   - Automatically adding target 'is_delayed' to flight features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Some(Vector(RelativeHumidity, feature_visibility_c..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.flightdelay.features.balancer.DelayBalancedDatasetBuilder\n",
    "import com.flightdelay.utils.DebugUtils._\n",
    "import com.flightdelay.utils.MetricsUtils\n",
    "\n",
    "val weatherOriginDepthHours = experimentConfig.featureExtraction.weatherOriginDepthHours\n",
    "val weatherDestinationDepthHours = experimentConfig.featureExtraction.weatherDestinationDepthHours\n",
    "\n",
    "\n",
    "val labeledFlightData =  DelayBalancedDatasetBuilder.prepareLabeledDataset(\n",
    "  df = flightData,\n",
    "  dxCol = experimentConfig.featureExtraction.dxCol\n",
    ")\n",
    "\n",
    "val flightFeaturesWithTarget = experimentConfig.featureExtraction.flightSelectedFeatures.map { features =>\n",
    "    val featureNames = features.keys.toSeq\n",
    "    if (featureNames.contains(experimentConfig.target)) {\n",
    "      featureNames\n",
    "    } else {\n",
    "      info(s\"  - Automatically adding target '${experimentConfig.target}' to flight features\")\n",
    "      featureNames :+ experimentConfig.target\n",
    "    }\n",
    "}\n",
    "\n",
    "val weatherFeatures = experimentConfig.featureExtraction.weatherSelectedFeatures.map(_.keys.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a341a8d-56e2-4b0d-97bb-72819938dae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cores = 14\n",
       "numPartsOrigin = 46\n",
       "numPartsDest = 73\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "pickParts: (mult: Double, minAbs: Int, maxAbs: Int)Int\n",
       "hhmmHourCol: (c: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// =====================================================================\n",
    "// Flight × Weather en DataFrame (Map → Hash partition → Reduce)\n",
    "// - Fenêtre 12h avant départ (Wo_*) et 12h avant arrivée (Wd_*)\n",
    "// - Gestion veille via relHour (duplication J et J+1)\n",
    "// - Un seul job global avec Metrics.withJob\n",
    "// =====================================================================\n",
    "import org.apache.spark.sql.{DataFrame, Row, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "\n",
    "// ------------------------\n",
    "// Paramètres de partitions \"reducers\"\n",
    "// ------------------------\n",
    "\n",
    "\n",
    "val cores = spark.sparkContext.defaultParallelism\n",
    "def pickParts(mult: Double, minAbs: Int, maxAbs: Int): Int =\n",
    "math.min(maxAbs, math.max(minAbs, math.round(cores * mult).toInt))\n",
    "\n",
    "val numPartsOrigin = pickParts(3.3, 32, 128) // ≈ 40\n",
    "val numPartsDest   = pickParts(5.2, 48, 192) // ≈ 64\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", numPartsDest) // borne haute DF\n",
    "\n",
    "// ------------------------\n",
    "// Utilitaire HHMM -> hour [0..23] (sans UDF) (Map)\n",
    "// ------------------------\n",
    "def hhmmHourCol(c: Column): Column = {\n",
    "  val s  = regexp_replace(c.cast(\"string\"), \":\", \"\")\n",
    "  val p4 = lpad(s, 4, \"0\")\n",
    "  (substring(p4, 1, 2).cast(\"int\") % 24)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f721e751-0437-4679-8142-69cefdb97933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 25/12/11 22:13:11 NoteBook.FeaturePipeline.join → Starting job: NoteBook.FeaturePipeline.join\n",
      "Exception in thread \"Executor task launch worker for task 3.0 in stage 40.0 (TID 532)\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:133)\n",
      "\tat java.base/java.lang.Runtime.halt(Runtime.java:277)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:76)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:826)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 3 in stage 40.0 failed 1 times, most recent failure: Lost task 3.0 in stage 40.0 (TID 532) (jupyter-spark executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5956/0x0000000801fc9840.apply(Unknown Source)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike$$Lambda$151/0x00000008002d6840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n\tat org.apache.spark.storage.BlockManager$$Lambda$3380/0x0000000801498040.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 40.0 failed 1 times, most recent failure: Lost task 3.0 in stage 40.0 (TID 532) (jupyter-spark executor driver): java.lang.OutOfMemoryError: Java heap space",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5956/0x0000000801fc9840.apply(Unknown Source)",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "\tat scala.collection.TraversableLike$$Lambda$151/0x00000008002d6840.apply(Unknown Source)",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$3380/0x0000000801498040.apply(Unknown Source)",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "Caused by: java.lang.OutOfMemoryError: Java heap space",
      "  at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)",
      "  at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)",
      "  at org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)",
      "  at org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)",
      "  at org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)",
      "  at org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)",
      "  at org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)",
      "  at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)",
      "  at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5956/0x0000000801fc9840.apply(Unknown Source)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike$$Lambda$151/0x00000008002d6840.apply(Unknown Source)",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)",
      "  at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)",
      "  at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)",
      "  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)",
      "  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)",
      "  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)",
      "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)",
      "  at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)",
      "  at org.apache.spark.storage.BlockManager$$Lambda$3380/0x0000000801498040.apply(Unknown Source)",
      "  at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)",
      "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)",
      "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)",
      "  at org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)",
      "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import com.flightdelay.utils.MetricsUtils\n",
    "\n",
    "MetricsUtils.withUiLabels(\n",
    "  groupId = \"NoteBook.FeaturePipeline.join\",\n",
    "  desc    = \"NoteBook.FeaturePipeline.join\",\n",
    "  tags    = \"sampling,split,balance\"\n",
    ") {\n",
    "\n",
    "  // ------------------------\n",
    "  // Sélection des colonnes utiles (MAP)\n",
    "  // ------------------------\n",
    "  val flights = labeledFlightData\n",
    "\n",
    "  val weather = weatherData\n",
    "    .select(\n",
    "      col(\"Date\").cast(DateType).as(\"WDATE\"),\n",
    "      col(\"Time\").as(\"WTIME_HHMM\"),\n",
    "      col(\"*\")\n",
    "    )\n",
    "    .where(\n",
    "      col(\"WBAN\").isNotNull &&\n",
    "      length(col(\"WBAN\")) > 0 &&\n",
    "      col(\"WDATE\").isNotNull\n",
    "    )\n",
    "\n",
    "  // ------------------------\n",
    "  // Préparation météo avec relHour + duplication J/J+1 (MAP)\n",
    "  // ------------------------\n",
    "  val weatherWithHour = weather\n",
    "    .withColumn(\"hour\", hhmmHourCol(col(\"WTIME_HHMM\")))\n",
    "    .na.fill(Map(\"hour\" -> -1))\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "  // matérialise le cache météo brut (1 seule lecture parquet)\n",
    "  weatherWithHour.count()\n",
    "\n",
    "  val meteoSameDay = weatherWithHour\n",
    "    .withColumn(\"relHour\", col(\"hour\"))\n",
    "    .withColumn(\"DATE\", col(\"WDATE\"))\n",
    "\n",
    "  val meteoNextDay = weatherWithHour\n",
    "    .withColumn(\"relHour\", col(\"hour\") - lit(24))\n",
    "    .withColumn(\"DATE\", date_add(col(\"WDATE\"), 1))\n",
    "\n",
    "  val weatherRel = meteoSameDay\n",
    "    .unionByName(meteoNextDay)\n",
    "    .filter(col(\"relHour\").between(-24, 23))\n",
    "\n",
    "  // ------------------------\n",
    "  // Reduce météo par clé → Map relHour -> struct (REDUCE)\n",
    "  // ------------------------\n",
    "\n",
    "  val staticCols = Seq(\n",
    "    col(\"relHour\").as(\"hour\"),\n",
    "    col(\"WBAN\"),\n",
    "    col(\"WDATE\"),\n",
    "    col(\"WTIME_HHMM\")\n",
    "  )\n",
    "\n",
    "  val staticNames = Set(\"relHour\", \"WBAN\", \"WDATE\", \"WTIME_HHMM\")\n",
    "\n",
    "  val allWeatherCols: Seq[String] =\n",
    "    weatherFeatures.getOrElse(Seq.empty[String])\n",
    "\n",
    "  val dynamicFeatureCols =\n",
    "    allWeatherCols\n",
    "      .filterNot(staticNames.contains)\n",
    "      .map(c => col(c).alias(c))\n",
    "\n",
    "  val weatherStruct = struct((staticCols ++ dynamicFeatureCols): _*)\n",
    "\n",
    "  val weatherByKey: DataFrame =\n",
    "    weatherRel\n",
    "      .groupBy(col(\"WBAN\"), col(\"DATE\"))\n",
    "      .agg(\n",
    "        map_from_entries(\n",
    "          collect_list(struct(col(\"relHour\"), weatherStruct))\n",
    "        ).as(\"wmap\")\n",
    "      )\n",
    "      .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "  // matérialise la météo agrégée (évite de tout recalculer dans les jobs suivants)\n",
    "  weatherByKey.count()\n",
    "\n",
    "  // ------------------------\n",
    "  // JOIN #1 — ORIGIN\n",
    "  // ------------------------\n",
    "  val flightsDep = flights\n",
    "    .withColumn(\n",
    "      \"depHour\",\n",
    "      coalesce(hhmmHourCol(col(\"UTC_CRS_DEP_TIME\")), lit(0))\n",
    "    )\n",
    "\n",
    "  val originPre = flightsDep\n",
    "    .join(\n",
    "      // on garde un shuffle_hash join, plus safe que le broadcast en mémoire limitée\n",
    "      weatherByKey.hint(\"shuffle_hash\"),\n",
    "      col(\"ORIGIN_WBAN\") === weatherByKey(\"WBAN\") &&\n",
    "      col(\"UTC_FL_DATE\") === weatherByKey(\"DATE\"),\n",
    "      \"left\"   // mets \"inner\" si tu veux supprimer les vols sans météo\n",
    "    )\n",
    "    .drop(weatherByKey(\"WBAN\"))\n",
    "    .drop(weatherByKey(\"DATE\"))\n",
    "\n",
    "  val originWithWoArr = originPre\n",
    "    .withColumn(\n",
    "      \"Wo\",\n",
    "      expr(\"transform(sequence(1, 3), i -> element_at(wmap, depHour - i))\")\n",
    "    )\n",
    "    .drop(\"wmap\")\n",
    "\n",
    "  val woCols = (0 until 3).map(i => col(\"Wo\").getItem(i).as(s\"Wo_h${i + 1}\"))\n",
    "\n",
    "  val originDF = originWithWoArr\n",
    "    .select(col(\"*\") +: woCols: _*)\n",
    "    .drop(\"Wo\")\n",
    "    // plus de persist ici : dataset très gros, une seule vraie réutilisation\n",
    "\n",
    "  // ------------------------\n",
    "  // JOIN #2 — DEST\n",
    "  // ------------------------\n",
    "  val flightsArr = originDF\n",
    "    .withColumn(\n",
    "      \"arrHour\",\n",
    "      coalesce(hhmmHourCol(col(\"UTC_ARR_TIME\")), lit(0))\n",
    "    )\n",
    "\n",
    "  val destPre = flightsArr\n",
    "    .join(\n",
    "      weatherByKey.hint(\"shuffle_hash\"),\n",
    "      col(\"DEST_WBAN\") === weatherByKey(\"WBAN\") &&\n",
    "      col(\"UTC_ARR_DATE\") === weatherByKey(\"DATE\"),\n",
    "      \"left\"   // idem, \"inner\" si tu veux filtrer les vols sans météo\n",
    "    )\n",
    "    .drop(weatherByKey(\"WBAN\"))\n",
    "    .drop(weatherByKey(\"DATE\"))\n",
    "\n",
    "  val destWithWdArr = destPre\n",
    "    .withColumn(\n",
    "      \"Wd\",\n",
    "      expr(\"transform(sequence(1, 3), i -> element_at(wmap, arrHour - i))\")\n",
    "    )\n",
    "    .drop(\"wmap\")\n",
    "\n",
    "  val wdCols = (0 until 3).map(i => col(\"Wd\").getItem(i).as(s\"Wd_h${i + 1}\"))\n",
    "\n",
    "  val baseCols: Seq[Column] =\n",
    "    originDF.columns.map(col).toSeq\n",
    "\n",
    "  val joinedDF = destWithWdArr\n",
    "    .select((baseCols ++ wdCols): _*)\n",
    "    .drop(\"Wd\")\n",
    "    // pas de persist ici non plus : très gros DF, utilisé seulement pour le count\n",
    "\n",
    "  // ------------------------\n",
    "  // Actions finales (déclenchent l'exécution)\n",
    "  // ------------------------\n",
    "  println(\n",
    "    s\"Rows after ORIGIN join: ${originDF.count()}, \" +\n",
    "      s\"rows after DEST join: ${joinedDF.count()}\"\n",
    "  )\n",
    "\n",
    "  // Optionnel : libérer la mémoire si tu enchaînes d'autres traitements\n",
    "  // weatherByKey.unpersist()\n",
    "  // weatherWithHour.unpersist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5df1516-55d5-46ea-a8ad-71099f11d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- feature_departure_hour_rounded_cos: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- feature_departure_hour_rounded_sin: double (nullable = true)\n",
      " |-- feature_arrival_time_period: string (nullable = true)\n",
      " |-- feature_flight_week_of_year_cos: double (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- feature_flight_week_of_year_sin: double (nullable = true)\n",
      " |-- feature_departure_time_period: string (nullable = true)\n",
      " |-- ARR_DELAY_NEW: double (nullable = true)\n",
      " |-- D1: integer (nullable = true)\n",
      " |-- D2_15: integer (nullable = true)\n",
      " |-- D2_30: integer (nullable = true)\n",
      " |-- D2_45: integer (nullable = true)\n",
      " |-- D2_60: integer (nullable = true)\n",
      " |-- D2_90: integer (nullable = true)\n",
      " |-- D3: integer (nullable = true)\n",
      " |-- D4: integer (nullable = true)\n",
      " |-- UTC_FL_DATE: date (nullable = true)\n",
      " |-- feature_utc_departure_hour_rounded: long (nullable = true)\n",
      " |-- feature_utc_arrival_hour_rounded: string (nullable = true)\n",
      " |-- feature_flight_unique_id: string (nullable = true)\n",
      " |-- ORIGIN_WBAN: string (nullable = true)\n",
      " |-- DEST_WBAN: string (nullable = true)\n",
      " |-- UTC_ARR_DATE: date (nullable = true)\n",
      " |-- UTC_CRS_DEP_TIME: string (nullable = true)\n",
      " |-- UTC_ARR_TIME: string (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- is_delayed: integer (nullable = false)\n",
      " |-- depHour: integer (nullable = false)\n",
      " |-- Wo_h1: struct (nullable = true)\n",
      " |    |-- hour: integer (nullable = false)\n",
      " |    |-- WBAN: string (nullable = true)\n",
      " |    |-- WDATE: date (nullable = true)\n",
      " |    |-- WTIME_HHMM: string (nullable = true)\n",
      " |    |-- RelativeHumidity: double (nullable = true)\n",
      " |    |-- feature_visibility_category: string (nullable = true)\n",
      " |    |-- Temp_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |    |-- WindSpeed_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_is_pressure_dropping_fast: integer (nullable = true)\n",
      " |    |-- Visibility_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_weather_type: string (nullable = true)\n",
      " |    |-- HourlyPrecip: double (nullable = true)\n",
      " |    |-- feature_icing_risk_level: integer (nullable = true)\n",
      " |    |-- feature_weather_intensity: string (nullable = true)\n",
      " |    |-- press_change_abs: double (nullable = true)\n",
      " |    |-- feature_temp_dewpoint_spread: double (nullable = true)\n",
      " |    |-- feature_icing_risk_flag: integer (nullable = true)\n",
      " |    |-- Humidity_Delta_1hr: double (nullable = true)\n",
      " |-- Wo_h2: struct (nullable = true)\n",
      " |    |-- hour: integer (nullable = false)\n",
      " |    |-- WBAN: string (nullable = true)\n",
      " |    |-- WDATE: date (nullable = true)\n",
      " |    |-- WTIME_HHMM: string (nullable = true)\n",
      " |    |-- RelativeHumidity: double (nullable = true)\n",
      " |    |-- feature_visibility_category: string (nullable = true)\n",
      " |    |-- Temp_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |    |-- WindSpeed_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_is_pressure_dropping_fast: integer (nullable = true)\n",
      " |    |-- Visibility_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_weather_type: string (nullable = true)\n",
      " |    |-- HourlyPrecip: double (nullable = true)\n",
      " |    |-- feature_icing_risk_level: integer (nullable = true)\n",
      " |    |-- feature_weather_intensity: string (nullable = true)\n",
      " |    |-- press_change_abs: double (nullable = true)\n",
      " |    |-- feature_temp_dewpoint_spread: double (nullable = true)\n",
      " |    |-- feature_icing_risk_flag: integer (nullable = true)\n",
      " |    |-- Humidity_Delta_1hr: double (nullable = true)\n",
      " |-- Wo_h3: struct (nullable = true)\n",
      " |    |-- hour: integer (nullable = false)\n",
      " |    |-- WBAN: string (nullable = true)\n",
      " |    |-- WDATE: date (nullable = true)\n",
      " |    |-- WTIME_HHMM: string (nullable = true)\n",
      " |    |-- RelativeHumidity: double (nullable = true)\n",
      " |    |-- feature_visibility_category: string (nullable = true)\n",
      " |    |-- Temp_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |    |-- WindSpeed_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_is_pressure_dropping_fast: integer (nullable = true)\n",
      " |    |-- Visibility_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_weather_type: string (nullable = true)\n",
      " |    |-- HourlyPrecip: double (nullable = true)\n",
      " |    |-- feature_icing_risk_level: integer (nullable = true)\n",
      " |    |-- feature_weather_intensity: string (nullable = true)\n",
      " |    |-- press_change_abs: double (nullable = true)\n",
      " |    |-- feature_temp_dewpoint_spread: double (nullable = true)\n",
      " |    |-- feature_icing_risk_flag: integer (nullable = true)\n",
      " |    |-- Humidity_Delta_1hr: double (nullable = true)\n",
      " |-- Wd_h1: struct (nullable = true)\n",
      " |    |-- hour: integer (nullable = false)\n",
      " |    |-- WBAN: string (nullable = true)\n",
      " |    |-- WDATE: date (nullable = true)\n",
      " |    |-- WTIME_HHMM: string (nullable = true)\n",
      " |    |-- RelativeHumidity: double (nullable = true)\n",
      " |    |-- feature_visibility_category: string (nullable = true)\n",
      " |    |-- Temp_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |    |-- WindSpeed_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_is_pressure_dropping_fast: integer (nullable = true)\n",
      " |    |-- Visibility_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_weather_type: string (nullable = true)\n",
      " |    |-- HourlyPrecip: double (nullable = true)\n",
      " |    |-- feature_icing_risk_level: integer (nullable = true)\n",
      " |    |-- feature_weather_intensity: string (nullable = true)\n",
      " |    |-- press_change_abs: double (nullable = true)\n",
      " |    |-- feature_temp_dewpoint_spread: double (nullable = true)\n",
      " |    |-- feature_icing_risk_flag: integer (nullable = true)\n",
      " |    |-- Humidity_Delta_1hr: double (nullable = true)\n",
      " |-- Wd_h2: struct (nullable = true)\n",
      " |    |-- hour: integer (nullable = false)\n",
      " |    |-- WBAN: string (nullable = true)\n",
      " |    |-- WDATE: date (nullable = true)\n",
      " |    |-- WTIME_HHMM: string (nullable = true)\n",
      " |    |-- RelativeHumidity: double (nullable = true)\n",
      " |    |-- feature_visibility_category: string (nullable = true)\n",
      " |    |-- Temp_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |    |-- WindSpeed_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_is_pressure_dropping_fast: integer (nullable = true)\n",
      " |    |-- Visibility_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_weather_type: string (nullable = true)\n",
      " |    |-- HourlyPrecip: double (nullable = true)\n",
      " |    |-- feature_icing_risk_level: integer (nullable = true)\n",
      " |    |-- feature_weather_intensity: string (nullable = true)\n",
      " |    |-- press_change_abs: double (nullable = true)\n",
      " |    |-- feature_temp_dewpoint_spread: double (nullable = true)\n",
      " |    |-- feature_icing_risk_flag: integer (nullable = true)\n",
      " |    |-- Humidity_Delta_1hr: double (nullable = true)\n",
      " |-- Wd_h3: struct (nullable = true)\n",
      " |    |-- hour: integer (nullable = false)\n",
      " |    |-- WBAN: string (nullable = true)\n",
      " |    |-- WDATE: date (nullable = true)\n",
      " |    |-- WTIME_HHMM: string (nullable = true)\n",
      " |    |-- RelativeHumidity: double (nullable = true)\n",
      " |    |-- feature_visibility_category: string (nullable = true)\n",
      " |    |-- Temp_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |    |-- WindSpeed_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_is_pressure_dropping_fast: integer (nullable = true)\n",
      " |    |-- Visibility_Delta_1hr: double (nullable = true)\n",
      " |    |-- feature_weather_type: string (nullable = true)\n",
      " |    |-- HourlyPrecip: double (nullable = true)\n",
      " |    |-- feature_icing_risk_level: integer (nullable = true)\n",
      " |    |-- feature_weather_intensity: string (nullable = true)\n",
      " |    |-- press_change_abs: double (nullable = true)\n",
      " |    |-- feature_temp_dewpoint_spread: double (nullable = true)\n",
      " |    |-- feature_icing_risk_flag: integer (nullable = true)\n",
      " |    |-- Humidity_Delta_1hr: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23233df8-5a2b-4f57-9b30-8477ce74cf66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_scala - Scala",
   "language": "scala",
   "name": "apache_toree_scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
