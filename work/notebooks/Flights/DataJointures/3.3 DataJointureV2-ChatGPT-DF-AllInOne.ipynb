{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e159c5ae-37c7-43b7-a800-6349093ad707",
   "metadata": {},
   "source": [
    "# Data Jointure V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e54813-1f4c-4ba8-ae6f-bd3f4571fb6a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b9013a-88ff-4c24-8b38-c69e9114ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar\n",
      "Finished download of Emiasd-Flight-Data-Analysis.jar\n",
      "Using cached version of Emiasd-Flight-Data-Analysis.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee6509b-16ae-49fc-b829-db9aed8c8820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "args = Array(jupyter)\n",
       "spark = org.apache.spark.sql.SparkSession@53a15a09\n",
       "session = org.apache.spark.sql.SparkSession@53a15a09\n",
       "configuration = AppConfiguration(local,CommonConfig(42,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "AppConfiguration(local,CommonConfig(42,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import com.flightdelay.config.{AppConfiguration, ConfigurationLoader, ExperimentConfig}\n",
    "import com.flightdelay.data.loaders.FlightDataLoader\n",
    "\n",
    "//Env Configuration\n",
    "val args: Array[String] = Array(\"jupyter\")\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(sc.getConf)\n",
    "  .getOrCreate()\n",
    "\n",
    "// Rendre la session Spark implicite\n",
    "implicit val session = spark\n",
    "implicit val configuration: AppConfiguration = ConfigurationLoader.loadConfiguration(args)\n",
    "implicit val experiment: ExperimentConfig = configuration.experiments(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dd5ff-6a58-4ec9-bd15-b0d629e468c6",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96dc104-1631-4d57-8b1a-6b6aeb896805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Flight DF Count: ,1928396)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDFPath = /home/jovyan/work/output/common/data/processed_flights.parquet\n",
       "flightDF = [DEST_WBAN: string, ORIGIN_WBAN: string ... 118 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_WBAN: string, ORIGIN_WBAN: string ... 118 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDFPath = s\"${configuration.common.output.basePath}/common/data/processed_flights.parquet\"\n",
    "val flightDF = spark.read.parquet(flightDFPath)\n",
    "\n",
    "println(\"Flight DF Count: \", flightDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccf286a-46b7-4aaf-881d-048ad866fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Weather DF Count: ,755247)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weatherDFPath = /home/jovyan/work/output/common/data/processed_weather.parquet\n",
       "weatherDF = [WBAN: string, Date: date ... 92 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[WBAN: string, Date: date ... 92 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weatherDFPath = s\"${configuration.common.output.basePath}/common/data/processed_weather.parquet\"\n",
    "val weatherDF = spark.read.parquet(weatherDFPath)\n",
    "\n",
    "println(\"Weather DF Count: \", weatherDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bc400-6556-45e1-82d3-10fe8b4dd8a2",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353c2a3-598d-456e-9924-e0266608cfd2",
   "metadata": {},
   "source": [
    "### On garde les vols pour lesquels nous avons de la météo à l'aéroport de départ et d'arrivée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcac082-137a-4b92-b722-590d54ede21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WBAN filter] Flights before: 1928396, after: 1928396, removed: 0\n",
      "Saved filtered flights to: /home/jovyan/work/output/common/data/flight_filtered_has_weather.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weatherStations = [WBAN: string]\n",
       "flightsWBAN = [DEST_WBAN: string, ORIGIN_WBAN: string ... 118 more fields]\n",
       "countBefore = 1928396\n",
       "originStations = [ORIGIN_WBAN: string]\n",
       "flightsHasOrigin = [ORIGIN_WBAN: string, DEST_WBAN: string ... 118 more fields]\n",
       "destStations = [DEST_WBAN: string]\n",
       "flightDF_filtered = [DEST_WBAN: string, ORIGIN_WBAN: string ... 118 more fields]\n",
       "countAfter = 1928396\n",
       "outPath = /home/jovyan/work/output/common/data/flight_filtered_has_weather.parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/home/jovyan/work/output/common/data/flight_filtered_has_weather.parquet"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spark 3.5.3 / Scala 2.12\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "/** 1) WBAN valides côté météo (distinct, non nuls, nettoyés) */\n",
    "val weatherStations = weatherDF\n",
    "  .select(trim(col(\"WBAN\")).as(\"WBAN\"))\n",
    "  .where(col(\"WBAN\").isNotNull && length(col(\"WBAN\")) > 0)\n",
    "  .distinct()\n",
    "  .repartition(200)        // optionnel: ajuste selon cluster\n",
    "  .cache()\n",
    "\n",
    "/** 2) Prépare les colonnes WBAN côté vols (nettoyage basique) */\n",
    "val flightsWBAN = flightDF\n",
    "  .withColumn(\"ORIGIN_WBAN\", trim(col(\"ORIGIN_WBAN\")))\n",
    "  .withColumn(\"DEST_WBAN\",   trim(col(\"DEST_WBAN\")))\n",
    "\n",
    "/** 3) Comptage avant filtrage (optionnel) */\n",
    "val countBefore = flightsWBAN.count()\n",
    "\n",
    "/** 4) Garde uniquement les vols dont ORIGIN_WBAN existe dans la météo */\n",
    "val originStations = weatherStations\n",
    "  .select(col(\"WBAN\").as(\"ORIGIN_WBAN\"))\n",
    "\n",
    "val flightsHasOrigin = flightsWBAN\n",
    "  .join(originStations, Seq(\"ORIGIN_WBAN\"), \"left_semi\")\n",
    "\n",
    "/** 5) Puis garde uniquement ceux dont DEST_WBAN existe aussi */\n",
    "val destStations = weatherStations\n",
    "  .select(col(\"WBAN\").as(\"DEST_WBAN\"))\n",
    "\n",
    "val flightDF_filtered = flightsHasOrigin\n",
    "  .join(destStations, Seq(\"DEST_WBAN\"), \"left_semi\")\n",
    "  .cache()\n",
    "\n",
    "/** 6) Comptage après filtrage et petit bilan */\n",
    "val countAfter  = flightDF_filtered.count()\n",
    "println(s\"[WBAN filter] Flights before: $countBefore, after: $countAfter, removed: ${countBefore - countAfter}\")\n",
    "\n",
    "/** 7) (Optionnel) Sauvegarde */\n",
    "val outPath = s\"${configuration.common.output.basePath}/common/data/flight_filtered_has_weather.parquet\"\n",
    "flightDF_filtered\n",
    "  .repartition(400)        // optionnel: selon volume/cluster\n",
    "  .write.mode(\"overwrite\")\n",
    "  .parquet(outPath)\n",
    "println(s\"Saved filtered flights to: $outPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71c0f5-5acf-4594-a3e6-89bf22356f6f",
   "metadata": {},
   "source": [
    "### On garde la météo qui est associé à un aéroport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12dd1ac3-a8fa-41eb-b038-66c87d7af410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Weather prune] Weather rows before: 755247, after: 755247, removed: 0\n",
      "Saved pruned weather to: /home/jovyan/work/output/common/data/weather_pruned_by_flights.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightWBANs = [WBAN: string]\n",
       "weatherBefore = 755247\n",
       "weatherDF_pruned = [WBAN: string, Date: date ... 92 more fields]\n",
       "weatherAfter = 755247\n",
       "outPathWeather = /home/jovyan/work/output/common/data/weather_pruned_by_flights.parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/home/jovyan/work/output/common/data/weather_pruned_by_flights.parquet"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spark 3.5.3 / Scala 2.12\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "/** 1) WBAN référencés par au moins un vol (origine ou destination) */\n",
    "val flightWBANs: DataFrame =\n",
    "  flightDF\n",
    "    .select(trim(col(\"ORIGIN_WBAN\")).as(\"WBAN\"))\n",
    "    .where(col(\"WBAN\").isNotNull && length(col(\"WBAN\")) > 0)\n",
    "    .unionByName(\n",
    "      flightDF\n",
    "        .select(trim(col(\"DEST_WBAN\")).as(\"WBAN\"))\n",
    "        .where(col(\"WBAN\").isNotNull && length(col(\"WBAN\")) > 0)\n",
    "    )\n",
    "    .distinct()\n",
    "    .repartition(200) // optionnel selon cluster/volume\n",
    "    .cache()\n",
    "\n",
    "/** 2) Comptage avant (optionnel) */\n",
    "val weatherBefore = weatherDF.count()\n",
    "\n",
    "/** 3) Filtrer la météo: garder uniquement les WBAN utilisés par les vols */\n",
    "val weatherDF_pruned =\n",
    "  weatherDF\n",
    "    .withColumn(\"WBAN\", trim(col(\"WBAN\")))\n",
    "    .where(col(\"WBAN\").isNotNull && length(col(\"WBAN\")) > 0)\n",
    "    .join(flightWBANs, Seq(\"WBAN\"), \"left_semi\")\n",
    "    .cache()\n",
    "\n",
    "/** 4) Comptage après et bilan */\n",
    "val weatherAfter = weatherDF_pruned.count()\n",
    "println(s\"[Weather prune] Weather rows before: $weatherBefore, after: $weatherAfter, removed: ${weatherBefore - weatherAfter}\")\n",
    "\n",
    "/** 5) (Optionnel) Sauvegarde */\n",
    "val outPathWeather = s\"${configuration.common.output.basePath}/common/data/weather_pruned_by_flights.parquet\"\n",
    "weatherDF_pruned\n",
    "  .repartition(400) // optionnel\n",
    "  .write.mode(\"overwrite\")\n",
    "  .parquet(outPathWeather)\n",
    "println(s\"Saved pruned weather to: $outPathWeather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dba5c-5005-4791-a1b7-45bfacf03f24",
   "metadata": {},
   "source": [
    "## Restreindre les vols aux mois couverts par la météo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d983022-c5ec-42da-9da1-532ae87adb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step1] Flights in covered months: 1928396 / 1928396 (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightsCoveredMonths = [DEST_WBAN: string, ORIGIN_WBAN: string ... 119 more fields]\n",
       "weatherMonths = [month_utc: string]\n",
       "flightDF_mCovered = [month_utc: string, DEST_WBAN: string ... 119 more fields]\n",
       "kept = 1928396\n",
       "total = 1928396\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "1928396"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val flightsCoveredMonths =\n",
    "  flightDF_filtered\n",
    "    .withColumn(\"month_utc\", date_format(col(\"UTC_FL_DATE\"), \"yyyy-MM\"))\n",
    "\n",
    "val weatherMonths =\n",
    "  weatherDF_pruned\n",
    "    .withColumn(\"month_utc\", date_format(col(\"Date\"), \"yyyy-MM\"))\n",
    "    .select(\"month_utc\").distinct()\n",
    "\n",
    "val flightDF_mCovered =\n",
    "  flightsCoveredMonths.join(weatherMonths, Seq(\"month_utc\"), \"left_semi\")\n",
    "\n",
    "val kept = flightDF_mCovered.count()\n",
    "val total = flightDF_filtered.count()\n",
    "println(f\"[Step1] Flights in covered months: $kept / $total (${kept.toDouble * 100.0 / total}%.2f%%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed6e42-487a-4b3c-a39b-97d282e1c459",
   "metadata": {},
   "source": [
    "## Jointure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0996a5a4-9c25-4a84-a67e-b3a2c168be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Flight for join count ->,1928396)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightData = [month_utc: string, DEST_WBAN: string ... 119 more fields]\n",
       "weatherData = [WBAN: string, Date: date ... 92 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Weather for join count ->,755247)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WBAN: string, Date: date ... 92 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightData = flightDF_mCovered\n",
    "val weatherData = weatherDF_pruned\n",
    "\n",
    "println(\"Flight for join count ->\", flightData.count())\n",
    "println(\"Weather for join count ->\", weatherData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a341a8d-56e2-4b0d-97bb-72819938dae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Metrics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after ORIGIN join: 3908461, rows after DEST join: 3908461\n",
      "[METRIC][flight-joins-df] Build flights joins DataFrame took 67511.60 ms\n"
     ]
    }
   ],
   "source": [
    "// =====================================================================\n",
    "// Flight × Weather en DataFrame (Map → Hash partition → Reduce)\n",
    "// - Fenêtre 12h avant départ (Wo_*) et 12h avant arrivée (Wd_*)\n",
    "// - Gestion veille via relHour (duplication J et J+1)\n",
    "// - Un seul job global avec Metrics.withJob\n",
    "// =====================================================================\n",
    "import org.apache.spark.sql.{DataFrame, Row, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// ------------------------\n",
    "// 0) Objet Metrics (inchangé)\n",
    "// ------------------------\n",
    "object Metrics {\n",
    "  def withJob[T](id: String, desc: String)(body: => T): T = {\n",
    "    val sc = spark.sparkContext\n",
    "    sc.setJobGroup(id, desc, interruptOnCancel = false)\n",
    "    val t0 = System.nanoTime()\n",
    "    val res = body\n",
    "    val dtMs = (System.nanoTime() - t0) / 1e6\n",
    "    println(f\"[METRIC][$id] $desc took ${dtMs}%.2f ms\")\n",
    "    sc.clearJobGroup()\n",
    "    res\n",
    "  }\n",
    "  import org.apache.spark.storage.StorageLevel\n",
    "  def persistCount(id: String, desc: String, rdd: org.apache.spark.rdd.RDD[_], lvl: StorageLevel = StorageLevel.MEMORY_ONLY): Long = {\n",
    "    rdd.persist(lvl); val n = withJob(id, desc) { rdd.count() }\n",
    "    println(s\"[METRIC][$id] records=$n partitions=${rdd.getNumPartitions}\"); n\n",
    "  }\n",
    "  def persistCountDF(id: String, desc: String, df: DataFrame, lvl: StorageLevel = StorageLevel.MEMORY_ONLY): Long = {\n",
    "    df.persist(lvl); val n = withJob(id, desc) { df.count() }\n",
    "    println(s\"[METRIC][$id] records=$n partitions=${df.rdd.getNumPartitions}\"); n\n",
    "  }\n",
    "}\n",
    "\n",
    "// =====================================================================\n",
    "// 1) Jointure complète exécutée sous un SEUL job\n",
    "// =====================================================================\n",
    "\n",
    "\n",
    "// ------------------------\n",
    "// Paramètres de partitions \"reducers\"\n",
    "// ------------------------\n",
    "\n",
    "Metrics.withJob(\"flight-joins-df\", \"Build flights joins DataFrame\") {\n",
    "    val cores = spark.sparkContext.defaultParallelism\n",
    "    def pickParts(mult: Double, minAbs: Int, maxAbs: Int): Int =\n",
    "    math.min(maxAbs, math.max(minAbs, math.round(cores * mult).toInt))\n",
    "    \n",
    "    val numPartsOrigin = pickParts(3.3, 32, 128) // ≈ 40\n",
    "    val numPartsDest   = pickParts(5.2, 48, 192) // ≈ 64\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", numPartsDest) // borne haute DF\n",
    "    \n",
    "    // ------------------------\n",
    "    // Sélection des colonnes utiles (Map)\n",
    "    // ------------------------\n",
    "    //flightDF_filtered\n",
    "    val flights = flightDF_mCovered.select(\n",
    "    col(\"feature_flight_unique_id\").as(\"flightId\"),\n",
    "    col(\"ORIGIN_WBAN\"), col(\"DEST_WBAN\"),\n",
    "    col(\"UTC_FL_DATE\").cast(DateType).as(\"DEP_DATE\"),\n",
    "    col(\"UTC_ARR_DATE\").cast(DateType).as(\"ARR_DATE\"),\n",
    "    col(\"UTC_CRS_DEP_TIME\").as(\"DEP_TIME_HHMM\"),\n",
    "    col(\"UTC_ARR_TIME\").as(\"ARR_TIME_HHMM\"),\n",
    "    col(\"*\")\n",
    "    )\n",
    "    \n",
    "    val weather = weatherDF_pruned.select(\n",
    "    trim(col(\"WBAN\")).as(\"WBAN\"),\n",
    "    col(\"Date\").cast(DateType).as(\"WDATE\"),\n",
    "    col(\"Time\").as(\"WTIME_HHMM\"),\n",
    "    col(\"Visibility\").as(\"vis\"),\n",
    "    col(\"WindSpeed\").as(\"ws\"),\n",
    "    col(\"WindDirection\").as(\"wd\"),\n",
    "    col(\"DryBulbCelsius\").as(\"tempC\"),\n",
    "    col(\"SeaLevelPressure\").as(\"slp\"),\n",
    "    col(\"HourlyPrecip\").as(\"precip\"),\n",
    "    col(\"feature_weather_severity_index\"),\n",
    "    col(\"feature_flight_category_ordinal\")\n",
    "    ).where(col(\"WBAN\").isNotNull && length(col(\"WBAN\")) > 0 && col(\"WDATE\").isNotNull)\n",
    "    \n",
    "    // ------------------------\n",
    "    // Utilitaire HHMM -> hour [0..23] (sans UDF) (Map)\n",
    "    // ------------------------\n",
    "    def hhmmHourCol(c: Column): Column = {\n",
    "    val s  = regexp_replace(c.cast(\"string\"), \":\", \"\")\n",
    "    val p4 = lpad(s, 4, \"0\")\n",
    "    (substring(p4, 1, 2).cast(\"int\") % 24)\n",
    "    }\n",
    "    \n",
    "    // ------------------------\n",
    "    // Préparation météo avec relHour + duplication J/J+1 (Map)\n",
    "    // ------------------------\n",
    "    val weatherWithHour = weather\n",
    "    .withColumn(\"hour\", hhmmHourCol(col(\"WTIME_HHMM\")))\n",
    "    .na.fill(Map(\"hour\" -> -1))\n",
    "    \n",
    "    val meteoSameDay = weatherWithHour\n",
    "    .withColumn(\"relHour\", col(\"hour\"))\n",
    "    .withColumn(\"DATE\", col(\"WDATE\"))\n",
    "    \n",
    "    val meteoNextDay = weatherWithHour\n",
    "    .withColumn(\"relHour\", col(\"hour\") - lit(24))\n",
    "    .withColumn(\"DATE\", date_add(col(\"WDATE\"), 1))\n",
    "    \n",
    "    val weatherRel = meteoSameDay.unionByName(meteoNextDay)\n",
    "    .filter(col(\"relHour\").between(-24, 23))\n",
    "    \n",
    "    // ------------------------\n",
    "    // Reduce météo par clé → Map relHour -> struct (Reduce)\n",
    "    // ------------------------\n",
    "    val weatherStruct =\n",
    "    struct(\n",
    "      col(\"relHour\").as(\"hour\"),\n",
    "      col(\"WBAN\"), col(\"WDATE\"), col(\"WTIME_HHMM\"),\n",
    "      col(\"vis\"), col(\"ws\"), col(\"wd\"),\n",
    "      col(\"tempC\"), col(\"slp\"), col(\"precip\"),\n",
    "      col(\"feature_weather_severity_index\"),\n",
    "      col(\"feature_flight_category_ordinal\")\n",
    "    )\n",
    "    \n",
    "    val weatherByKey: DataFrame =\n",
    "    weatherRel\n",
    "      .groupBy(col(\"WBAN\"), col(\"DATE\"))\n",
    "      .agg(\n",
    "        map_from_entries(\n",
    "          collect_list(struct(col(\"relHour\"), weatherStruct))\n",
    "        ).as(\"wmap\")\n",
    "      )\n",
    "    // (WBAN, DATE, wmap: map<int, struct{hour,WBAN,WDATE,WTIME_HHMM,...}>)\n",
    "    \n",
    "    // ------------------------\n",
    "    // JOIN #1 — ORIGIN (Partition = hash(ORIGIN_WBAN, DEP_DATE))\n",
    "    // ------------------------\n",
    "    val flightsDep = flights\n",
    "    .withColumn(\"depHour\", coalesce(hhmmHourCol(col(\"DEP_TIME_HHMM\")), lit(0)))\n",
    "    \n",
    "    val originPre = flightsDep\n",
    "    .repartition(numPartsOrigin, col(\"ORIGIN_WBAN\"), col(\"DEP_DATE\")) // <-- Hash partition explicite\n",
    "    .join(\n",
    "      weatherByKey.hint(\"shuffle_hash\"),\n",
    "      col(\"ORIGIN_WBAN\") === weatherByKey(\"WBAN\") &&\n",
    "      col(\"DEP_DATE\")    === weatherByKey(\"DATE\"),\n",
    "      \"left\"\n",
    "    )\n",
    "    .drop(weatherByKey(\"WBAN\")).drop(weatherByKey(\"DATE\"))\n",
    "    \n",
    "    val originWithWoArr = originPre\n",
    "    .withColumn(\"Wo\", expr(\"transform(sequence(1, 12), i -> element_at(wmap, depHour - i))\"))\n",
    "    .drop(\"wmap\")\n",
    "    \n",
    "    val woCols = (0 until 12).map(i => col(\"Wo\").getItem(i).as(s\"Wo_h${i+1}\"))\n",
    "    val originDF = originWithWoArr\n",
    "    .select(col(\"*\") +: woCols: _*)\n",
    "    .drop(\"Wo\")\n",
    "    .persist()\n",
    "    \n",
    "    // ------------------------\n",
    "    // JOIN #2 — DEST (Partition = hash(DEST_WBAN, ARR_DATE))\n",
    "    // ------------------------\n",
    "    val flightsArr = originDF\n",
    "    .withColumn(\"arrHour\", coalesce(hhmmHourCol(col(\"ARR_TIME_HHMM\")), lit(0)))\n",
    "    \n",
    "    val destPre = flightsArr\n",
    "    .repartition(numPartsDest, col(\"DEST_WBAN\"), col(\"ARR_DATE\"))     // <-- Hash partition explicite\n",
    "    .join(\n",
    "      weatherByKey.hint(\"shuffle_hash\"),\n",
    "      col(\"DEST_WBAN\") === weatherByKey(\"WBAN\") &&\n",
    "      col(\"ARR_DATE\")  === weatherByKey(\"DATE\"),\n",
    "      \"left\"\n",
    "    )\n",
    "    .drop(weatherByKey(\"WBAN\")).drop(weatherByKey(\"DATE\"))\n",
    "    \n",
    "    val destWithWdArr = destPre\n",
    "    .withColumn(\"Wd\", expr(\"transform(sequence(1, 12), i -> element_at(wmap, arrHour - i))\"))\n",
    "    .drop(\"wmap\")\n",
    "    \n",
    "    val wdCols = (0 until 12).map(i => col(\"Wd\").getItem(i).as(s\"Wd_h${i+1}\"))\n",
    "    val joinedDF = destWithWdArr\n",
    "    .select(col(\"*\") +: wdCols: _*)\n",
    "    .drop(\"Wd\")\n",
    "    .persist()\n",
    "    \n",
    "    // ------------------------\n",
    "    // Action finale (déclenche l'exécution)\n",
    "    // ------------------------\n",
    "    println(s\"Rows after ORIGIN join: ${originDF.count()}, rows after DEST join: ${joinedDF.count()}\")\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabccf69-99f0-4ea7-98e8-f1c51af8e99f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:66: error: not found: value joinedDF\n       joinedDF.show(1, 1000, true)\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "joinedDF.show(1, 1000, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721e751-0437-4679-8142-69cefdb97933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_scala - Scala",
   "language": "scala",
   "name": "apache_toree_scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
