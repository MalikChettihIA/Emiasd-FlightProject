# Fix: DateAwareVectorIndexer Serialization Issue

## Probl√®me Initial

L'ex√©cution √©chouait avec une erreur **Task not serializable** :

```
org.apache.spark.SparkException: Task not serializable
Caused by: java.io.NotSerializableException: scala.collection.immutable.MapLike$anon$1
Serialization stack:
  - object not serializable (class: scala.collection.immutable.MapLike$anon$1, value: Map(...))
  - field (class: org.apache.spark.ml.feature.VectorIndexerModel, name: categoryMaps, type: interface scala.collection.immutable.Map)
  - object (class org.apache.spark.ml.feature.VectorIndexerModel, VectorIndexerModel: uid=dateAwareVectorIndexer_...)
```

**Contexte :** L'erreur survenait lors du fit du pipeline avec le nouveau `DateAwareVectorIndexer`.

---

## Cause Racine

Dans `DateAwareVectorIndexer.scala`, ligne 49 :

```scala
val filteredCategoryMaps = baseModel.categoryMaps.filterKeys(idx => !dateIndices.contains(idx))
```

**Probl√®me :** La m√©thode `.filterKeys()` cr√©e une **vue lazy** sur la Map originale :
- Type retourn√© : `scala.collection.immutable.MapLike$anon$1`
- Cette vue n'est **pas s√©rialisable**
- Quand Spark essaie de s√©rialiser le `VectorIndexerModel` pour l'envoyer aux workers, la s√©rialisation √©choue

**Pourquoi c'est un probl√®me :**
1. Spark distribue les t√¢ches en s√©rialisant les objets (closures, mod√®les, etc.)
2. Les vues lazy comme `MapLike$anon$1` contiennent des r√©f√©rences √† la Map originale et des fonctions
3. Ces r√©f√©rences ne sont pas s√©rialisables

---

## Solution Impl√©ment√©e

**Fichier :** `DateAwareVectorIndexer.scala`, ligne 49-50

**Avant :**
```scala
// ‚ùå filterKeys cr√©e une vue non-s√©rialisable
val filteredCategoryMaps = baseModel.categoryMaps.filterKeys(idx => !dateIndices.contains(idx))
```

**Apr√®s :**
```scala
// ‚úÖ filter cr√©e une Map concr√®te s√©rialisable
// Use .filter instead of .filterKeys to create a serializable Map
val filteredCategoryMaps = baseModel.categoryMaps.filter { case (idx, _) => !dateIndices.contains(idx) }
```

**Diff√©rence cl√© :**

| M√©thode | Type retourn√© | S√©rialisable ? |
|---------|---------------|----------------|
| `.filterKeys(f)` | `MapLike$anon$1` (vue lazy) | ‚ùå Non |
| `.filter { case (k, v) => f(k) }` | `Map[Int, Map[Double, Int]]` (Map concr√®te) | ‚úÖ Oui |

---

## Explication Technique

### `.filterKeys()` (Probl√©matique)

```scala
val map = Map(1 -> "a", 2 -> "b", 3 -> "c")
val filtered = map.filterKeys(_ > 1)  // ‚ùå Vue lazy
```

**Caract√©ristiques :**
- Cr√©e une vue qui applique le filtre **√† la demande**
- Garde une r√©f√©rence √† la Map originale
- Type : `scala.collection.MapLike$anon$1`
- **Non s√©rialisable** (contient des closures)

---

### `.filter()` (Solution)

```scala
val map = Map(1 -> "a", 2 -> "b", 3 -> "c")
val filtered = map.filter { case (k, _) => k > 1 }  // ‚úÖ Map concr√®te
```

**Caract√©ristiques :**
- Cr√©e une **nouvelle Map** en m√©moire
- Ne garde pas de r√©f√©rence √† la Map originale
- Type : `scala.collection.immutable.Map[K, V]`
- **S√©rialisable** (Map standard)

---

## Tests de Validation

### 1. Compilation

```bash
sbt compile
```

**R√©sultat attendu :**
```
[info] done compiling
[success] Total time: 2 s
```

### 2. Packaging

```bash
sbt package
```

**R√©sultat attendu :**
```
[success] Total time: 0 s
```

### 3. Ex√©cution

```bash
./work/scripts/spark-submit.sh
```

**Logs attendus :**
```
[DateAwareVectorIndexer] Excluded 5 date-derived features from categorical treatment:
  [36] date_UTC_FL_DATE_year
  [37] date_UTC_FL_DATE_month
  [38] date_UTC_FL_DATE_day
  [39] date_UTC_FL_DATE_dayofweek
  [40] date_UTC_FL_DATE_unix

[VectorIndexer] Feature Type Analysis
================================================================================
Total features in vector: 41
Categorical features detected: X  ‚Üê R√©duit (sans date features)
Numeric features: Y              ‚Üê Augment√© (avec date features)
```

**Aucune erreur de s√©rialisation !** ‚úÖ

---

## Bonnes Pratiques Spark/Scala

### √âviter les Vues Lazy dans Spark

Les m√©thodes suivantes cr√©ent des vues **non-s√©rialisables** :

‚ùå **√Ä √âVITER dans Spark :**
- `map.filterKeys(f)` ‚Üí utiliser `map.filter { case (k, _) => f(k) }`
- `map.mapValues(f)` ‚Üí utiliser `map.map { case (k, v) => k -> f(v) }`
- `seq.view.map(f)` ‚Üí utiliser `seq.map(f)` directement

‚úÖ **√Ä UTILISER :**
- `map.filter { case (k, v) => ... }` ‚Üí Map concr√®te
- `map.map { case (k, v) => ... }` ‚Üí Map concr√®te
- `seq.map(f)` ‚Üí Collection concr√®te

---

## Impact sur les Performances

**Question :** Est-ce que `.filter()` est plus lent que `.filterKeys()` ?

**R√©ponse :** Non, n√©gligeable dans ce contexte :

| Aspect | `.filterKeys()` | `.filter()` |
|--------|-----------------|-------------|
| **Cr√©ation** | O(1) (vue) | O(n) (copie) |
| **Acc√®s** | O(n) par acc√®s (lazy) | O(1) par acc√®s (Map) |
| **S√©rialisation** | ‚ùå Impossible | ‚úÖ O(n) |

**Dans notre cas :**
- La Map `categoryMaps` est petite (< 50 entr√©es typiquement)
- Elle est cr√©√©e **une seule fois** pendant le fit
- Elle est ensuite **s√©rialis√©e plusieurs fois** (envoy√©e aux workers)
- **Conclusion :** `.filter()` est plus efficace car √©vite les erreurs de s√©rialisation

---

## Autres Cas de S√©rialisation dans le Projet

Si vous rencontrez d'autres erreurs `Task not serializable`, v√©rifiez :

### 1. Closures Capturant des Objets Non-S√©rialisables

```scala
// ‚ùå Capture tout l'objet `this`
class MyClass {
  val data = Map(...)

  def process(rdd: RDD[Int]): RDD[Int] = {
    rdd.map(x => x + data.size)  // ‚ùå Capture `this.data`
  }
}

// ‚úÖ Extrait la valeur avant la closure
class MyClass {
  val data = Map(...)

  def process(rdd: RDD[Int]): RDD[Int] = {
    val size = data.size  // Copie locale
    rdd.map(x => x + size)  // ‚úÖ Capture seulement `size` (Int)
  }
}
```

### 2. R√©f√©rences √† des Objets Spark Non-S√©rialisables

```scala
// ‚ùå SparkContext n'est pas s√©rialisable
val sc = spark.sparkContext
rdd.map(x => sc.broadcast(...))  // ‚ùå Erreur

// ‚úÖ Cr√©er le broadcast AVANT le map
val bcData = sc.broadcast(data)
rdd.map(x => bcData.value)  // ‚úÖ OK
```

### 3. Utilisation de Classes Java Non-S√©rialisables

```scala
// ‚ùå SimpleDateFormat n'est pas serializable
val formatter = new SimpleDateFormat("yyyy-MM-dd")
rdd.map(x => formatter.parse(x))  // ‚ùå Erreur

// ‚úÖ Cr√©er l'objet dans chaque partition
rdd.mapPartitions { partition =>
  val formatter = new SimpleDateFormat("yyyy-MM-dd")  // Un par partition
  partition.map(x => formatter.parse(x))
}
```

---

## Troubleshooting

### Erreur Persiste Apr√®s Fix

**Sympt√¥me :**
```
Task not serializable
```

**V√©rifications :**
1. Recompiler compl√®tement :
   ```bash
   sbt clean compile package
   ```

2. V√©rifier que le JAR est bien r√©g√©n√©r√© :
   ```bash
   ls -lh target/scala-2.12/*.jar
   ```

3. V√©rifier que Spark utilise le bon JAR :
   ```bash
   grep "main JAR" work/scripts/spark-submit.sh
   ```

---

### Comment D√©boguer les Erreurs de S√©rialisation

Activer le mode debug de s√©rialisation :

```scala
// Dans SparkConf
spark.conf.set("spark.serializer.objectStreamReset", "100")
spark.conf.set("spark.kryo.registrationRequired", "false")
```

Ou ajouter dans les logs :
```scala
import org.apache.spark.serializer.SerializationDebugger
SerializationDebugger.find(monObjet)
```

---

## R√©sum√©

| Avant | Apr√®s |
|-------|-------|
| ‚ùå `.filterKeys()` ‚Üí Vue lazy MapLike$anon$1 | ‚úÖ `.filter { case ... }` ‚Üí Map concr√®te |
| ‚ùå Task not serializable | ‚úÖ S√©rialisation OK |
| ‚ùå Pipeline √©choue au fit | ‚úÖ Pipeline s'ex√©cute compl√®tement |
| ‚ùå Date features trait√©es comme cat√©gorielles | ‚úÖ Date features trait√©es comme num√©riques |

**Le fix est minimal (1 ligne chang√©e) mais critique pour le fonctionnement !**

---

## R√©f√©rences

- [Spark Documentation - Closures](https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures)
- [Scala Collections - Views](https://docs.scala-lang.org/overviews/collections/views.html)
- [Spark Serialization](https://spark.apache.org/docs/latest/tuning.html#data-serialization)

**Prochaine √©tape :** Relancer le pipeline pour v√©rifier que les date features sont bien trait√©es comme num√©riques ! üéØ

```bash
./work/scripts/spark-submit.sh
```
