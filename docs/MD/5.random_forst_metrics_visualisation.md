# Flight Delay Prediction - Metrics Visualization

## Overview

Le pipeline d'entraînement génère automatiquement des fichiers CSV contenant les métriques du modèle. Ces fichiers peuvent être visualisés avec le script Python `visualize_metrics.py`.

## Fichiers Générés

Après l'entraînement, les métriques sont sauvegardées dans :
```
/output/metrics/{model_name}_{model_type}/
├── train_test_comparison.csv      # Comparaison train/test pour chaque métrique
├── confusion_matrix_train.csv      # Matrice de confusion (train)
├── confusion_matrix_test.csv       # Matrice de confusion (test)
├── feature_importance.csv          # Importance des features (Random Forest)
├── roc_data_train.csv             # Données pour courbe ROC (train)
└── roc_data_test.csv              # Données pour courbe ROC (test)
```

Pour notre configuration actuelle :
```
/output/metrics/flight_delay_rf_randomforest/
```

## Structure des Fichiers CSV

### 1. `train_test_comparison.csv`
```csv
metric,train,test,gap
accuracy,0.850000,0.840000,0.010000
precision,0.830000,0.820000,0.010000
recall,0.870000,0.860000,0.010000
f1_score,0.850000,0.840000,0.010000
auc_roc,0.920000,0.910000,0.010000
auc_pr,0.880000,0.870000,0.010000
```

### 2. `confusion_matrix_test.csv`
```csv
,Predicted_Positive,Predicted_Negative
Actual_Positive,12500,1500
Actual_Negative,2000,34000
```

### 3. `feature_importance.csv`
```csv
feature_index,importance
42,0.123456
15,0.098765
89,0.087654
...
```

### 4. `roc_data_train.csv` / `roc_data_test.csv`
```csv
label,prediction,prob_positive
1.0,1.0,0.892345
0.0,0.0,0.123456
1.0,1.0,0.765432
0.0,1.0,0.654321
...
```
- **label** : Vraie classe (0 = à l'heure, 1 = retardé)
- **prediction** : Prédiction du modèle (0 ou 1)
- **prob_positive** : Probabilité de la classe positive (retard)

## Installation des Dépendances Python

```bash
pip install pandas matplotlib seaborn numpy scikit-learn
```

Ou avec conda :
```bash
conda install pandas matplotlib seaborn numpy scikit-learn
```

## Utilisation du Script de Visualisation

### Commande de Base

```bash
python work/scripts/visualize_metrics.py /output/metrics/flight_delay_rf_randomforest
```

### Exemple Complet (depuis la racine du projet)

```bash
# Après l'entraînement du modèle
cd /Users/malikchettih/Projects/Emiasd-Projects/Emiasd-FlightProject

# Générer les visualisations
python ./work/scripts/visualize_metrics.py ./work/output/metrics/flight_delay_rf_randomforest 
```

## Graphiques Générés

Le script génère automatiquement les visualisations suivantes dans le sous-répertoire `plots/` :

### 1. **train_test_comparison.png**
- **Bar Chart** : Comparaison des métriques train vs test
- **Gap Analysis** : Analyse de l'overfitting (différence train-test)
- Seuil d'alerte à 5% de gap

### 2. **confusion_matrices.png**
- Matrices de confusion côte à côte (train et test)
- Heatmap avec annotations des valeurs

### 3. **feature_importance.png**
- Top 30 features les plus importantes
- Bar chart horizontal avec valeurs
- Gradient de couleur selon l'importance

### 4. **metrics_radar.png**
- Radar chart des métriques principales
- Comparaison visuelle train vs test
- Métriques : accuracy, precision, recall, F1, AUC-ROC

### 5. **roc_curve.png**
- Courbe ROC (Receiver Operating Characteristic)
- Affichage train vs test avec AUC
- Comparaison avec classificateur aléatoire
- Boîte d'interprétation avec évaluation qualitative

### 6. **precision_recall_curve.png**
- Courbe Précision-Recall
- Particulièrement utile pour datasets déséquilibrés
- Average Precision (AP) pour train et test
- Comparaison avec baseline aléatoire

### 7. **summary_report.txt**
- Rapport texte résumant les performances
- Analyse de l'overfitting
- Top 10 features les plus importantes

## Structure des Outputs

```
/output/metrics/flight_delay_rf_randomforest/
├── train_test_comparison.csv
├── confusion_matrix_train.csv
├── confusion_matrix_test.csv
├── feature_importance.csv
├── roc_data_train.csv
├── roc_data_test.csv
└── plots/
    ├── train_test_comparison.png
    ├── confusion_matrices.png
    ├── feature_importance.png
    ├── metrics_radar.png
    ├── roc_curve.png
    ├── precision_recall_curve.png
    └── summary_report.txt
```

## Personnalisation

### Modifier le nombre de features affichées

```python
# Dans visualize_metrics.py, ligne ~160
plot_feature_importance(metrics, output_dir, top_n=50)  # Au lieu de 30
```

### Changer les couleurs

```python
# Modifier les couleurs dans les fonctions de plot
colors = ['#your_color_1', '#your_color_2']
```

### Ajouter de nouvelles métriques

1. Modifier `ModelEvaluator.scala` pour calculer la métrique
2. Ajouter la métrique à `saveMetricsToFile()`
3. Modifier `visualize_metrics.py` pour afficher la nouvelle métrique

## Exemple d'Utilisation Complète

```bash
# 1. Entraîner le modèle (génère automatiquement les métriques)
cd work
./local-submit.sh

# 2. Attendre la fin de l'entraînement

# 3. Générer les visualisations
python scripts/visualize_metrics.py /output/metrics/flight_delay_rf_randomforest

# 4. Consulter les résultats
open /output/metrics/flight_delay_rf_randomforest/plots/
```

## Interprétation des Résultats

### Gap Analysis
- **Gap < 5%** : Modèle bien généralisé ✓
- **Gap 5-10%** : Overfitting modéré ⚠
- **Gap > 10%** : Overfitting significatif ✗

### Feature Importance
- Identifie les features les plus utiles pour la prédiction
- Aide à comprendre quels facteurs influencent les retards
- Peut guider l'ingénierie de features futures

### Confusion Matrix
- **True Positives (TP)** : Retards correctement prédits
- **True Negatives (TN)** : À l'heure correctement prédit
- **False Positives (FP)** : Faux retards (Type I error)
- **False Negatives (FN)** : Retards manqués (Type II error)

### ROC Curve (Receiver Operating Characteristic)
- Affiche le compromis entre True Positive Rate (Recall) et False Positive Rate
- **AUC (Area Under Curve)** :
  - **AUC = 1.0** : Classificateur parfait
  - **AUC > 0.9** : Excellent modèle
  - **AUC = 0.8-0.9** : Bon modèle
  - **AUC = 0.7-0.8** : Modèle acceptable
  - **AUC = 0.5** : Classificateur aléatoire (pas mieux que le hasard)
- Plus la courbe est proche du coin supérieur gauche, meilleur est le modèle
- Utile pour comparer différents modèles

### Precision-Recall Curve
- Alternative à la courbe ROC, particulièrement utile pour datasets déséquilibrés
- Affiche le compromis entre Precision et Recall
- **Average Precision (AP)** : Aire sous la courbe PR
- Plus sensible aux performances sur la classe minoritaire (retards)
- Préférable quand les faux positifs ont un coût important

## Dépannage

### Erreur : "No metrics files found"
- Vérifier que l'entraînement s'est bien terminé
- Vérifier le chemin du répertoire de métriques
- S'assurer que les fichiers CSV ont bien été générés

### Erreur : "ModuleNotFoundError: No module named 'pandas'" ou 'sklearn'
```bash
pip install pandas matplotlib seaborn numpy scikit-learn
```

### Les graphiques ne s'affichent pas
- Les graphiques sont sauvegardés en PNG, pas affichés à l'écran
- Chercher dans le sous-répertoire `plots/`

## Notes Techniques

- **Format CSV** : Compatible avec Excel, R, Julia, etc.
- **Encodage** : UTF-8
- **Séparateur** : Virgule (`,`)
- **DPI des images** : 300 (haute qualité pour publications)
- **Format des images** : PNG (transparent background)
