#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Visualization script for ML Pipeline Results (K-Fold CV + Hold-out Test)

This script visualizes comprehensive metrics generated by the MLPipeline:
- Cross-validation fold metrics
- CV summary statistics
- Hold-out test set performance
- Best hyperparameters from grid search
- ROC Curve from hold-out test

Usage:
    python visualize_ml_pipeline.py <metrics_directory>

Example:
    python visualize_ml_pipeline.py /output/exp1_rf_pca/metrics
"""

import sys
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from sklearn.metrics import roc_curve, auc

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (5, 3)  # Reduced again for MLFlow UI
plt.rcParams['font.size'] = 7  # Reduced font size

def load_ml_pipeline_metrics(metrics_dir):
    """Load all ML Pipeline metrics CSV files"""
    metrics_dir = Path(metrics_dir)
    metrics = {}

    # Load CV fold metrics
    cv_folds_file = metrics_dir / "cv_fold_metrics.csv"
    if cv_folds_file.exists():
        metrics['cv_folds'] = pd.read_csv(cv_folds_file)
        print(f"✓ Loaded: {cv_folds_file}")

    # Load CV summary (mean ± std)
    cv_summary_file = metrics_dir / "cv_summary.csv"
    if cv_summary_file.exists():
        metrics['cv_summary'] = pd.read_csv(cv_summary_file)
        print(f"✓ Loaded: {cv_summary_file}")

    # Load hold-out test metrics
    holdout_file = metrics_dir / "holdout_test_metrics.csv"
    if holdout_file.exists():
        metrics['holdout'] = pd.read_csv(holdout_file)
        print(f"✓ Loaded: {holdout_file}")

    # Load best hyperparameters
    hp_file = metrics_dir / "best_hyperparameters.csv"
    if hp_file.exists():
        metrics['hyperparameters'] = pd.read_csv(hp_file)
        print(f"✓ Loaded: {hp_file}")

    # Load ROC data for curve plotting
    roc_file = metrics_dir / "holdout_roc_data.csv"
    if roc_file.exists():
        metrics['roc_data'] = pd.read_csv(roc_file)
        print(f"✓ Loaded: {roc_file}")

    return metrics

def plot_cv_folds_detailed(metrics, output_dir):
    """Plot detailed metrics across CV folds"""
    if 'cv_folds' not in metrics:
        print("✗ No CV folds data found")
        return

    df = metrics['cv_folds']
    metric_cols = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    fig, axes = plt.subplots(2, 3, figsize=(7, 4))  # Reduced by half for MLFlow
    axes = axes.flatten()

    for idx, metric in enumerate(metric_cols):
        ax = axes[idx]

        # Line plot with markers
        ax.plot(df['fold'], df[metric] * 100, marker='o', linewidth=2,
                markersize=8, color='#3498db', alpha=0.8, label='Fold metric')

        # Add mean line
        mean_val = df[metric].mean() * 100
        std_val = df[metric].std() * 100
        ax.axhline(y=mean_val, color='#e74c3c', linestyle='--',
                   linewidth=2, alpha=0.7, label=f'Mean: {mean_val:.2f}%')

        # Add std dev band
        ax.fill_between(df['fold'], mean_val - std_val, mean_val + std_val,
                        alpha=0.2, color='#e74c3c', label=f'±1σ: {std_val:.2f}%')

        # Styling
        ax.set_xlabel('Fold Number', fontweight='bold', fontsize=6)
        ax.set_ylabel(f'{metric.replace("_", " ").title()} (%)', fontweight='bold', fontsize=6)
        ax.set_title(f'{metric.replace("_", " ").title()} Across CV Folds',
                     fontsize=7, fontweight='bold')
        ax.legend(fontsize=5)
        ax.grid(alpha=0.3)
        ax.set_xticks(df['fold'])
        ax.set_ylim([0, 105])

    # Remove extra subplot
    fig.delaxes(axes[5])

    plt.tight_layout()
    output_file = output_dir / "cv_folds_detailed.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_cv_vs_holdout_comparison(metrics, output_dir):
    """Plot CV average vs hold-out test metrics comparison"""
    if 'cv_summary' not in metrics or 'holdout' not in metrics:
        print("✗ Missing CV summary or hold-out data")
        return

    cv_df = metrics['cv_summary']
    holdout_df = metrics['holdout']

    # Prepare data - only classification metrics
    main_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    cv_data = []
    holdout_data = []
    cv_std = []
    labels = []

    for metric in main_metrics:
        cv_row = cv_df[cv_df['metric'] == metric]
        holdout_row = holdout_df[holdout_df['metric'] == metric]

        if not cv_row.empty and not holdout_row.empty:
            cv_data.append(cv_row['mean'].values[0] * 100)
            cv_std.append(cv_row['std'].values[0] * 100)
            holdout_data.append(holdout_row['value'].values[0] * 100)
            labels.append(metric.replace('_', '\n').title())

    fig, axes = plt.subplots(1, 2, figsize=(7, 2.5))  # Reduced by half for MLFlow

    # Plot 1: Bar chart comparison
    ax1 = axes[0]
    x = np.arange(len(labels))
    width = 0.35

    bars1 = ax1.bar(x - width/2, cv_data, width, yerr=cv_std,
                    label='CV Mean ± Std', alpha=0.8, color='#2ecc71',
                    capsize=5, error_kw={'linewidth': 2, 'elinewidth': 2})
    bars2 = ax1.bar(x + width/2, holdout_data, width,
                    label='Hold-out Test', alpha=0.8, color='#3498db')

    # Add value labels
    for i, bar in enumerate(bars1):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + cv_std[i] + 1,
                f'{cv_data[i]:.1f}%',
                ha='center', va='bottom', fontsize=5, fontweight='bold')

    for i, bar in enumerate(bars2):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{holdout_data[i]:.1f}%',
                ha='center', va='bottom', fontsize=5, fontweight='bold')

    ax1.set_xlabel('Metric', fontweight='bold', fontsize=6)
    ax1.set_ylabel('Score (%)', fontweight='bold', fontsize=6)
    ax1.set_title('Cross-Validation vs Hold-out Test Performance',
                  fontsize=7, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(labels, fontsize=5)
    ax1.legend(fontsize=6, loc='lower right')
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim([0, 105])

    # Plot 2: Generalization gap analysis
    ax2 = axes[1]
    gaps = [cv - ho for cv, ho in zip(cv_data, holdout_data)]
    colors = ['#e74c3c' if abs(gap) > 5 else '#2ecc71' if abs(gap) < 2 else '#f39c12'
              for gap in gaps]

    bars = ax2.barh(labels, gaps, color=colors, alpha=0.8, edgecolor='black', linewidth=1)

    # Add value labels
    for i, (bar, gap) in enumerate(zip(bars, gaps)):
        width = bar.get_width()
        ax2.text(width, bar.get_y() + bar.get_height()/2,
                f' {gap:.2f}%' if width >= 0 else f'{gap:.2f}% ',
                ha='left' if width >= 0 else 'right',
                va='center', fontsize=5, fontweight='bold')

    ax2.set_xlabel('Gap (CV - Hold-out) [%]', fontweight='bold', fontsize=6)
    ax2.set_title('Generalization Gap Analysis', fontsize=7, fontweight='bold')
    ax2.axvline(x=0, color='black', linestyle='-', linewidth=1)
    ax2.axvline(x=2, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='±2% threshold')
    ax2.axvline(x=-2, color='orange', linestyle='--', linewidth=1, alpha=0.5)
    ax2.axvline(x=5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='±5% threshold')
    ax2.axvline(x=-5, color='red', linestyle='--', linewidth=1, alpha=0.5)
    ax2.legend(fontsize=5, loc='best')
    ax2.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    output_file = output_dir / "cv_vs_holdout_comparison.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_box_plot_stability(metrics, output_dir):
    """Box plot showing distribution and stability of metrics across folds"""
    if 'cv_folds' not in metrics:
        return

    df = metrics['cv_folds']
    metric_cols = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    # Prepare data
    data_to_plot = [df[col].values * 100 for col in metric_cols]

    fig, ax = plt.subplots(figsize=(5, 3))  # Reduced by half for MLFlow

    bp = ax.boxplot(data_to_plot,
                    labels=[m.replace('_', '\n').title() for m in metric_cols],
                    patch_artist=True,
                    notch=True,
                    showmeans=True,
                    meanprops={'marker': 'D', 'markerfacecolor': 'red',
                              'markeredgecolor': 'red', 'markersize': 8},
                    medianprops={'linewidth': 2, 'color': 'darkblue'},
                    boxprops={'linewidth': 1.5},
                    whiskerprops={'linewidth': 1.5},
                    capprops={'linewidth': 1.5})

    # Color boxes with gradient
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(metric_cols)))
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax.set_ylabel('Score (%)', fontweight='bold', fontsize=6)
    ax.set_title('Distribution of Metrics Across CV Folds (Stability Analysis)',
                 fontsize=7, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim([0, 105])

    # Add legend
    from matplotlib.lines import Line2D
    from matplotlib.patches import Patch
    legend_elements = [
        Line2D([0], [0], color='darkblue', linewidth=2, label='Median'),
        Line2D([0], [0], marker='D', color='w', markerfacecolor='red',
               markersize=8, label='Mean'),
        Patch(facecolor='gray', alpha=0.3, label='IQR (25%-75%)')
    ]
    ax.legend(handles=legend_elements, loc='lower right', fontsize=5)

    # Add stability assessment
    max_std = df[metric_cols].std().max() * 100
    if max_std < 1:
        stability = "Excellent"
        color = '#2ecc71'
    elif max_std < 2:
        stability = "Good"
        color = '#f39c12'
    else:
        stability = "Moderate"
        color = '#e74c3c'

    textstr = f'Stability: {stability}\nMax StdDev: {max_std:.2f}%'
    props = dict(boxstyle='round', facecolor=color, alpha=0.3, edgecolor=color, linewidth=2)
    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=6,
            verticalalignment='top', bbox=props, fontweight='bold')

    plt.tight_layout()
    output_file = output_dir / "cv_stability_boxplot.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_confusion_matrix_holdout(metrics, output_dir):
    """Plot confusion matrix from hold-out test set"""
    if 'holdout' not in metrics:
        print("✗ No hold-out data found")
        return

    holdout_df = metrics['holdout']

    # Extract confusion matrix values
    try:
        tp = int(holdout_df[holdout_df['metric'] == 'true_positives']['value'].values[0])
        tn = int(holdout_df[holdout_df['metric'] == 'true_negatives']['value'].values[0])
        fp = int(holdout_df[holdout_df['metric'] == 'false_positives']['value'].values[0])
        fn = int(holdout_df[holdout_df['metric'] == 'false_negatives']['value'].values[0])
    except:
        print("✗ Could not extract confusion matrix values")
        return

    cm = np.array([[tn, fp], [fn, tp]])
    total = cm.sum()

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))  # Reduced by half for MLFlow

    # Plot 1: Confusion Matrix (counts)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted\nNegative (0)', 'Predicted\nPositive (1)'],
                yticklabels=['Actual\nNegative (0)', 'Actual\nPositive (1)'],
                ax=ax1, cbar_kws={'label': 'Count'},
                annot_kws={'fontsize': 8, 'fontweight': 'bold'})

    ax1.set_title('Confusion Matrix - Hold-out Test (Counts)',
                 fontsize=7, fontweight='bold', pad=5)

    # Add percentage annotations
    for i in range(2):
        for j in range(2):
            percentage = cm[i, j] / total * 100
            ax1.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                   ha='center', va='center', fontsize=6, color='gray')

    # Plot 2: Normalized Confusion Matrix (percentages)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    sns.heatmap(cm_norm, annot=True, fmt='.1f', cmap='RdYlGn',
                xticklabels=['Predicted\nNegative (0)', 'Predicted\nPositive (1)'],
                yticklabels=['Actual\nNegative (0)', 'Actual\nPositive (1)'],
                ax=ax2, cbar_kws={'label': 'Percentage (%)'},
                annot_kws={'fontsize': 8, 'fontweight': 'bold'})

    ax2.set_title('Normalized Confusion Matrix (Row %)',
                 fontsize=7, fontweight='bold', pad=5)

    # Add performance metrics as text
    accuracy = (tp + tn) / total * 100
    precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    textstr = '\n'.join([
        'Performance:',
        f'Accuracy:  {accuracy:.2f}%',
        f'Precision: {precision:.2f}%',
        f'Recall:    {recall:.2f}%',
        f'F1-Score:  {f1:.2f}%'
    ])
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5, edgecolor='black', linewidth=2)
    ax2.text(1.35, 0.5, textstr, transform=ax2.transAxes, fontsize=6,
            verticalalignment='center', bbox=props, fontfamily='monospace')

    plt.tight_layout()
    output_file = output_dir / "confusion_matrix_holdout.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_metrics_radar(metrics, output_dir):
    """Plot radar chart comparing CV and hold-out metrics"""
    if 'cv_summary' not in metrics or 'holdout' not in metrics:
        return

    cv_df = metrics['cv_summary']
    holdout_df = metrics['holdout']

    # Select metrics for radar
    radar_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    cv_values = []
    holdout_values = []
    categories = []

    for metric in radar_metrics:
        cv_row = cv_df[cv_df['metric'] == metric]
        holdout_row = holdout_df[holdout_df['metric'] == metric]

        if not cv_row.empty and not holdout_row.empty:
            cv_values.append(cv_row['mean'].values[0])
            holdout_values.append(holdout_row['value'].values[0])
            categories.append(metric.replace('_', ' ').title())

    if not categories:
        return

    fig, ax = plt.subplots(figsize=(4, 4), subplot_kw=dict(projection='polar'))  # Reduced by half for MLFlow

    # Number of variables
    N = len(categories)

    # Compute angle for each axis
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    # Plot data (close the loop)
    cv_plot = cv_values + cv_values[:1]
    holdout_plot = holdout_values + holdout_values[:1]

    ax.plot(angles, cv_plot, 'o-', linewidth=3, label='CV Mean',
            color='#2ecc71', markersize=10)
    ax.fill(angles, cv_plot, alpha=0.25, color='#2ecc71')

    ax.plot(angles, holdout_plot, 'o-', linewidth=3, label='Hold-out Test',
            color='#3498db', markersize=10)
    ax.fill(angles, holdout_plot, alpha=0.25, color='#3498db')

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, size=6, fontweight='bold')
    ax.set_ylim(0, 1)
    ax.set_title('Model Performance Radar Chart',
                 size=7, fontweight='bold', pad=10)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=6)
    ax.grid(True, linewidth=1.5, alpha=0.5)

    # Add concentric circles labels
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'], fontsize=5)

    plt.tight_layout()
    output_file = output_dir / "metrics_radar.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_hyperparameters_summary(metrics, output_dir):
    """Plot best hyperparameters from grid search"""
    if 'hyperparameters' not in metrics:
        print("✗ No hyperparameters data found (Grid Search was likely disabled)")
        return

    hp_df = metrics['hyperparameters']

    fig, ax = plt.subplots(figsize=(4, max(2.5, len(hp_df) * 0.2)))  # Reduced by half for MLFlow

    # Create horizontal bar chart
    y_pos = np.arange(len(hp_df))

    # Format values for display
    hp_df['display_value'] = hp_df['value'].astype(str)

    # Create bars (just for visual representation, all same length)
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(hp_df)))
    ax.barh(y_pos, [1] * len(hp_df), color=colors, alpha=0.7,
            edgecolor='black', linewidth=1.5)

    ax.set_yticks(y_pos)
    ax.set_yticklabels(hp_df['parameter'], fontsize=6, fontweight='bold')
    ax.set_xlim([0, 1.5])
    ax.set_xticks([])  # Remove x-axis ticks

    # Add value labels
    for i, (_, row) in enumerate(hp_df.iterrows()):
        ax.text(0.5, i, f"  {row['display_value']}",
               va='center', ha='center', fontsize=6,
               fontweight='bold', color='white',
               bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.7))

    ax.set_title('Best Hyperparameters from Grid Search',
                 fontsize=7, fontweight='bold', pad=5)
    ax.set_xlabel('')

    # Add note
    note = "These are the optimal hyperparameters found through grid search\nbased on cross-validation performance"
    ax.text(0.5, -0.15, note, transform=ax.transAxes,
           fontsize=5, ha='center', style='italic', color='gray')

    plt.tight_layout()
    output_file = output_dir / "best_hyperparameters.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_per_class_recall(metrics, output_dir):
    """Plot RECd (Recall Delayed) and RECo (Recall On-time) comparison"""
    if 'holdout' not in metrics:
        print("✗ No hold-out data found for per-class recall")
        return

    holdout_df = metrics['holdout']

    try:
        # Extract recall values for both classes
        recall_delayed = float(holdout_df[holdout_df['metric'] == 'recall_delayed']['value'].values[0])
        recall_ontime = float(holdout_df[holdout_df['metric'] == 'recall_ontime']['value'].values[0])
    except Exception as e:
        print(f"✗ Could not extract per-class recall: {e}")
        return

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))

    # Plot 1: Bar chart comparison
    classes = ['Delayed\n(Class 1)', 'On-time\n(Class 0)']
    recalls = [recall_delayed * 100, recall_ontime * 100]
    colors = ['#e74c3c', '#2ecc71']

    bars = ax1.bar(classes, recalls, color=colors, alpha=0.8, edgecolor='black', linewidth=2)

    # Add value labels
    for bar, recall in zip(bars, recalls):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{recall:.2f}%',
                ha='center', va='bottom', fontsize=7, fontweight='bold')

    ax1.set_ylabel('Recall (%)', fontweight='bold', fontsize=7)
    ax1.set_title('Per-Class Recall Comparison', fontsize=8, fontweight='bold')
    ax1.set_ylim([0, 105])
    ax1.grid(axis='y', alpha=0.3)
    ax1.axhline(y=50, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='50% baseline')
    ax1.legend(fontsize=6)

    # Add performance assessment
    avg_recall = (recall_delayed + recall_ontime) / 2
    if avg_recall >= 80:
        performance = "Excellent"
        color = '#2ecc71'
    elif avg_recall >= 70:
        performance = "Good"
        color = '#f39c12'
    else:
        performance = "Needs Improvement"
        color = '#e74c3c'

    textstr = f'Average: {avg_recall:.2f}%\nBalance: {performance}'
    props = dict(boxstyle='round', facecolor=color, alpha=0.3, edgecolor=color, linewidth=2)
    ax1.text(0.98, 0.98, textstr, transform=ax1.transAxes, fontsize=6,
            verticalalignment='top', horizontalalignment='right',
            bbox=props, fontweight='bold')

    # Plot 2: Donut chart for visual representation
    recalls_data = [recall_delayed, recall_ontime]
    recalls_labels = [f'RECd\n{recall_delayed*100:.1f}%', f'RECo\n{recall_ontime*100:.1f}%']

    wedges, texts, autotexts = ax2.pie(recalls_data, labels=recalls_labels,
                                         colors=colors, autopct='',
                                         startangle=90, wedgeprops=dict(width=0.5, edgecolor='white', linewidth=2))

    for text in texts:
        text.set_fontsize(7)
        text.set_fontweight('bold')

    ax2.set_title('Recall Distribution by Class', fontsize=8, fontweight='bold')

    # Add center text
    ax2.text(0, 0, f'Balanced\nRecall', ha='center', va='center',
            fontsize=7, fontweight='bold', color='#34495e')

    plt.tight_layout()
    output_file = output_dir / "per_class_recall.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_roc_curve(metrics, output_dir):
    """Plot ROC curve from hold-out test set predictions"""
    if 'roc_data' not in metrics:
        print("✗ No ROC data found")
        return

    roc_df = metrics['roc_data']

    # Check required columns
    if not all(col in roc_df.columns for col in ['label', 'prob_positive']):
        print(f"✗ ROC data missing required columns. Found: {roc_df.columns.tolist()}")
        return

    print(f"  - Records: {len(roc_df):,}")
    print(f"  - Positive class: {(roc_df['label'] == 1).sum():,} samples")
    print(f"  - Negative class: {(roc_df['label'] == 0).sum():,} samples")

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(roc_df['label'], roc_df['prob_positive'])
    roc_auc = auc(fpr, tpr)

    print(f"\n✓ ROC Analysis:")
    print(f"  - AUC: {roc_auc:.4f}")
    print(f"  - Number of thresholds: {len(thresholds)}")

    # Create figure
    fig, ax = plt.subplots(figsize=(4, 3))  # Reduced by half for MLFlow

    # Plot ROC curve
    ax.plot(fpr, tpr, color='#3498db', lw=3,
            label=f'ROC Curve (AUC = {roc_auc:.4f})', zorder=3)

    # Plot diagonal (random classifier)
    ax.plot([0, 1], [0, 1], color='#e74c3c', lw=2, linestyle='--',
            label='Random Classifier (AUC = 0.5000)', zorder=2)

    # Styling
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate (FPR)', fontweight='bold', fontsize=6)
    ax.set_ylabel('True Positive Rate (TPR / Recall)', fontweight='bold', fontsize=6)
    ax.set_title('ROC Curve - Hold-out Test Set', fontsize=7, fontweight='bold', pad=5)
    ax.legend(loc="lower right", fontsize=5, frameon=True, shadow=True)
    ax.grid(alpha=0.3, zorder=1)

    # Add performance zones
    ax.fill_between([0, 1], [0, 1], 1, alpha=0.08, color='green', zorder=0)  # Good zone
    ax.fill_between([0, 1], [0, 1], 0, alpha=0.08, color='red', zorder=0)    # Poor zone

    # Find optimal threshold (Youden's J statistic)
    j_scores = tpr - fpr
    optimal_idx = np.argmax(j_scores)
    optimal_threshold = thresholds[optimal_idx]
    optimal_fpr = fpr[optimal_idx]
    optimal_tpr = tpr[optimal_idx]

    print(f"\n✓ Optimal Threshold (Youden's J):")
    print(f"  - Threshold: {optimal_threshold:.4f}")
    print(f"  - TPR (Recall): {optimal_tpr:.4f}")
    print(f"  - FPR: {optimal_fpr:.4f}")
    print(f"  - J-statistic: {j_scores[optimal_idx]:.4f}")

    # Plot optimal point
    ax.plot(optimal_fpr, optimal_tpr, 'ro', markersize=12,
            label=f'Optimal Threshold = {optimal_threshold:.4f}', zorder=4)

    # Annotate optimal point
    ax.annotate(f'Optimal Point\n(FPR={optimal_fpr:.3f}, TPR={optimal_tpr:.3f})',
               xy=(optimal_fpr, optimal_tpr),
               xytext=(min(optimal_fpr + 0.2, 0.85), max(optimal_tpr - 0.2, 0.15)),
               fontsize=5,
               bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),
               arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3',
                             lw=1, color='red'),
               zorder=5)

    # Add AUC interpretation box
    if roc_auc >= 0.9:
        performance = "Excellent"
        color = '#2ecc71'
    elif roc_auc >= 0.8:
        performance = "Good"
        color = '#f39c12'
    elif roc_auc >= 0.7:
        performance = "Fair"
        color = '#e67e22'
    else:
        performance = "Poor"
        color = '#e74c3c'

    textstr = f'AUC = {roc_auc:.4f}\nPerformance: {performance}'
    props = dict(boxstyle='round', facecolor=color, alpha=0.3,
                edgecolor=color, linewidth=2)
    ax.text(0.98, 0.02, textstr, transform=ax.transAxes, fontsize=6,
            verticalalignment='bottom', horizontalalignment='right',
            bbox=props, fontweight='bold', zorder=6)

    plt.tight_layout()
    output_file = output_dir / "roc_curve_holdout.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"\n✓ Saved: {output_file}")
    plt.close()

def generate_comprehensive_report(metrics, output_dir):
    """Generate comprehensive text summary report"""
    report = []
    report.append("=" * 80)
    report.append("ML PIPELINE COMPREHENSIVE PERFORMANCE REPORT")
    report.append("=" * 80)
    report.append("")

    # Hyperparameters (if available)
    if 'hyperparameters' in metrics:
        hp_df = metrics['hyperparameters']
        report.append("Best Hyperparameters (Grid Search):")
        report.append("-" * 80)
        for _, row in hp_df.iterrows():
            report.append(f"  {row['parameter']:30s}: {row['value']}")
        report.append("")

    # Cross-Validation Summary
    if 'cv_summary' in metrics:
        cv_df = metrics['cv_summary']
        report.append("Cross-Validation Results (K-Fold):")
        report.append("-" * 80)
        report.append(f"{'Metric':<20s} {'Mean':>12s} {'Std Dev':>12s} {'95% CI':>20s}")
        report.append("-" * 80)
        for _, row in cv_df.iterrows():
            mean_pct = row['mean'] * 100
            std_pct = row['std'] * 100
            ci_lower = mean_pct - 1.96 * std_pct
            ci_upper = mean_pct + 1.96 * std_pct
            report.append(f"{row['metric']:<20s} {mean_pct:>11.2f}% "
                         f"±{std_pct:>10.2f}% "
                         f"[{ci_lower:>6.2f}%, {ci_upper:>6.2f}%]")
        report.append("")

    # Per-Fold Results
    if 'cv_folds' in metrics:
        df_folds = metrics['cv_folds']
        num_folds = len(df_folds)
        report.append(f"Per-Fold Results ({num_folds} folds):")
        report.append("-" * 80)
        report.append(f"{'Fold':>6s} {'Accuracy':>12s} {'Precision':>12s} "
                     f"{'Recall':>12s} {'F1-Score':>12s} {'AUC-ROC':>12s}")
        report.append("-" * 80)

        for _, row in df_folds.iterrows():
            report.append(f"{int(row['fold']):>6d} "
                         f"{row['accuracy']*100:>11.2f}% "
                         f"{row['precision']*100:>11.2f}% "
                         f"{row['recall']*100:>11.2f}% "
                         f"{row['f1_score']*100:>11.2f}% "
                         f"{row['auc_roc']:>12.4f}")
        report.append("-" * 80)
        report.append("")

    # Hold-out Test Results
    if 'holdout' in metrics:
        holdout_df = metrics['holdout']
        report.append("Hold-out Test Set Results:")
        report.append("-" * 80)

        # Performance metrics
        metric_rows = holdout_df[~holdout_df['metric'].str.contains('positive|negative')]
        for _, row in metric_rows.iterrows():
            value = float(row['value'])
            if row['metric'] in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'auc_pr',
                                  'recall_delayed', 'recall_ontime']:
                metric_name = row['metric'].replace('recall_delayed', 'RECd (Delayed)').replace('recall_ontime', 'RECo (On-time)')
                report.append(f"  {metric_name:20s}: {value*100:>8.2f}%")
            else:
                report.append(f"  {row['metric']:20s}: {value:>8.4f}")
        report.append("")

        # Confusion Matrix
        report.append("Confusion Matrix:")
        report.append("-" * 80)
        try:
            tp = int(holdout_df[holdout_df['metric'] == 'true_positives']['value'].values[0])
            tn = int(holdout_df[holdout_df['metric'] == 'true_negatives']['value'].values[0])
            fp = int(holdout_df[holdout_df['metric'] == 'false_positives']['value'].values[0])
            fn = int(holdout_df[holdout_df['metric'] == 'false_negatives']['value'].values[0])

            report.append(f"  True Positives (TP)  : {tp:>10,d}")
            report.append(f"  True Negatives (TN)  : {tn:>10,d}")
            report.append(f"  False Positives (FP) : {fp:>10,d}")
            report.append(f"  False Negatives (FN) : {fn:>10,d}")
            report.append("")

            total = tp + tn + fp + fn
            report.append(f"  Total Predictions    : {total:>10,d}")
            report.append("")
        except:
            report.append("  [Could not extract confusion matrix values]")
            report.append("")

    # Generalization Analysis
    if 'cv_summary' in metrics and 'holdout' in metrics:
        cv_df = metrics['cv_summary']
        holdout_df = metrics['holdout']

        report.append("Generalization Analysis (CV vs Hold-out):")
        report.append("-" * 80)
        report.append(f"{'Metric':<20s} {'CV Mean':>12s} {'Hold-out':>12s} {'Gap':>12s}")
        report.append("-" * 80)

        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:
            cv_row = cv_df[cv_df['metric'] == metric]
            ho_row = holdout_df[holdout_df['metric'] == metric]

            if not cv_row.empty and not ho_row.empty:
                cv_val = cv_row['mean'].values[0] * 100
                ho_val = ho_row['value'].values[0] * 100
                gap = cv_val - ho_val

                gap_status = "✓" if abs(gap) < 2 else "⚠" if abs(gap) < 5 else "✗"
                report.append(f"  {metric:<18s} {cv_val:>11.2f}% {ho_val:>11.2f}% "
                             f"{gap:>+10.2f}% {gap_status}")

        report.append("")
        report.append("  Gap Legend:")
        report.append("    ✓ Excellent generalization (gap < 2%)")
        report.append("    ⚠ Good generalization (2% ≤ gap < 5%)")
        report.append("    ✗ Potential overfitting (gap ≥ 5%)")
        report.append("")

    # Stability Assessment
    if 'cv_folds' in metrics:
        df_folds = metrics['cv_folds']
        report.append("Model Stability Assessment:")
        report.append("-" * 80)

        for metric in ['accuracy', 'f1_score', 'auc_roc']:
            std = df_folds[metric].std() * 100
            report.append(f"  {metric.replace('_', ' ').title():20s} StdDev: {std:.2f}%")

        max_std = df_folds[['accuracy', 'f1_score', 'auc_roc']].std().max() * 100

        report.append("")
        if max_std < 1:
            report.append("  ✓ Excellent stability - Very consistent across folds")
        elif max_std < 2:
            report.append("  ✓ Good stability - Reliable performance")
        elif max_std < 5:
            report.append("  ⚠ Moderate stability - Some variance across folds")
        else:
            report.append("  ✗ Poor stability - High variance, consider more data")
        report.append("")

    report.append("=" * 80)

    # Write to file
    output_file = output_dir / "ml_pipeline_report.txt"
    with open(output_file, 'w') as f:
        f.write('\n'.join(report))

    print(f"✓ Saved: {output_file}")

    # Also print to console
    print("\n" + '\n'.join(report))

def main():
    if len(sys.argv) < 2:
        print("Usage: python visualize_ml_pipeline.py <metrics_directory>")
        print("Example: python visualize_ml_pipeline.py /output/exp1_rf_pca/metrics")
        sys.exit(1)

    metrics_dir = Path(sys.argv[1])

    if not metrics_dir.exists():
        print(f"✗ Error: Directory not found: {metrics_dir}")
        sys.exit(1)

    print("=" * 80)
    print("ML Pipeline - Comprehensive Metrics Visualization")
    print("=" * 80)
    print(f"\nMetrics directory: {metrics_dir}\n")

    # Load metrics
    print("Loading metrics...")
    metrics = load_ml_pipeline_metrics(metrics_dir)

    if not metrics:
        print("✗ No metrics files found!")
        print("\nExpected files:")
        print("  - cv_fold_metrics.csv")
        print("  - cv_summary.csv")
        print("  - holdout_test_metrics.csv")
        print("  - best_hyperparameters.csv (optional)")
        print("  - holdout_roc_data.csv (optional)")
        sys.exit(1)

    print(f"\n✓ Loaded {len(metrics)} metric files\n")

    # Create output directory for plots
    output_dir = metrics_dir / "plots-ml-pipeline"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Saving plots to: {output_dir}\n")

    # Generate visualizations
    print("Generating visualizations...")
    plot_cv_folds_detailed(metrics, output_dir)
    plot_cv_vs_holdout_comparison(metrics, output_dir)
    plot_box_plot_stability(metrics, output_dir)
    plot_confusion_matrix_holdout(metrics, output_dir)
    plot_metrics_radar(metrics, output_dir)
    plot_per_class_recall(metrics, output_dir)
    plot_hyperparameters_summary(metrics, output_dir)
    plot_roc_curve(metrics, output_dir)

    # Generate comprehensive report
    print("\nGenerating comprehensive report...")
    generate_comprehensive_report(metrics, output_dir)

    print("\n" + "=" * 80)
    print("✓ Visualization complete!")
    print(f"✓ All results saved in: {output_dir}")
    print("=" * 80)
    print("\nGenerated files:")
    print("  - cv_folds_detailed.png          : Metrics across CV folds")
    print("  - cv_vs_holdout_comparison.png   : CV vs Hold-out comparison")
    print("  - cv_stability_boxplot.png       : Stability analysis (box plots)")
    print("  - confusion_matrix_holdout.png   : Confusion matrix (counts & normalized)")
    print("  - metrics_radar.png              : Performance radar chart")
    print("  - per_class_recall.png           : Per-class recall (RECd & RECo)")
    print("  - roc_curve_holdout.png          : ROC curve with optimal threshold")
    print("  - best_hyperparameters.png       : Best hyperparameters (if available)")
    print("  - ml_pipeline_report.txt         : Comprehensive text report")
    print()

if __name__ == "__main__":
    main()
