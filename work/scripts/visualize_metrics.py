#!/usr/bin/env python3
"""
Visualization script for Flight Delay Prediction Model Metrics

This script loads CSV metrics generated by the Scala ML pipeline and creates
comprehensive visualizations using matplotlib and seaborn.

Usage:
    python visualize_metrics.py <metrics_directory>

Example:
    python visualize_metrics.py /output/metrics/flight_delay_rf_randomforest
"""

import sys
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

def load_metrics(metrics_dir):
    """Load all metrics CSV files from directory"""
    metrics_dir = Path(metrics_dir)

    metrics = {}

    # Load train/test comparison
    comparison_file = metrics_dir / "train_test_comparison.csv"
    if comparison_file.exists():
        metrics['comparison'] = pd.read_csv(comparison_file)
        print(f"✓ Loaded: {comparison_file}")

    # Load confusion matrices
    for split in ['train', 'test']:
        cm_file = metrics_dir / f"confusion_matrix_{split}.csv"
        if cm_file.exists():
            metrics[f'confusion_matrix_{split}'] = pd.read_csv(cm_file, index_col=0)
            print(f"✓ Loaded: {cm_file}")

    # Load feature importance
    fi_file = metrics_dir / "feature_importance.csv"
    if fi_file.exists():
        metrics['feature_importance'] = pd.read_csv(fi_file)
        print(f"✓ Loaded: {fi_file}")

    return metrics

def plot_train_test_comparison(metrics, output_dir):
    """Plot train vs test metrics comparison"""
    if 'comparison' not in metrics:
        print("⚠ No comparison data found")
        return

    df = metrics['comparison']

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Plot 1: Bar chart of train vs test
    ax1 = axes[0]
    x = np.arange(len(df))
    width = 0.35

    ax1.bar(x - width/2, df['train'], width, label='Train', alpha=0.8, color='#2ecc71')
    ax1.bar(x + width/2, df['test'], width, label='Test', alpha=0.8, color='#3498db')

    ax1.set_xlabel('Metric', fontweight='bold')
    ax1.set_ylabel('Score', fontweight='bold')
    ax1.set_title('Train vs Test Metrics Comparison', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(df['metric'], rotation=45, ha='right')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)

    # Plot 2: Gap analysis
    ax2 = axes[1]
    colors = ['#e74c3c' if gap > 0.05 else '#95a5a6' for gap in df['gap']]
    ax2.barh(df['metric'], df['gap'], color=colors, alpha=0.8)
    ax2.set_xlabel('Gap (Train - Test)', fontweight='bold')
    ax2.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')
    ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    ax2.axvline(x=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5, label='5% threshold')
    ax2.legend()
    ax2.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    output_file = output_dir / "train_test_comparison.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_confusion_matrices(metrics, output_dir):
    """Plot confusion matrices for train and test sets"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    for idx, split in enumerate(['train', 'test']):
        key = f'confusion_matrix_{split}'
        if key not in metrics:
            continue

        cm = metrics[key]
        ax = axes[idx]

        # Create heatmap
        sns.heatmap(cm.values, annot=True, fmt='d', cmap='Blues',
                   xticklabels=cm.columns, yticklabels=cm.index,
                   ax=ax, cbar_kws={'label': 'Count'})

        ax.set_title(f'Confusion Matrix - {split.capitalize()}',
                    fontsize=14, fontweight='bold')
        ax.set_ylabel('Actual', fontweight='bold')
        ax.set_xlabel('Predicted', fontweight='bold')

    plt.tight_layout()
    output_file = output_dir / "confusion_matrices.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_feature_importance(metrics, output_dir, top_n=30):
    """Plot top N feature importances"""
    if 'feature_importance' not in metrics:
        print("⚠ No feature importance data found")
        return

    df = metrics['feature_importance'].copy()
    df = df.sort_values('importance', ascending=False).head(top_n)

    fig, ax = plt.subplots(figsize=(10, 12))

    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(df)))
    ax.barh(range(len(df)), df['importance'], color=colors, alpha=0.8)
    ax.set_yticks(range(len(df)))
    ax.set_yticklabels([f"Feature {int(idx)}" for idx in df['feature_index']])
    ax.invert_yaxis()
    ax.set_xlabel('Importance', fontweight='bold')
    ax.set_title(f'Top {top_n} Feature Importances', fontsize=14, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)

    # Add values on bars
    for i, (idx, row) in enumerate(df.iterrows()):
        ax.text(row['importance'], i, f" {row['importance']:.4f}",
               va='center', fontsize=8)

    plt.tight_layout()
    output_file = output_dir / "feature_importance.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def plot_metrics_radar(metrics, output_dir):
    """Plot radar chart of test metrics"""
    if 'comparison' not in metrics:
        return

    df = metrics['comparison']

    # Select metrics for radar chart (remove auc_pr which might be on different scale)
    radar_metrics = df[df['metric'].isin(['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc'])].copy()

    if len(radar_metrics) == 0:
        return

    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

    # Number of variables
    categories = radar_metrics['metric'].tolist()
    N = len(categories)

    # Compute angle for each axis
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    # Plot data
    train_values = radar_metrics['train'].tolist()
    train_values += train_values[:1]

    test_values = radar_metrics['test'].tolist()
    test_values += test_values[:1]

    ax.plot(angles, train_values, 'o-', linewidth=2, label='Train', color='#2ecc71')
    ax.fill(angles, train_values, alpha=0.25, color='#2ecc71')

    ax.plot(angles, test_values, 'o-', linewidth=2, label='Test', color='#3498db')
    ax.fill(angles, test_values, alpha=0.25, color='#3498db')

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, size=10)
    ax.set_ylim(0, 1)
    ax.set_title('Model Performance Radar Chart', size=14, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    ax.grid(True)

    plt.tight_layout()
    output_file = output_dir / "metrics_radar.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {output_file}")
    plt.close()

def generate_summary_report(metrics, output_dir):
    """Generate a text summary report"""
    if 'comparison' not in metrics:
        return

    df = metrics['comparison']

    report = []
    report.append("=" * 80)
    report.append("MODEL PERFORMANCE SUMMARY REPORT")
    report.append("=" * 80)
    report.append("")

    # Test metrics
    report.append("Test Set Performance:")
    report.append("-" * 40)
    for _, row in df.iterrows():
        report.append(f"{row['metric']:20s}: {row['test']:8.4f}")
    report.append("")

    # Overfitting analysis
    report.append("Overfitting Analysis:")
    report.append("-" * 40)
    max_gap = df['gap'].max()
    avg_gap = df['gap'].mean()
    report.append(f"Maximum gap:         {max_gap:8.4f}")
    report.append(f"Average gap:         {avg_gap:8.4f}")

    if max_gap > 0.10:
        report.append("⚠ WARNING: Significant overfitting detected (gap > 10%)")
    elif max_gap > 0.05:
        report.append("⚠ Moderate overfitting detected (gap > 5%)")
    else:
        report.append("✓ Model generalizes well")
    report.append("")

    # Feature importance summary
    if 'feature_importance' in metrics:
        fi_df = metrics['feature_importance']
        top_features = fi_df.nlargest(10, 'importance')
        report.append("Top 10 Most Important Features:")
        report.append("-" * 40)
        for _, row in top_features.iterrows():
            report.append(f"Feature {int(row['feature_index']):3d}: {row['importance']:8.6f}")
        report.append("")

    report.append("=" * 80)

    # Write to file
    output_file = output_dir / "summary_report.txt"
    with open(output_file, 'w') as f:
        f.write('\n'.join(report))

    print(f"✓ Saved: {output_file}")

    # Also print to console
    print("\n" + '\n'.join(report))

def main():
    if len(sys.argv) < 2:
        print("Usage: python visualize_metrics.py <metrics_directory>")
        print("Example: python visualize_metrics.py /output/metrics/flight_delay_rf_randomforest")
        sys.exit(1)

    metrics_dir = Path(sys.argv[1])

    if not metrics_dir.exists():
        print(f"✗ Error: Directory not found: {metrics_dir}")
        sys.exit(1)

    print("=" * 80)
    print("Flight Delay Prediction - Metrics Visualization")
    print("=" * 80)
    print(f"\nMetrics directory: {metrics_dir}\n")

    # Load metrics
    print("Loading metrics...")
    metrics = load_metrics(metrics_dir)

    if not metrics:
        print("✗ No metrics files found!")
        sys.exit(1)

    print(f"\n✓ Loaded {len(metrics)} metric files\n")

    # Create output directory for plots
    output_dir = metrics_dir / "plots"
    output_dir.mkdir(exist_ok=True)
    print(f"Saving plots to: {output_dir}\n")

    # Generate visualizations
    print("Generating visualizations...")
    plot_train_test_comparison(metrics, output_dir)
    plot_confusion_matrices(metrics, output_dir)
    plot_feature_importance(metrics, output_dir)
    plot_metrics_radar(metrics, output_dir)

    # Generate summary report
    print("\nGenerating summary report...")
    generate_summary_report(metrics, output_dir)

    print("\n" + "=" * 80)
    print("✓ Visualization complete!")
    print("=" * 80)

if __name__ == "__main__":
    main()
