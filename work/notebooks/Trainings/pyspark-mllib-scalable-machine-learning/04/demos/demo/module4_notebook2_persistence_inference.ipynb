{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c2a68b-c078-44f4-a786-ef27afdad54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 4 Notebook 2: Model Persistence and Distributed Inference\n",
    "\n",
    "**Objective:** Demonstrate how to load a saved PySpark ML Pipeline (persisted using MLflow in M4N1) and use it for scalable batch inference on new, unseen data.\n",
    "\n",
    "**Recap:** In M4N1, we built an end-to-end `Pipeline` for our regression task and saved the resulting `PipelineModel` using MLflow, logging its `run_id`.\n",
    "\n",
    "**Goal:** Load the saved `PipelineModel` and apply it to new raw data to generate predictions, showcasing the inference process and discussing Spark's ability to handle this at scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69eba820-1533-4fe9-b77b-ca091b81d6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml import PipelineModel\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "# MLflow details from M4N1\n",
    "MLFLOW_RUN_ID = \"2a473aa8df5f45209a4dd93b22d7fc63\"\n",
    "MLFLOW_ARTIFACT_PATH = \"ecommerce_regression_model\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2e61b8-2c1f-48a8-9a05-e26b539488a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load the Trained Pipeline Model via MLflow\n",
    "\n",
    "We use the `run_id` and artifact path logged during the M4N1 execution to load the complete `PipelineModel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c430243b-a767-4a38-9608-fa29fec2cf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: runs:/2a473aa8df5f45209a4dd93b22d7fc63/ecommerce_regression_model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/21 01:55:31 INFO mlflow.spark: 'runs:/2a473aa8df5f45209a4dd93b22d7fc63/ecommerce_regression_model' resolved as 'dbfs:/databricks/mlflow-tracking/232784118800640/2a473aa8df5f45209a4dd93b22d7fc63/artifacts/ecommerce_regression_model'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361a04f08f284a32a3a856e22dfc9862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e655d37214d41d4ac5f85c93e7f5af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel loaded successfully!\nType: <class 'pyspark.ml.pipeline.PipelineModel'>\n\nStages in the loaded pipeline:\n  Stage 0: StringIndexerModel\n  Stage 1: StringIndexerModel\n  Stage 2: StringIndexerModel\n  Stage 3: StringIndexerModel\n  Stage 4: OneHotEncoderModel\n  Stage 5: VectorAssembler\n  Stage 6: StandardScalerModel\n  Stage 7: VectorAssembler\n  Stage 8: LinearRegressionModel\n"
     ]
    }
   ],
   "source": [
    "# Construct the model URI\n",
    "model_uri = f\"runs:/{MLFLOW_RUN_ID}/{MLFLOW_ARTIFACT_PATH}\"\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "\n",
    "# Load the PipelineModel\n",
    "loaded_pipeline_model = None # Initialize variable\n",
    "try:\n",
    "    loaded_pipeline_model = mlflow.spark.load_model(model_uri)\n",
    "    print(\"PipelineModel loaded successfully!\")\n",
    "    print(f\"Type: {type(loaded_pipeline_model)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure the MLFLOW_RUN_ID is correct and the model was logged successfully in M4N1.\")\n",
    "    # Stop execution if model loading fails (optional, comment out if not in Databricks)\n",
    "    # dbutils.notebook.exit(\"Model loading failed\") \n",
    "\n",
    "#Display stages of the loaded pipeline\n",
    "if loaded_pipeline_model is not None:\n",
    "    print(\"\\nStages in the loaded pipeline:\")\n",
    "    for i, stage in enumerate(loaded_pipeline_model.stages):\n",
    "        print(f'  Stage {i}: {stage.__class__.__name__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d3a4d2-cfcc-479a-b7f0-3eb6ad561911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Prepare Sample New Data\n",
    "\n",
    "Now, let's create a small batch of new, raw customer-product data. This data simulates what might arrive for prediction. It must have the same schema as the *initial* input to the pipeline created in M4N1 (before any transformations like StringIndexer or StandardScaler were applied). It should **not** contain the target variable (`total_purchase_amount`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc963bd-de6d-4d25-ab17-52919cc5a7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the new data:\nroot\n |-- customer_id: long (nullable = true)\n |-- product_id: long (nullable = true)\n |-- age: integer (nullable = true)\n |-- tenure_days: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- previous_visits: long (nullable = true)\n |-- view_count: long (nullable = true)\n |-- add_to_cart_count: long (nullable = true)\n |-- review_count: long (nullable = true)\n |-- total_interactions: long (nullable = true)\n |-- total_time_spent: long (nullable = true)\n |-- interaction_time_span_days: double (nullable = true)\n |-- avg_user_rating: double (nullable = true)\n |-- gender: string (nullable = true)\n |-- country: string (nullable = true)\n |-- membership_level: string (nullable = true)\n |-- category: string (nullable = true)\n\n\nSample new data content:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_id</th><th>age</th><th>tenure_days</th><th>price</th><th>avg_rating</th><th>previous_visits</th><th>view_count</th><th>add_to_cart_count</th><th>review_count</th><th>total_interactions</th><th>total_time_spent</th><th>interaction_time_span_days</th><th>avg_user_rating</th><th>gender</th><th>country</th><th>membership_level</th><th>category</th></tr></thead><tbody><tr><td>20001</td><td>501</td><td>65</td><td>1500</td><td>499.99</td><td>4.7</td><td>20</td><td>30</td><td>5</td><td>2</td><td>37</td><td>2500</td><td>30.0</td><td>4.5</td><td>F</td><td>US</td><td>Platinum</td><td>Electronics</td></tr><tr><td>20002</td><td>602</td><td>40</td><td>600</td><td>75.0</td><td>4.1</td><td>10</td><td>15</td><td>3</td><td>1</td><td>19</td><td>900</td><td>10.0</td><td>4.0</td><td>M</td><td>CA</td><td>Gold</td><td>Clothing</td></tr><tr><td>20003</td><td>703</td><td>25</td><td>150</td><td>19.95</td><td>3.8</td><td>4</td><td>8</td><td>2</td><td>0</td><td>10</td><td>500</td><td>5.0</td><td>3.5</td><td>Other</td><td>UK</td><td>Bronze</td><td>Books</td></tr><tr><td>20004</td><td>804</td><td>33</td><td>800</td><td>120.0</td><td>4.4</td><td>12</td><td>22</td><td>4</td><td>1</td><td>27</td><td>1500</td><td>25.5</td><td>4.2</td><td>F</td><td>DE</td><td>Silver</td><td>Home & Kitchen</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20001,
         501,
         65,
         1500,
         499.99,
         4.7,
         20,
         30,
         5,
         2,
         37,
         2500,
         30.0,
         4.5,
         "F",
         "US",
         "Platinum",
         "Electronics"
        ],
        [
         20002,
         602,
         40,
         600,
         75.0,
         4.1,
         10,
         15,
         3,
         1,
         19,
         900,
         10.0,
         4.0,
         "M",
         "CA",
         "Gold",
         "Clothing"
        ],
        [
         20003,
         703,
         25,
         150,
         19.95,
         3.8,
         4,
         8,
         2,
         0,
         10,
         500,
         5.0,
         3.5,
         "Other",
         "UK",
         "Bronze",
         "Books"
        ],
        [
         20004,
         804,
         33,
         800,
         120.0,
         4.4,
         12,
         22,
         4,
         1,
         27,
         1500,
         25.5,
         4.2,
         "F",
         "DE",
         "Silver",
         "Home & Kitchen"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tenure_days",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "previous_visits",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "view_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "add_to_cart_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "review_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_interactions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_time_spent",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "interaction_time_span_days",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_user_rating",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "membership_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the schema based on the raw features used in M4N1's input\n",
    "# Ensure this matches the columns selected in M4N1 before the pipeline\n",
    "# feature_columns = [\"age\", \"tenure_days\", ..., \"category\"]\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", LongType(), True), # Added for context\n",
    "    StructField(\"product_id\", LongType(), True),   # Added for context\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tenure_days\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"avg_rating\", DoubleType(), True), # Product avg rating\n",
    "    StructField(\"previous_visits\", LongType(), True),\n",
    "    StructField(\"view_count\", LongType(), True),\n",
    "    StructField(\"add_to_cart_count\", LongType(), True),\n",
    "    StructField(\"review_count\", LongType(), True),\n",
    "    StructField(\"total_interactions\", LongType(), True),\n",
    "    StructField(\"total_time_spent\", LongType(), True),\n",
    "    StructField(\"interaction_time_span_days\", DoubleType(), True),\n",
    "    StructField(\"avg_user_rating\", DoubleType(), True), # Customer avg rating\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"membership_level\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create sample raw data (using Pandas for convenience)\n",
    "new_data_pd = pd.DataFrame([\n",
    "    {\n",
    "        \"customer_id\": 20001, \"product_id\": 501, \"age\": 65, \"tenure_days\": 1500, \"price\": 499.99, \"avg_rating\": 4.7, # High Value\n",
    "        \"previous_visits\": 20, \"view_count\": 30, \"add_to_cart_count\": 5, \"review_count\": 2, \"total_interactions\": 37,\n",
    "        \"total_time_spent\": 2500, \"interaction_time_span_days\": 30.0, \"avg_user_rating\": 4.5,\n",
    "        \"gender\": \"F\", \"country\": \"US\", \"membership_level\": \"Platinum\", \"category\": \"Electronics\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": 20002, \"product_id\": 602, \"age\": 40, \"tenure_days\": 600, \"price\": 75.00, \"avg_rating\": 4.1, # Mid-Range\n",
    "        \"previous_visits\": 10, \"view_count\": 15, \"add_to_cart_count\": 3, \"review_count\": 1, \"total_interactions\": 19,\n",
    "        \"total_time_spent\": 900, \"interaction_time_span_days\": 10.0, \"avg_user_rating\": 4.0,\n",
    "        \"gender\": \"M\", \"country\": \"CA\", \"membership_level\": \"Gold\", \"category\": \"Clothing\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": 20003, \"product_id\": 703, \"age\": 25, \"tenure_days\": 150, \"price\": 19.95, \"avg_rating\": 3.8, # Lower-End Purchaser\n",
    "        \"previous_visits\": 4, \"view_count\": 8, \"add_to_cart_count\": 2, \"review_count\": 0, \"total_interactions\": 10,\n",
    "        \"total_time_spent\": 500, \"interaction_time_span_days\": 5.0, \"avg_user_rating\": 3.5,\n",
    "        \"gender\": \"Other\", \"country\": \"UK\", \"membership_level\": \"Bronze\", \"category\": \"Books\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": 20004, \"product_id\": 804, \"age\": 33, \"tenure_days\": 800, \"price\": 120.00, \"avg_rating\": 4.4, # Established Purchaser\n",
    "        \"previous_visits\": 12, \"view_count\": 22, \"add_to_cart_count\": 4, \"review_count\": 1, \"total_interactions\": 27,\n",
    "        \"total_time_spent\": 1500, \"interaction_time_span_days\": 25.5, \"avg_user_rating\": 4.2,\n",
    "        \"gender\": \"F\", \"country\": \"DE\", \"membership_level\": \"Silver\", \"category\": \"Home & Kitchen\"\n",
    "    }\n",
    "])\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame with the defined schema\n",
    "sample_new_data_df = spark.createDataFrame(new_data_pd, schema=schema)\n",
    "\n",
    "print(\"Schema of the new data:\")\n",
    "sample_new_data_df.printSchema()\n",
    "\n",
    "print(\"\\nSample new data content:\")\n",
    "sample_new_data_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de5d355-ae63-4e28-bb09-35ee637095da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Perform Batch Inference\n",
    "\n",
    "Now, we apply the loaded `PipelineModel` to our new data using the `.transform()` method. This single call executes all the necessary preprocessing steps (StringIndexing, OneHotEncoding, Scaling, Assembling) using the parameters learned during training, followed by the prediction step from the trained Linear Regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef75c27-8884-4919-b94f-7dcb79f33539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying loaded pipeline model to new data...\nInference complete.\n\nPredictions on new data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_id</th><th>price</th><th>category</th><th>prediction</th></tr></thead><tbody><tr><td>20001</td><td>501</td><td>499.99</td><td>Electronics</td><td>1112.6040491092283</td></tr><tr><td>20002</td><td>602</td><td>75.0</td><td>Clothing</td><td>79.93871112852068</td></tr><tr><td>20003</td><td>703</td><td>19.95</td><td>Books</td><td>0.0</td></tr><tr><td>20004</td><td>804</td><td>120.0</td><td>Home & Kitchen</td><td>300.53514159022046</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20001,
         501,
         499.99,
         "Electronics",
         1112.6040491092283
        ],
        [
         20002,
         602,
         75.0,
         "Clothing",
         79.93871112852068
        ],
        [
         20003,
         703,
         19.95,
         "Books",
         0.0
        ],
        [
         20004,
         804,
         120.0,
         "Home & Kitchen",
         300.53514159022046
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the pipeline to the new data\n",
    "print(\"Applying loaded pipeline model to new data...\")\n",
    "new_predictions_df = loaded_pipeline_model.transform(sample_new_data_df)\n",
    "print(\"Inference complete.\")\n",
    "\n",
    "# Show the results\n",
    "# Select some key input columns and the prediction\n",
    "print(\"\\nPredictions on new data:\")\n",
    "\n",
    "new_predictions_df = new_predictions_df.withColumn(\n",
    "    \"prediction\",\n",
    "    when(col(\"prediction\") < 0, 0.0).otherwise(col(\"prediction\"))\n",
    ")\n",
    "\n",
    "new_predictions_df.select(\n",
    "    \"customer_id\", \n",
    "    \"product_id\", \n",
    "    \"price\", \n",
    "    \"category\", \n",
    "    \"prediction\" # The output column from the LinearRegressionModel stage\n",
    ").limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a7c4ab-ba72-49f6-9352-9b9970e023fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Distributed Inference Discussion\n",
    "\n",
    "- **Scalability:** We used a small, manually created DataFrame here for demonstration. However, the `sample_new_data_df` could easily point to a large dataset stored in cloud storage (e.g., reading from Parquet files: `spark.read.parquet(\"/path/to/new/data/\")`). Spark would automatically distribute the `.transform()` operation across the cluster, applying the entire pipeline (preprocessing + model prediction) in parallel to partitions of the data. This allows inference to scale to terabytes or petabytes of data.\n",
    "- **Consistency:** The key benefit of using the saved `PipelineModel` is that it guarantees the *exact same* preprocessing steps (with the same fitted parameters like StringIndexer mappings and StandardScaler means/stds) are applied to the new data as were applied during training. This prevents inconsistencies and potential errors that could arise from manually reimplementing the preprocessing logic for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b81260-358a-477c-bb33-bac6655797d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we successfully:\n",
    "*   Loaded a pre-trained `PipelineModel` from MLflow using its `run_id`.\n",
    "*   Prepared sample raw input data mirroring the schema expected by the pipeline.\n",
    "*   Performed batch inference by applying the loaded `PipelineModel`'s `.transform()` method to the new data.\n",
    "*   Observed the generated predictions alongside original features.\n",
    "*   Discussed how this process scales seamlessly in Spark and ensures consistent preprocessing.\n",
    "\n",
    "**Benefits Demonstrated:**\n",
    "*   **Model Reusability:** Easily load and reuse complex, trained pipelines.\n",
    "*   **Simplified Inference:** A single `.transform()` call handles all steps.\n",
    "*   **Production Readiness:** Using MLflow for persistence provides a standard way to manage models for deployment.\n",
    "*   **Scalable Batch Prediction:** Leverage Spark's distributed nature for large datasets.\n",
    "\n",
    "**Next Steps:** In the final notebook (Module 4 Notebook 3), we will explore techniques to optimize the performance of Spark ML workloads, such as caching, partitioning, and other configuration tuning.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "module4_notebook2_persistence_inference",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}