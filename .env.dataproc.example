# ====================================================================
# Google Cloud Dataproc Configuration
# ====================================================================
# Copy this file to .env.dataproc and fill in your values
# cp .env.dataproc.example .env.dataproc
# ====================================================================

# Google Cloud Project Configuration
GCP_PROJECT_ID="your-project-id"
GCP_REGION="europe-west1"
GCP_ZONE="europe-west1-b"

# Google Cloud Storage Bucket
# Format: gs://your-bucket-name (without trailing slash)
GCS_BUCKET="gs://your-bucket-name"

# Workflow Configuration
WORKFLOW_NAME="flight-delay-workflow"

# Cluster Configuration
CLUSTER_NAME="flight-temp-cluster"
MASTER_MACHINE_TYPE="n1-standard-16"
MASTER_BOOT_DISK_SIZE="100GB"
NUM_WORKERS="2"
WORKER_MACHINE_TYPE="n1-standard-8"
WORKER_BOOT_DISK_SIZE="100GB"
IMAGE_VERSION="2.2-debian12"

# Application Configuration
CONFIG_FILE="prod-config.yml"
CONFIG_NAME="prod"

# Default tasks to run (can be overridden with command line argument)
# Options: data-pipeline, feature-extraction, train
DEFAULT_TASKS="data-pipeline,feature-extraction,train"

# Spark Configuration
SPARK_DRIVER_MEMORY="40g"
SPARK_DRIVER_CORES="10"
SPARK_DRIVER_MAX_RESULT_SIZE="8g"
SPARK_EXECUTOR_MEMORY="16g"
SPARK_EXECUTOR_CORES="6"
SPARK_SQL_SHUFFLE_PARTITIONS="400"
SPARK_DEFAULT_PARALLELISM="400"
SPARK_MEMORY_OFF_HEAP_SIZE="10g"

# JAR Files (usually don't need to change these)
APP_JAR="Emiasd-Flight-Data-Analysis-assembly-1.0.jar"
MLFLOW_CLIENT_JAR="mlflow-client-3.4.0.jar"
MLFLOW_SPARK_JAR="mlflow-spark_2.13-3.4.0.jar"
