{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98fddb7e-373e-424e-8555-4e765b7a2242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 4 Notebook 3: Performance Tuning Concepts\n",
    "\n",
    "**Objective:** Explore and demonstrate the syntax and concepts behind common techniques for optimizing the performance of PySpark ML workloads, particularly for large-scale data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9489a803-2bf1-4019-bc9c-ab95afb5a9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, avg, min, datediff, first, lit, when, expr, unix_timestamp, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f891ee6c-388e-4d3f-bc3d-26f154c5679f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Data Preparation (Baseline Reference)\n",
    "\n",
    "First, let's quickly reload and prepare the `regression_base_data` DataFrame as we did in M4N1. This will serve as the input for demonstrating the optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fccd8a-eb94-4b53-a19a-e0d12cb75e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created baseline DataFrame `regression_base_data` for demonstration.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product_id</th><th>age</th><th>country</th><th>price</th></tr></thead><tbody><tr><td>3245</td><td>252</td><td>32</td><td>US</td><td>52.16</td></tr><tr><td>5843</td><td>115</td><td>18</td><td>CA</td><td>38.71</td></tr><tr><td>8519</td><td>240</td><td>27</td><td>JP</td><td>414.86</td></tr><tr><td>7752</td><td>980</td><td>38</td><td>FR</td><td>74.04</td></tr><tr><td>317</td><td>682</td><td>43</td><td>US</td><td>177.74</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3245,
         252,
         32,
         "US",
         52.16
        ],
        [
         5843,
         115,
         18,
         "CA",
         38.71
        ],
        [
         8519,
         240,
         27,
         "JP",
         414.86
        ],
        [
         7752,
         980,
         38,
         "FR",
         74.04
        ],
        [
         317,
         682,
         43,
         "US",
         177.74
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load raw data (assuming tables exist from previous notebooks)\n",
    "interactions_df = spark.table(\"ecommerce.interactions\")\n",
    "customers_df = spark.table(\"ecommerce.customers\")\n",
    "products_df = spark.table(\"ecommerce.products\")\n",
    "\n",
    "# --- Minimal Feature Engineering Logic (from M4N1) --- \n",
    "# We only need enough to join and filter for the regression base\n",
    "window_spec = Window.partitionBy(\"customer_id\", \"product_id\").orderBy(\"timestamp\")\n",
    "\n",
    "customer_product_agg = interactions_df.groupBy(\"customer_id\", \"product_id\").agg(\n",
    "        count(when(col(\"interaction_type\") == \"purchase\", 1)).alias(\"purchase_count\") # Need purchase_count to filter\n",
    "    )\n",
    "\n",
    "# --- Join Data --- \n",
    "base_df = customer_product_agg.join(customers_df, \"customer_id\").join(products_df, \"product_id\")\n",
    "\n",
    "# --- Filter for Regression Base --- \n",
    "regression_base_data = base_df.filter(col(\"purchase_count\") > 0).select(\"customer_id\", \"product_id\", \"age\", \"country\", \"price\") # Select a few cols for demo\n",
    "\n",
    "print(\"Created baseline DataFrame `regression_base_data` for demonstration.\")\n",
    "regression_base_data.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1e127c-53b1-4962-9ebd-272fec261ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Optimization Technique 1: Caching (`.cache()` / `.persist()`)\n",
    "\n",
    "**Concept:** Caching stores the result of a DataFrame computation in memory (and potentially disk). If you use the same DataFrame multiple times in subsequent actions, Spark can reuse the cached result instead of recomputing it from scratch. This is extremely beneficial for iterative algorithms (like many ML training loops) or when branching off multiple analyses from one expensive data preparation step, **especially with large datasets**.\n",
    "\n",
    "- `.cache()` is a shortcut for `.persist(StorageLevel.MEMORY_ONLY)`.\n",
    "- `.persist(level)` allows specifying different storage levels (e.g., `MEMORY_AND_DISK`, `DISK_ONLY`). `MEMORY_AND_DISK` is often a good default, spilling to disk if the data doesn't fit entirely in memory.\n",
    "\n",
    "**Demonstration:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba9690b-a005-4b9a-b526-aabaf57b0e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting regression_base_data...\nTriggering action to populate cache (e.g., count)...\nDataFrame count (cached): 25238\nSimulating reuse (e.g., show). On large data, this would reuse the cache.\n+-----------+----------+---+-------+------+\n|customer_id|product_id|age|country| price|\n+-----------+----------+---+-------+------+\n|       3245|       252| 32|     US| 52.16|\n|       5843|       115| 18|     CA| 38.71|\n|       8519|       240| 27|     JP|414.86|\n|       7752|       980| 38|     FR| 74.04|\n|        317|       682| 43|     US|177.74|\n+-----------+----------+---+-------+------+\nonly showing top 5 rows\nUnpersisting the data to free up resources...\nData unpersisted.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate persisting the DataFrame\n",
    "print(\"Persisting regression_base_data...\")\n",
    "cached_data = regression_base_data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Explain laziness: Persist marks it, but caching happens on first action\n",
    "print(\"Triggering action to populate cache (e.g., count)...\")\n",
    "cache_count = cached_data.count()\n",
    "print(f\"DataFrame count (cached): {cache_count}\")\n",
    "\n",
    "# Explain reuse: Any subsequent action on `cached_data` *should* be faster on large data\n",
    "print(\"Simulating reuse (e.g., show). On large data, this would reuse the cache.\")\n",
    "cached_data.show(5)\n",
    "\n",
    "# Explain importance of unpersisting\n",
    "print(\"Unpersisting the data to free up resources...\")\n",
    "cached_data.unpersist()\n",
    "print(\"Data unpersisted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f986e4fa-66e2-4ab8-a6ab-a88ab8c1cc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Optimization Technique 2: Data Partitioning (`.repartition()` / `.coalesce()`)\n",
    "\n",
    "**Concept:** Spark processes data in parallel using partitions. The number of partitions can significantly impact performance.\n",
    "- **Too few partitions:** Limits parallelism, executors might sit idle.\n",
    "- **Too many partitions:** Scheduling overhead for tiny tasks, potentially inefficient data transfer.\n",
    "\n",
    "- `.repartition(N)`: Changes the number of partitions to `N`. **Involves a full shuffle** of the data across the network, which is expensive. Use when you need to increase partitions or change partitioning keys.\n",
    "- `.coalesce(N)`: Changes the number of partitions to `N` (where `N` must be *less* than the current number). **Avoids a full shuffle** by combining data on existing executors. Much more efficient if you only need to reduce partitions.\n",
    "- `repartition(N, col(\"key\"))`: Partitions data based on the hash of one or more key columns. This can drastically improve performance for subsequent joins or aggregations on those keys by colocating related data, **especially on large, skewed datasets**.\n",
    "\n",
    "**Demonstration:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7c4edc-3c79-41d4-b977-e04a7abf8a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before: 1\nDemonstrating repartition to 10 partitions...\nNumber of partitions after repartition: 10\nDemonstrating coalesce to 5 partitions...\nNumber of partitions after coalesce: 5\nDemonstrating repartition by key:\nCreated DataFrame repartitioned by 'customer_id'\n"
     ]
    }
   ],
   "source": [
    "# Get current number of partitions\n",
    "num_partitions_before = regression_base_data.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions before: {num_partitions_before}\")\n",
    "\n",
    "# Demonstrate repartition (e.g., doubling, minimum 10 for demo)\n",
    "target_repartition = max(10, num_partitions_before * 2)\n",
    "print(f\"Demonstrating repartition to {target_repartition} partitions...\")\n",
    "repartitioned_data = regression_base_data.repartition(target_repartition)\n",
    "num_partitions_after_repart = repartitioned_data.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after repartition: {num_partitions_after_repart}\")\n",
    "# Explain that this forced a shuffle (expensive on large data)\n",
    "\n",
    "# Demonstrate coalesce (e.g., halving, minimum 1)\n",
    "target_coalesce = max(1, num_partitions_after_repart // 2)\n",
    "print(f\"Demonstrating coalesce to {target_coalesce} partitions...\")\n",
    "coalesced_data = repartitioned_data.coalesce(target_coalesce)\n",
    "num_partitions_after_coal = coalesced_data.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after coalesce: {num_partitions_after_coal}\")\n",
    "# Explain that coalesce is cheaper as it avoids a full shuffle\n",
    "\n",
    "# Demonstrate repartition by key\n",
    "print(\"Demonstrating repartition by key:\")\n",
    "# Useful if we frequently joined or grouped by customer_id on large data\n",
    "partitioned_by_key_data = regression_base_data.repartition(target_repartition, col(\"customer_id\"))\n",
    "print(f\"Created DataFrame repartitioned by 'customer_id'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993e529d-d3af-469a-8518-65e27fbc5b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Optimization Technique 3: Broadcast Joins\n",
    "\n",
    "**Concept:** When joining a large DataFrame with a significantly smaller DataFrame, Spark can perform a *Broadcast Hash Join*. It sends (broadcasts) the entire small DataFrame to every executor node. The join then happens locally on each executor without requiring a massive shuffle of the large DataFrame's data across the network, which is typically required by other join strategies like Sort-Merge Join. This is often a huge performance win.\n",
    "\n",
    "- Spark attempts to do this automatically based on the `spark.sql.autoBroadcastJoinThreshold` configuration (default is often 10MB). If a table is smaller than this threshold, it's usually broadcasted.\n",
    "- You can verify which join strategy was used by examining the query plan in the Spark UI (SQL/Jobs tab).\n",
    "- You can also *hint* to Spark to broadcast a specific DataFrame using the `broadcast()` function.\n",
    "\n",
    "**Demonstration:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca9851d-6f42-45b5-bb86-9ccc9cfd030a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating broadcast hint syntax...\nHinted broadcast join complete. Check Spark plan should definitely show BroadcastHashJoin.\n== Physical Plan ==\nAdaptiveSparkPlan (10)\n+- == Initial Plan ==\n   Project (9)\n   +- BroadcastHashJoin Inner BuildRight (8)\n      :- Project (3)\n      :  +- Filter (2)\n      :     +- Scan parquet spark_catalog.ecommerce.interactions (1)\n      +- Exchange (7)\n         +- Project (6)\n            +- Filter (5)\n               +- Scan parquet spark_catalog.ecommerce.customers (4)\n\n\n(1) Scan parquet spark_catalog.ecommerce.interactions\nOutput [10]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L, _databricks_internal_edge_computed_column_skip_row#15960]\nBatched: true\nLocation: PreparedDeltaFileIndex [dbfs:/user/hive/warehouse/ecommerce.db/interactions]\nPushedFilters: [IsNotNull(customer_id)]\nReadSchema: struct<customer_id:int,product_id:bigint,timestamp:timestamp_ntz,interaction_type:string,time_spent_seconds:bigint,purchase_amount:double,user_rating:double,device:string,previous_visits:bigint,_databricks_internal_edge_computed_column_skip_row:boolean>\n\n(2) Filter\nInput [10]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L, _databricks_internal_edge_computed_column_skip_row#15960]\nCondition : (if (isnotnull(_databricks_internal_edge_computed_column_skip_row#15960)) (_databricks_internal_edge_computed_column_skip_row#15960 = false) else isnotnull(raise_error(DELTA_SKIP_ROW_COLUMN_NOT_FILLED, map(keys: [], values: []), NullType)) AND isnotnull(customer_id#15842))\n\n(3) Project\nOutput [9]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L]\nInput [10]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L, _databricks_internal_edge_computed_column_skip_row#15960]\n\n(4) Scan parquet spark_catalog.ecommerce.customers\nOutput [7]: [customer_id#15866, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871, _databricks_internal_edge_computed_column_skip_row#15961]\nBatched: true\nLocation: PreparedDeltaFileIndex [dbfs:/user/hive/warehouse/ecommerce.db/customers]\nPushedFilters: [IsNotNull(customer_id)]\nReadSchema: struct<customer_id:int,age:int,gender:string,country:string,tenure_days:int,membership_level:string,_databricks_internal_edge_computed_column_skip_row:boolean>\n\n(5) Filter\nInput [7]: [customer_id#15866, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871, _databricks_internal_edge_computed_column_skip_row#15961]\nCondition : (if (isnotnull(_databricks_internal_edge_computed_column_skip_row#15961)) (_databricks_internal_edge_computed_column_skip_row#15961 = false) else isnotnull(raise_error(DELTA_SKIP_ROW_COLUMN_NOT_FILLED, map(keys: [], values: []), NullType)) AND isnotnull(customer_id#15866))\n\n(6) Project\nOutput [6]: [customer_id#15866, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871]\nInput [7]: [customer_id#15866, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871, _databricks_internal_edge_computed_column_skip_row#15961]\n\n(7) Exchange\nInput [6]: [customer_id#15866, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871]\nArguments: SinglePartition, EXECUTOR_BROADCAST, [plan_id=10948]\n\n(8) BroadcastHashJoin\nLeft keys [1]: [customer_id#15842]\nRight keys [1]: [customer_id#15866]\nJoin type: Inner\nJoin condition: None\n\n(9) Project\nOutput [14]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871]\nInput [15]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L, customer_id#15866, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871]\n\n(10) AdaptiveSparkPlan\nOutput [14]: [customer_id#15842, product_id#15843L, timestamp#15844, interaction_type#15845, time_spent_seconds#15846L, purchase_amount#15847, user_rating#15848, device#15849, previous_visits#15850L, age#15867, gender#15868, country#15869, tenure_days#15870, membership_level#15871]\nArguments: isFinalPlan=false\n\n\n== Optimizer Statistics (table names per statistics state) ==\n  missing = customers, interactions\n  partial = \n  full    = \nCorrective actions: consider running the following command on all tables with missing or partial statistics\n  ANALYZE TABLE <table-name> COMPUTE STATISTICS FOR ALL COLUMNS\n\n"
     ]
    }
   ],
   "source": [
    "# Reload data for join demonstration\n",
    "interactions_large_df = spark.table(\"ecommerce.interactions\")\n",
    "customers_small_df = spark.table(\"ecommerce.customers\")\n",
    "\n",
    "# Demonstrate broadcast hint syntax\n",
    "print(\"Demonstrating broadcast hint syntax...\")\n",
    "hinted_join_df = interactions_large_df.join(broadcast(customers_small_df), \"customer_id\")\n",
    "hinted_join_df.count() # Action to trigger join execution\n",
    "print(\"Hinted broadcast join complete. Check Spark plan should definitely show BroadcastHashJoin.\")\n",
    "# Explain: Useful if auto-broadcast doesn't trigger or for explicit control on larger datasets.\n",
    "\n",
    "hinted_join_df.explain(mode=\"formatted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8697c645-10b6-4e7d-a5e6-2c49854db737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the syntax and underlying concepts for three key Spark performance optimization techniques:\n",
    "1.  **Caching/Persistence:** Avoiding recomputation of expensive DataFrame operations.\n",
    "2.  **Partitioning:** Controlling data distribution and parallelism using `repartition` and `coalesce`.\n",
    "3.  **Broadcast Joins:** Efficiently joining large DataFrames with small ones by avoiding shuffles.\n",
    "\n",
    "**Key Takeaway:** While the effects weren't dramatic on our small course dataset, understanding and applying these techniques is **essential** for building performant and scalable ML pipelines and data processing jobs on large, real-world datasets. Always use the **Spark UI** to monitor your job's execution, identify bottlenecks (like shuffles or stages taking too long), and verify if your optimizations are having the intended effect.\n",
    "\n",
    "This concludes Module 4 and the PySpark MLlib Course! We hope you now have a solid foundation for building scalable machine learning applications with PySpark.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "module4_notebook3_performance_tuning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}