# MLFlow Integration Guide

## Overview

This project integrates MLFlow for comprehensive experiment tracking and model management. MLFlow automatically logs:

- **Parameters**: Experiment configuration, hyperparameters, seeds, etc.
- **Metrics**: Cross-validation metrics (mean Â± std), hold-out test metrics, training time
- **Artifacts**: Trained models, metrics CSVs, PCA analysis files
- **Tags**: Experiment descriptions, environment info

## Quick Start

### 1. Start MLFlow Server

```bash
# Start the MLFlow tracking server
./work/scripts/start-mlflow-server.sh
```

The server will start on `http://localhost:5000` with:
- **Backend Store**: SQLite database at `work/mlflow/mlflow.db`
- **Artifact Store**: File system at `/output/mlflow-artifacts`

### 2. Enable MLFlow Tracking

MLFlow tracking is configured in your config file (e.g., `local-config.yml`):

```yaml
common:
  mlflow:
    enabled: true
    trackingUri: "http://localhost:5000"
```

### 3. Run Experiments

Simply run your experiments as usual:

```bash
./work/scripts/spark-submit.sh local
```

All experiment data will be automatically logged to MLFlow!

### 4. View Results

Open your browser and navigate to:

```
http://localhost:5000
```

## What Gets Logged

### Parameters

For each experiment run, the following parameters are logged:

| Parameter | Description | Example |
|-----------|-------------|---------|
| `experiment_name` | Unique experiment identifier | `exp4_rf_pca_cv_gs_15min` |
| `target` | Target label column | `label_is_delayed_15min` |
| `model_type` | ML model type | `randomforest` |
| `train_ratio` | Train/dev split ratio | `0.8` |
| `cv_folds` | Number of CV folds | `5` |
| `grid_search_enabled` | Whether grid search is used | `true` |
| `grid_search_metric` | Metric for grid search | `f1` |
| `feature_extraction_type` | Feature extraction method | `pca` |
| `pca_enabled` | Whether PCA is enabled | `true` |
| `pca_variance_threshold` | PCA variance threshold | `0.7` |
| `random_seed` | Random seed | `42` |
| `numTrees` | Number of trees (RF) | `50` |
| `maxDepth` | Max tree depth | `5` |
| `maxBins` | Max bins for splits | `32` |
| ... | Other hyperparameters | ... |

### Metrics

#### Cross-Validation Metrics (per fold)
- `cv_fold0_accuracy`, `cv_fold1_accuracy`, ...
- `cv_fold0_precision`, `cv_fold1_precision`, ...
- `cv_fold0_recall`, `cv_fold1_recall`, ...
- `cv_fold0_f1`, `cv_fold1_f1`, ...
- `cv_fold0_auc`, `cv_fold1_auc`, ...

#### Cross-Validation Aggregated Metrics
- `cv_mean_accuracy`, `cv_std_accuracy`
- `cv_mean_precision`, `cv_std_precision`
- `cv_mean_recall`, `cv_std_recall`
- `cv_mean_f1`, `cv_std_f1`
- `cv_mean_auc`, `cv_std_auc`

#### Hold-out Test Metrics
- `test_accuracy`
- `test_precision`
- `test_recall`
- `test_f1`
- `test_auc`

#### Other Metrics
- `training_time_seconds`

### Artifacts

The following files/directories are logged as artifacts:

| Artifact | Description |
|----------|-------------|
| `metrics/` | All metrics CSV files (CV, hold-out, ROC data, etc.) |
| `models/randomforest_final/` | Trained model in Spark ML format |
| `metrics/pca_variance.csv` | PCA variance analysis (if PCA enabled) |

## Using the MLFlow UI

### Compare Experiments

1. Navigate to the **Experiments** page
2. Select multiple runs using checkboxes
3. Click **Compare** button
4. View side-by-side comparison of:
   - Parameters
   - Metrics (with charts)
   - Artifacts

### Filter and Search

Use the search bar to filter runs:

```
metrics.test_f1 > 0.8
```

```
params.target = "label_is_delayed_15min"
```

```
tags.experiment_description LIKE "%Random Forest%"
```

### Visualize Metrics

1. Select runs you want to compare
2. Click on **Chart** view
3. Choose metrics to plot:
   - Scatter plots
   - Parallel coordinates
   - Box plots
   - Line charts (for per-fold metrics)

### Download Artifacts

1. Click on a run
2. Navigate to **Artifacts** tab
3. Browse and download:
   - Models
   - Metrics CSVs
   - Analysis files

## Advanced Usage

### Model Registry

Register your best models for deployment:

```python
import mlflow

# Register model
mlflow.register_model(
    model_uri="runs:/<run_id>/model",
    name="flight-delay-predictor"
)

# Transition to production
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name="flight-delay-predictor",
    version=1,
    stage="Production"
)
```

### Programmatic Access

```python
import mlflow

# Set tracking URI
mlflow.set_tracking_uri("http://localhost:5000")

# Search runs
experiment = mlflow.get_experiment_by_name("flight-delay-prediction")
runs = mlflow.search_runs(
    experiment_ids=[experiment.experiment_id],
    filter_string="metrics.test_f1 > 0.8",
    order_by=["metrics.test_f1 DESC"]
)

print(runs[["run_id", "params.experiment_name", "metrics.test_f1"]])
```

### API Access

MLFlow provides a REST API:

```bash
# Get experiment info
curl http://localhost:5000/api/2.0/mlflow/experiments/get-by-name?experiment_name=flight-delay-prediction

# Search runs
curl -X POST http://localhost:5000/api/2.0/mlflow/runs/search \
  -H "Content-Type: application/json" \
  -d '{"experiment_ids": ["0"], "filter": "metrics.test_f1 > 0.8"}'
```

## Troubleshooting

### Port Already in Use

If port 5000 is already in use:

1. Stop existing MLFlow server:
   ```bash
   lsof -ti:5000 | xargs kill -9
   ```

2. Or modify port in config and startup script

### Connection Refused

Check if MLFlow server is running:

```bash
curl http://localhost:5000/health
```

If not, restart the server:

```bash
./work/scripts/start-mlflow-server.sh
```

### Artifacts Not Logging

Verify artifact root directory exists and is writable:

```bash
ls -la /output/mlflow-artifacts
```

### Disable MLFlow Tracking

Set `enabled: false` in your config file:

```yaml
common:
  mlflow:
    enabled: false
```

## Configuration Files

### local-config.yml

```yaml
common:
  mlflow:
    enabled: true
    trackingUri: "http://localhost:5000"
```

### lamsade-config.yml

For production deployment, consider using a remote tracking server:

```yaml
common:
  mlflow:
    enabled: true
    trackingUri: "http://mlflow-server.example.com:5000"
```

## Best Practices

1. **Consistent Naming**: Use descriptive experiment names that include model type, features, and target
2. **Tag Everything**: Add tags for easy filtering (e.g., `baseline`, `production`, `experimental`)
3. **Version Models**: Use model registry to track production models
4. **Document Changes**: Use run notes/descriptions to document important changes
5. **Clean Up**: Periodically delete failed or test runs to keep the UI clean
6. **Backup**: Regularly backup the SQLite database (`work/mlflow/mlflow.db`)

## References

- [MLFlow Documentation](https://mlflow.org/docs/latest/index.html)
- [MLFlow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [MLFlow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [MLFlow REST API](https://mlflow.org/docs/latest/rest-api.html)
