{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e159c5ae-37c7-43b7-a800-6349093ad707",
   "metadata": {},
   "source": [
    "# Data Jointure V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e54813-1f4c-4ba8-ae6f-bd3f4571fb6a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b9013a-88ff-4c24-8b38-c69e9114ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar\n",
      "Finished download of Emiasd-Flight-Data-Analysis.jar\n",
      "Using cached version of Emiasd-Flight-Data-Analysis.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee6509b-16ae-49fc-b829-db9aed8c8820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "args = Array(jupyter)\n",
       "configuration = AppConfiguration(local,CommonConfig(42,true,debug,false,false,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output/data),FileConfig(/home/jovyan/work/output/model)),MLFlowConfig(false,http://localhost:5555)),Stream(ExperimentConfig(Experience-local,Baseline Random ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "AppConfiguration(local,CommonConfig(42,true,debug,false,false,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output/data),FileConfig(/home/jovyan/work/output/model)),MLFlowConfig(false,http://localhost:5555)),Stream(ExperimentConfig(Experience-local,Baseline Random ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import com.flightdelay.config.{AppConfiguration, ConfigurationLoader, ExperimentConfig}\n",
    "import com.flightdelay.data.loaders.FlightDataLoader\n",
    "\n",
    "// Env Configuration\n",
    "val args: Array[String] = Array(\"jupyter\")\n",
    "implicit val configuration: AppConfiguration = ConfigurationLoader.loadConfiguration(args)\n",
    "implicit val experiment: ExperimentConfig = configuration.experiments(0)\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(sc.getConf)\n",
    "  .config(\"spark.eventLog.enabled\", \"true\")\n",
    "  .config(\"spark.eventLog.dir\", \"file:///home/jovyan/work/spark-events\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Rendre la session Spark implicite\n",
    "implicit val session = spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dd5ff-6a58-4ec9-bd15-b0d629e468c6",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96dc104-1631-4d57-8b1a-6b6aeb896805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Flight DF Count: ,3908461)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDFPath = /home/jovyan/work/output/common/data/processed_flights.parquet\n",
       "flightDF = [OP_CARRIER_AIRLINE_ID: int, DEST_AIRPORT_ID: int ... 28 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[OP_CARRIER_AIRLINE_ID: int, DEST_AIRPORT_ID: int ... 28 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDFPath = s\"${configuration.common.output.basePath}/common/data/processed_flights.parquet\"\n",
    "val flightDF = spark.read.parquet(flightDFPath)\n",
    "\n",
    "println(\"Flight DF Count: \", flightDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccf286a-46b7-4aaf-881d-048ad866fd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weatherDFPath = /home/jovyan/work/output/common/data/processed_weather.parquet\n",
       "weatherDF = [WindDirection: double, PressureChange: double ... 63 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Weather DF Count: ,1549320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindDirection: double, PressureChange: double ... 63 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weatherDFPath = s\"${configuration.common.output.basePath}/common/data/processed_weather.parquet\"\n",
    "val weatherDF = spark.read.parquet(weatherDFPath)\n",
    "\n",
    "println(\"Weather DF Count: \", weatherDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed6e42-487a-4b3c-a39b-97d282e1c459",
   "metadata": {},
   "source": [
    "## Jointure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0996a5a4-9c25-4a84-a67e-b3a2c168be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Flight for join count ->,3908461)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDF_mCovered = [OP_CARRIER_AIRLINE_ID: int, DEST_AIRPORT_ID: int ... 28 more fields]\n",
       "weatherDF_pruned = [WindDirection: double, PressureChange: double ... 63 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Weather for join count ->,1549320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindDirection: double, PressureChange: double ... 63 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDF_mCovered = flightDF\n",
    "val weatherDF_pruned = weatherDF\n",
    "\n",
    "println(\"Flight for join count ->\", flightDF_mCovered.count())\n",
    "println(\"Weather for join count ->\", weatherDF_pruned.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabccf69-99f0-4ea7-98e8-f1c51af8e99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Metrics\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after ORIGIN join: 3908461, rows after DEST join: 3908461\n",
      "[METRIC][flight-joins-df2] Build flights joins DataFrame2 took 11004.70 ms\n"
     ]
    }
   ],
   "source": [
    "// =====================================================================\n",
    "// Flight × Weather en DataFrame (Map → Hash partition → Reduce)\n",
    "// - Fenêtre 12h avant départ (Wo_*) et 12h avant arrivée (Wd_*)\n",
    "// - Gestion veille via relHour (duplication J et J+1)\n",
    "// - Un seul job global avec Metrics.withJob\n",
    "// =====================================================================\n",
    "import org.apache.spark.sql.{DataFrame, Row, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// ------------------------\n",
    "// 0) Objet Metrics (inchangé)\n",
    "// ------------------------\n",
    "object Metrics {\n",
    "  def withJob[T](id: String, desc: String)(body: => T): T = {\n",
    "    val sc = spark.sparkContext\n",
    "    sc.setJobGroup(id, desc, interruptOnCancel = false)\n",
    "    val t0 = System.nanoTime()\n",
    "    val res = body\n",
    "    val dtMs = (System.nanoTime() - t0) / 1e6\n",
    "    println(f\"[METRIC][$id] $desc took ${dtMs}%.2f ms\")\n",
    "    sc.clearJobGroup()\n",
    "    res\n",
    "  }\n",
    "  import org.apache.spark.storage.StorageLevel\n",
    "  def persistCount(id: String, desc: String, rdd: org.apache.spark.rdd.RDD[_], lvl: StorageLevel = StorageLevel.MEMORY_ONLY): Long = {\n",
    "    rdd.persist(lvl); val n = withJob(id, desc) { rdd.count() }\n",
    "    println(s\"[METRIC][$id] records=$n partitions=${rdd.getNumPartitions}\"); n\n",
    "  }\n",
    "  def persistCountDF(id: String, desc: String, df: DataFrame, lvl: StorageLevel = StorageLevel.MEMORY_ONLY): Long = {\n",
    "    df.persist(lvl); val n = withJob(id, desc) { df.count() }\n",
    "    println(s\"[METRIC][$id] records=$n partitions=${df.rdd.getNumPartitions}\"); n\n",
    "  }\n",
    "}\n",
    "\n",
    "// =====================================================================\n",
    "// 1) Jointure complète exécutée sous un SEUL job\n",
    "// =====================================================================\n",
    "\n",
    "\n",
    "// ------------------------\n",
    "// Paramètres de partitions \"reducers\"\n",
    "// ------------------------\n",
    "\n",
    "Metrics.withJob(\"flight-joins-df2\", \"Build flights joins DataFrame2\") {\n",
    "    // =====================================================================\n",
    "    // Jointure Flight × Weather la plus fidèle possible à Belcastro et al.\n",
    "    // - Implémentation DataFrame, Map/Reduce-like\n",
    "    // - get_hourly_observations respecté via pré-calcul des buckets horaires\n",
    "    // =====================================================================\n",
    "    \n",
    "    import org.apache.spark.sql.{DataFrame, Column}\n",
    "    import org.apache.spark.sql.functions._\n",
    "    import org.apache.spark.sql.types._\n",
    "    import org.apache.spark.sql.expressions.Window\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 0) Helpers communs\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    // HHMM (ou \"HH:MM\") -> heure [0..23] (floor à l'heure)\n",
    "    def hhmmHourCol(c: Column): Column = {\n",
    "      val s  = regexp_replace(c.cast(\"string\"), \":\", \"\")\n",
    "      val p4 = lpad(s, 4, \"0\")\n",
    "      (substring(p4, 1, 2).cast(\"int\") % 24)\n",
    "    }\n",
    "    \n",
    "    // (date, HHMM) -> timestamp \"yyyy-MM-dd HH:mm\"\n",
    "    def dateTimeFromHHMM(dateCol: Column, hhmmCol: Column): Column = {\n",
    "      val s  = regexp_replace(hhmmCol.cast(\"string\"), \":\", \"\")\n",
    "      val p4 = lpad(s, 4, \"0\")\n",
    "      val hh = substring(p4, 1, 2)\n",
    "      val mm = substring(p4, 3, 2)\n",
    "    \n",
    "      val tsStr = concat(\n",
    "        date_format(dateCol, \"yyyy-MM-dd\"), lit(\" \"),\n",
    "        hh, lit(\":\"), mm\n",
    "      )\n",
    "      to_timestamp(tsStr, \"yyyy-MM-dd HH:mm\")\n",
    "    }\n",
    "    \n",
    "    // Construit le timestamp théorique du vol à partir (DATE, HHMM)\n",
    "    def flightTsCol(dateCol: Column, hhmmCol: Column): Column =\n",
    "      dateTimeFromHHMM(dateCol, hhmmCol)\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 1) Préparation des DataFrames de base (Map)\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    val flightsBase = flightDF_mCovered.select(\n",
    "      col(\"feature_flight_unique_id\").as(\"flightId\"),\n",
    "      col(\"ORIGIN_WBAN\"),\n",
    "      col(\"DEST_WBAN\"),\n",
    "      col(\"UTC_FL_DATE\").cast(DateType).as(\"DEP_DATE\"),\n",
    "      col(\"UTC_ARR_DATE\").cast(DateType).as(\"ARR_DATE\"),\n",
    "      col(\"UTC_CRS_DEP_TIME\").as(\"DEP_TIME_HHMM\"),\n",
    "      col(\"UTC_ARR_TIME\").as(\"ARR_TIME_HHMM\"),\n",
    "      col(\"*\")\n",
    "    )\n",
    "    \n",
    "    val weatherBase = weatherDF_pruned\n",
    "      .select(\n",
    "        trim(col(\"WBAN\")).as(\"WBAN\"),\n",
    "        col(\"Date\").cast(DateType).as(\"WDATE\"),\n",
    "        col(\"Time\").as(\"WTIME_HHMM\"),\n",
    "        col(\"WindSpeed\").as(\"ws\"),\n",
    "        col(\"WindDirection\").as(\"wd\"),\n",
    "        col(\"DryBulbCelsius\").as(\"tempC\"),\n",
    "        col(\"SeaLevelPressure\").as(\"slp\"),\n",
    "        col(\"HourlyPrecip\").as(\"precip\"),\n",
    "        col(\"feature_weather_severity_index\"),\n",
    "        col(\"feature_flight_category_ordinal\")\n",
    "      )\n",
    "      .where(col(\"WBAN\").isNotNull && length(col(\"WBAN\")) > 0 && col(\"WDATE\").isNotNull)\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 2) Étape \"AO\" fidèle : pour chaque (A0, date, heure h) on prend\n",
    "    //    l'observation la plus proche de \"date h:00\"  (get_hourly_observations)\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    // 2.1. Timestamp réel de la mesure météo\n",
    "    val weatherWithTs = weatherBase\n",
    "      .withColumn(\"w_ts\", dateTimeFromHHMM(col(\"WDATE\"), col(\"WTIME_HHMM\")))\n",
    "      .na.drop(Seq(\"w_ts\"))\n",
    "    \n",
    "    // 2.2. Générer, pour chaque (WBAN, WDATE), les 24 \"instants cibles\" h:00\n",
    "    val buckets = weatherWithTs\n",
    "      .select(\"WBAN\", \"WDATE\")\n",
    "      .distinct()\n",
    "      .withColumn(\"hourBucket\", explode(sequence(lit(0), lit(23)))) // 0..23\n",
    "      .withColumn(\n",
    "        \"target_ts\",\n",
    "        to_timestamp(\n",
    "          concat(\n",
    "            date_format(col(\"WDATE\"), \"yyyy-MM-dd\"), lit(\" \"),\n",
    "            lpad(col(\"hourBucket\").cast(\"string\"), 2, \"0\"), lit(\":00\")\n",
    "          ),\n",
    "          \"yyyy-MM-dd HH:mm\"\n",
    "        )\n",
    "      )\n",
    "    \n",
    "    // 2.3. Pour chaque (WBAN, WDATE, hourBucket) on choisit l'obs la plus proche\n",
    "    val bucketsJoined = buckets\n",
    "      .join(\n",
    "        weatherWithTs,\n",
    "        Seq(\"WBAN\", \"WDATE\"),\n",
    "        \"inner\"\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"dist\",\n",
    "        abs(col(\"w_ts\").cast(\"long\") - col(\"target_ts\").cast(\"long\"))\n",
    "      )\n",
    "    \n",
    "    val w = Window\n",
    "      .partitionBy(col(\"WBAN\"), col(\"WDATE\"), col(\"hourBucket\"))\n",
    "      .orderBy(col(\"dist\").asc_nulls_last)\n",
    "    \n",
    "    val weatherHourlyBest = bucketsJoined\n",
    "      .withColumn(\"rn\", row_number().over(w))\n",
    "      .where(col(\"rn\") === 1)\n",
    "      .drop(\"dist\", \"rn\")\n",
    "    \n",
    "    // À ce stade, weatherHourlyBest = AO “compressé” :\n",
    "    //   (WBAN, WDATE, hourBucket, target_ts, w_ts, ws, wd, tempC, ...)\n",
    "    // qui correspond très fidèlement à get_hourly_observations sur la grille horaire.\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 3) Gestion J / J+1 via relHour (comme dans ton code mais sur les buckets)\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    val weatherBucketsStruct = struct(\n",
    "      col(\"hourBucket\").as(\"hour\"),\n",
    "      col(\"WBAN\"),\n",
    "      col(\"WDATE\"),\n",
    "      col(\"target_ts\"),\n",
    "      col(\"w_ts\"),\n",
    "      col(\"ws\"), col(\"wd\"),\n",
    "      col(\"tempC\"), col(\"slp\"), col(\"precip\"),\n",
    "      col(\"feature_weather_severity_index\"),\n",
    "      col(\"feature_flight_category_ordinal\")\n",
    "    )\n",
    "    \n",
    "    // Jour D : relHour = 0..23, DATE = WDATE\n",
    "    val meteoSameDay = weatherHourlyBest\n",
    "      .withColumn(\"relHour\", col(\"hourBucket\"))\n",
    "      .withColumn(\"DATE\", col(\"WDATE\"))\n",
    "      .select(\n",
    "        col(\"WBAN\"), col(\"DATE\"),\n",
    "        col(\"relHour\"),\n",
    "        weatherBucketsStruct.as(\"wobs\")\n",
    "      )\n",
    "    \n",
    "    // Jour D+1 : relHour = hourBucket-24  => [-24 .. -1], DATE = WDATE+1\n",
    "    val meteoNextDay = weatherHourlyBest\n",
    "      .withColumn(\"relHour\", col(\"hourBucket\") - lit(24))\n",
    "      .withColumn(\"DATE\", date_add(col(\"WDATE\"), 1))\n",
    "      .select(\n",
    "        col(\"WBAN\"), col(\"DATE\"),\n",
    "        col(\"relHour\"),\n",
    "        weatherBucketsStruct.as(\"wobs\")\n",
    "      )\n",
    "    \n",
    "    val weatherRel = meteoSameDay.unionByName(meteoNextDay)\n",
    "      .filter(col(\"relHour\").between(-24, 23))\n",
    "    \n",
    "    // 3.1. Reduce météo par clé (WBAN, DATE) -> Map relHour -> struct (AO final)\n",
    "    val weatherByKey = weatherRel\n",
    "      .groupBy(col(\"WBAN\"), col(\"DATE\"))\n",
    "      .agg(\n",
    "        map_from_entries(\n",
    "          collect_list(struct(col(\"relHour\"), col(\"wobs\")))\n",
    "        ).as(\"wmap\")\n",
    "      )\n",
    "      // hash partitionnement sur la join key (Map/Partition)\n",
    "      .repartition(col(\"WBAN\"), col(\"DATE\"))\n",
    "      .persist()\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 4) JOIN #1 — ORIGIN : reproduction de l'ALG.1 sur l'aéroport d'origine\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    val numParts = 64\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", numParts)\n",
    "    \n",
    "    // 4.1. Timestamp & heure du départ (floor à l'heure)\n",
    "    val flightsDep = flightsBase\n",
    "      .withColumn(\"DEP_TS\", flightTsCol(col(\"DEP_DATE\"), col(\"DEP_TIME_HHMM\")))\n",
    "      .withColumn(\"depHour\", hhmmHourCol(col(\"DEP_TIME_HHMM\")))\n",
    "      .na.fill(Map(\"depHour\" -> 0))\n",
    "    \n",
    "    // 4.2. Join sur (A0, Date(t_sd))  ~ join_key = <A0, Date(t_sd)>\n",
    "    val originPre = flightsDep\n",
    "      .repartition(numParts, col(\"ORIGIN_WBAN\"), col(\"DEP_DATE\"))\n",
    "      .join(\n",
    "        weatherByKey.hint(\"shuffle_hash\"),\n",
    "        col(\"ORIGIN_WBAN\") === weatherByKey(\"WBAN\") &&\n",
    "        col(\"DEP_DATE\")    === weatherByKey(\"DATE\"),\n",
    "        \"left\"\n",
    "      )\n",
    "      .drop(weatherByKey(\"WBAN\"))\n",
    "      .drop(weatherByKey(\"DATE\"))\n",
    "    \n",
    "    // 4.3. get_hourly_observations(AO, f_tsd) via wmap[depHour - i]\n",
    "    val originWithWoArr = originPre\n",
    "      .withColumn(\n",
    "        \"Wo\",\n",
    "        expr(\"transform(sequence(1, 12), i -> element_at(wmap, depHour - i))\")\n",
    "      )\n",
    "      .drop(\"wmap\")\n",
    "    \n",
    "    val woCols = (1 to 12).map(i => col(\"Wo\").getItem(i - 1).as(s\"Wo_h$i\"))\n",
    "    \n",
    "    val originDF = originWithWoArr\n",
    "      .select(col(\"*\") +: woCols: _*)\n",
    "      .drop(\"Wo\")\n",
    "      .persist()\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 5) JOIN #2 — DESTINATION : même algo appliqué à A_d\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    val flightsArr = originDF\n",
    "      .withColumn(\"ARR_TS\", flightTsCol(col(\"ARR_DATE\"), col(\"ARR_TIME_HHMM\")))\n",
    "      .withColumn(\"arrHour\", hhmmHourCol(col(\"ARR_TIME_HHMM\")))\n",
    "      .na.fill(Map(\"arrHour\" -> 0))\n",
    "    \n",
    "    val destPre = flightsArr\n",
    "      .repartition(numParts, col(\"DEST_WBAN\"), col(\"ARR_DATE\"))\n",
    "      .join(\n",
    "        weatherByKey.hint(\"shuffle_hash\"),\n",
    "        col(\"DEST_WBAN\") === weatherByKey(\"WBAN\") &&\n",
    "        col(\"ARR_DATE\")  === weatherByKey(\"DATE\"),\n",
    "        \"left\"\n",
    "      )\n",
    "      .drop(weatherByKey(\"WBAN\"))\n",
    "      .drop(weatherByKey(\"DATE\"))\n",
    "    \n",
    "    val destWithWdArr = destPre\n",
    "      .withColumn(\n",
    "        \"Wd\",\n",
    "        expr(\"transform(sequence(1, 12), i -> element_at(wmap, arrHour - i))\")\n",
    "      )\n",
    "      .drop(\"wmap\")\n",
    "    \n",
    "    val wdCols = (1 to 12).map(i => col(\"Wd\").getItem(i - 1).as(s\"Wd_h$i\"))\n",
    "    \n",
    "    val joinedDF = destWithWdArr\n",
    "      .select(col(\"*\") +: wdCols: _*)\n",
    "      .drop(\"Wd\")\n",
    "      .persist()\n",
    "    \n",
    "    // ---------------------------------------------------------------------\n",
    "    // 6) Action finale (pour déclencher l'exécution)\n",
    "    // ---------------------------------------------------------------------\n",
    "    \n",
    "    println(\n",
    "      s\"Rows after ORIGIN join: ${originDF.count()}, rows after DEST join: ${joinedDF.count()}\"\n",
    "    )\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f721e751-0437-4679-8142-69cefdb97933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(Vector(OP_CARRIER_AIRLINE_ID, DEST_AIRPORT_ID, ORIGIN_AIRPORT_ID, CRS_ELAPSED_TIME, feature_arrival_time_period, CRS_DEP_TIME, feature_flight_week_of_year, feature_departure_time_period, feature_departure_hour_rounded))\n",
      "[INFO] [FlightWeatherJoiner] Starting origin join with 3 hours depth\n",
      "[INFO] [FlightWeatherJoiner] Weather bucketing completed for origin\n",
      "[INFO] [FlightWeatherJoiner] Colonnes ajoutées automatiquement pour origin: ORIGIN_WBAN, feature_utc_departure_hour_rounded, UTC_ARR_DATE, DEST_WBAN, UTC_FL_DATE, feature_utc_arrival_hour_rounded\n",
      "[INFO] [FlightWeatherJoiner] Completed origin join\n",
      "[INFO] [FlightWeatherJoiner] Starting destination join with 3 hours depth\n",
      "[INFO] [FlightWeatherJoiner] Weather bucketing completed for destination\n",
      "[INFO] [FlightWeatherJoiner] Colonnes ajoutées automatiquement pour destination: ORIGIN_WBAN, feature_utc_departure_hour_rounded, UTC_ARR_DATE, DEST_WBAN, UTC_FL_DATE, feature_utc_arrival_hour_rounded\n",
      "[INFO] [FlightWeatherJoiner] Completed destination join\n",
      "[INFO] [Anti-Leakage] Colonnes supprimées : UTC_ARR_DATE, feature_utc_arrival_hour_rounded\n",
      "[INFO] [Anti-Leakage] hour transformé en hours_before_flight (index relatif)\n",
      "[INFO] [Anti-Leakage] Champs conservés dans observations météo : WindDirection, PressureChange, feature_pressure_bucket, has_hail, DewPointFarenheit, has_visible_moisture, WetBulbCelsius, feature_flight_category, DryBulbFarenheit, weather_hazard_level, feature_weather_severity_index, has_obscuration, feature_visibility_miles, has_precipitation, RelativeHumidity, feature_visibility_category, weather_intensity, feature_has_broken, has_thunderstorm, feature_is_ifr_conditions, feature_flight_category_ordinal, Altimeter, DryBulbCelsius, feature_visibility_risk_score, has_freezing_precip, feature_requires_cat_ii, extracted_codes, WindSpeed, has_rain, feature_operations_risk_level, SeaLevelPressure, Icing_Risk_Flag, feature_lowest_cloud_height, feature_is_low_visibility, has_snow, StationPressure, Icing_Risk_Level, HourlyPrecip, feature_is_clear, feature_num_cloud_layers, feature_ceiling, ValueForWindCharacter, feature_has_low_ceiling, feature_visibility_km, feature_precipitation_intensity, feature_visibility_inverse, DewPointCelsius, feature_most_critical_sky, press_change_abs, feature_visibility_normalized, feature_has_overcast, feature_cloud_risk_score, feature_has_obscured, intensity_light, has_freezing, PressureTendency, intensity_heavy, feature_is_very_low_visibility, feature_pressure_visibility_combo, has_hazardous, WetBulbFarenheit, feature_is_vfr_conditions, hours_before_flight\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Multiple failures in stage materialization.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Multiple failures in stage materialization.",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:809)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:333)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)",
      "  at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3616)",
      "  at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3615)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)",
      "  at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)",
      "  at $anonfun$res4$1(<console>:122)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at Metrics$.withJob(<console>:70)",
      "  ... 44 elided",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 25.0 failed 1 times, most recent failure: Lost task 1.0 in stage 25.0 (TID 136) (jupyter-spark executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/jovyan/work/output/common/data/processed_weather.parquet/part-00312-b499899e-9414-4a73-9e9e-b8fe5c4489ec-c000.snappy.parquet does not exist",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.columnartorow_nextBatch_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.sort_addToSorter_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "Caused by: org.apache.spark.SparkFileNotFoundException: File file:/home/jovyan/work/output/common/data/processed_weather.parquet/part-00312-b499899e-9414-4a73-9e9e-b8fe5c4489ec-c000.snappy.parquet does not exist",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)",
      "  at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.columnartorow_nextBatch_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.sort_addToSorter_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)",
      "  at org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)",
      "  at org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)",
      "  at org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)",
      "  at org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)",
      "  at org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)",
      "  at org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  at java.base/java.lang.Thread.run(Thread.java:829)"
     ]
    }
   ],
   "source": [
    "import com.flightdelay.features.joiners.FlightWeatherDataJoiner\n",
    "\n",
    "\n",
    "// =====================================================================\n",
    "// Flight × Weather en DataFrame (Map → Hash partition → Reduce)\n",
    "// - Fenêtre 12h avant départ (Wo_*) et 12h avant arrivée (Wd_*)\n",
    "// - Gestion veille via relHour (duplication J et J+1)\n",
    "// - Un seul job global avec Metrics.withJob\n",
    "// =====================================================================\n",
    "import org.apache.spark.sql.{DataFrame, Row, Column}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// ------------------------\n",
    "// 0) Objet Metrics (inchangé)\n",
    "// ------------------------\n",
    "object Metrics {\n",
    "  def withJob[T](id: String, desc: String)(body: => T): T = {\n",
    "    val sc = spark.sparkContext\n",
    "    sc.setJobGroup(id, desc, interruptOnCancel = false)\n",
    "    val t0 = System.nanoTime()\n",
    "    val res = body\n",
    "    val dtMs = (System.nanoTime() - t0) / 1e6\n",
    "    println(f\"[METRIC][$id] $desc took ${dtMs}%.2f ms\")\n",
    "    sc.clearJobGroup()\n",
    "    res\n",
    "  }\n",
    "  import org.apache.spark.storage.StorageLevel\n",
    "  def persistCount(id: String, desc: String, rdd: org.apache.spark.rdd.RDD[_], lvl: StorageLevel = StorageLevel.MEMORY_ONLY): Long = {\n",
    "    rdd.persist(lvl); val n = withJob(id, desc) { rdd.count() }\n",
    "    println(s\"[METRIC][$id] records=$n partitions=${rdd.getNumPartitions}\"); n\n",
    "  }\n",
    "  def persistCountDF(id: String, desc: String, df: DataFrame, lvl: StorageLevel = StorageLevel.MEMORY_ONLY): Long = {\n",
    "    df.persist(lvl); val n = withJob(id, desc) { df.count() }\n",
    "    println(s\"[METRIC][$id] records=$n partitions=${df.rdd.getNumPartitions}\"); n\n",
    "  }\n",
    "}\n",
    "\n",
    "// =====================================================================\n",
    "// 1) Jointure complète exécutée sous un SEUL job\n",
    "// =====================================================================\n",
    "\n",
    "\n",
    "// ------------------------\n",
    "// Paramètres de partitions \"reducers\"\n",
    "// ------------------------\n",
    "\n",
    "Metrics.withJob(\"flight-joins-df-In-FlightWeatherDataJoiner\", \"Build flights joins DataFrame in class FlightWeatherDataJoiner\") {\n",
    "    // =====================================================================\n",
    "    // Jointure Flight × Weather la plus fidèle possible à Belcastro et al.\n",
    "    // - Implémentation DataFrame, Map/Reduce-like\n",
    "    // - get_hourly_observations respecté via pré-calcul des buckets horaires\n",
    "    // =====================================================================\n",
    "\n",
    "    // Extraire les noms de colonnes depuis les Maps de configuration\n",
    "    val flightCols = experiment.featureExtraction.flightSelectedFeatures.map(_.keys.toSeq)\n",
    "    val weatherCols = experiment.featureExtraction.weatherSelectedFeatures.map(_.keys.toSeq)\n",
    "\n",
    "    println(flightCols)\n",
    "    \n",
    "    val joinedDF = FlightWeatherDataJoiner.joinFlightsWithWeather(\n",
    "        flightDF_mCovered,\n",
    "        weatherDF_pruned,\n",
    "        experiment.featureExtraction.weatherOriginDepthHours,\n",
    "        experiment.featureExtraction.weatherDestinationDepthHours,\n",
    "        true,\n",
    "        flightCols,\n",
    "        weatherCols\n",
    "    )\n",
    "\n",
    "    println(\n",
    "      s\"Rows after ORIGIN join: ${flightDF_mCovered.count()}, rows after DEST join: ${joinedDF.count()}\"\n",
    "    )\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56133a88-2e25-48d7-98f2-bee70250078e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_scala - Scala",
   "language": "scala",
   "name": "apache_toree_scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
