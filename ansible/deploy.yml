---
- name: Flight Project - Deployment
  hosts: lamsade
  gather_facts: no
  
  vars:
    local_project_dir: "/home/nribal/psl/06-flight/02-git/Emiasd-FlightProject"
    cluster_user: "hbalamou"
    workspace: "~/workspace"
    hdfs_base: "/students/p6emiasd2025/{{ cluster_user }}"
    tasks_to_run: "data-pipeline,feature-extraction,train"
    ssh_key: "{{ lookup('env', 'HOME') }}/.ssh/id_ed25519_hbalamou.key"
    ssh_opts: "-p 5022 -o StrictHostKeyChecking=no -o LogLevel=ERROR"
    
  tasks:
    # Compilation
    - name: Compile with SBT
      delegate_to: localhost
      shell: cd {{ local_project_dir }} && sbt clean package
      tags: [compile, build]
    
    # Déploiement
    - name: Create remote directories
      delegate_to: localhost
      shell: ssh -i {{ ssh_key }} {{ ssh_opts }} {{ cluster_user }}@ssh.lamsade.dauphine.fr "mkdir -p ~/workspace/{apps,config,logs}"
      tags: [setup, deploy]
        
    - name: Deploy JAR files
      delegate_to: localhost
      shell: scp -i {{ ssh_key }} -P 5022 -o StrictHostKeyChecking=no -o LogLevel=ERROR {{ local_project_dir }}/work/apps/*.jar {{ cluster_user }}@ssh.lamsade.dauphine.fr:{{ workspace }}/apps/
      tags: [deploy]
        
    - name: Deploy configuration
      delegate_to: localhost
      shell: scp -i {{ ssh_key }} -P 5022 -o StrictHostKeyChecking=no -o LogLevel=ERROR {{ local_project_dir }}/src/main/resources/prodlamsade-config.yml {{ cluster_user }}@ssh.lamsade.dauphine.fr:{{ workspace }}/config/
      tags: [deploy]

    # Exécution
    - name: Clean HDFS output directory
      delegate_to: localhost
      shell: |
        ssh -i {{ ssh_key }} {{ ssh_opts }} {{ cluster_user }}@ssh.lamsade.dauphine.fr \
        "/opt/shared/hadoop-current/bin/hdfs dfs -rm -r -skipTrash {{ hdfs_base }}/output 2>/dev/null || true"
      tags: [run]

    - name: Run Spark job in screen
      delegate_to: localhost
      shell: |
        ssh -i {{ ssh_key }} {{ ssh_opts }} {{ cluster_user }}@ssh.lamsade.dauphine.fr "screen -dmS spark_job bash -c '
        LOG_DIR=\$HOME/workspace/logs && mkdir -p \$LOG_DIR
        LOG_FILE=\$LOG_DIR/flight-app-\$(date +%Y%m%d_%H%M%S).log
        /opt/shared/spark-current/bin/spark-submit \
          --master yarn --deploy-mode client \
          --class com.flightdelay.app.FlightDelayPredictionApp \
          --files \$HOME/workspace/config/prodlamsade-config.yml \
          --driver-memory 32G --driver-cores 8 \
          --executor-memory 8G --num-executors 6 \
          --conf spark.driver.maxResultSize=8g \
          --conf spark.sql.shuffle.partitions=400 \
          --conf spark.default.parallelism=400 \
          --conf spark.kryoserializer.buffer.max=1024m \
          --conf spark.memory.fraction=0.8 \
          --conf spark.rpc.message.maxSize=2047 \
          --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
          --conf spark.network.timeout=800s \
          --conf spark.executor.heartbeatInterval=60s \
          --conf spark.memory.offHeap.enabled=true \
          --conf spark.memory.offHeap.size=2g \
          --conf spark.memory.storageFraction=0.3 \
          --jars \$HOME/workspace/apps/mlflow-client-3.4.0.jar,\$HOME/workspace/apps/mlflow-spark_2.13-3.4.0.jar \
          \$HOME/workspace/apps/Emiasd-Flight-Data-Analysis.jar \
          prodlamsade {{ tasks_to_run }} 2>&1 | tee \$LOG_FILE
        ln -sf \$LOG_FILE \$HOME/workspace/logs/latest.log
        '"
      tags: [run]
    
    - name: Display info
      debug:
        msg: "Job started in screen 'spark_job'. Logs: ssh {{ cluster_user }}@ssh.lamsade.dauphine.fr tail -f ~/workspace/logs/latest.log"
      tags: [run]
