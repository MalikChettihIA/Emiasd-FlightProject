{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb374cd-924b-40bf-aa63-a6b4849ca322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 2 Notebook 2: Handling Numerical Data\n",
    "\n",
    "**Objective:** Process the numerical features from the previous notebook's output (`ecommerce.customer_product_features`) using scaling techniques and prepare two distinct feature sets: one for classification and one for regression.\n",
    "\n",
    "**Input:** `ecommerce.customer_product_features` table.\n",
    "**Outputs:**\n",
    "1. `ecommerce.classification_scaled_features`: Data ready for classification feature selection.\n",
    "2. `ecommerce.regression_scaled_features`: Data ready for regression feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "245e9f75-14ec-49ba-9772-350af8d61a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup & Load Data\n",
    "\n",
    "Load the features table created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'ecommerce' database\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er une SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MonApplication\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommerce\")\n",
    "spark.sql(\"USE ecommerce\")\n",
    "\n",
    "\n",
    "print(\"Using 'ecommerce' database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2631e86-f1d9-4d2b-80d9-c843d8cf431a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from ecommerce.customer_product_features\n",
      "+-----------+----------+-------------+--------------+--------------------+--------------+-------------+---+-----------+------+------------------+---------------+----------+-----------------+------------+--------------+------------------+----------------+--------------------------+----------------------------+---------------------+-------------------+---------------+-------------+\n",
      "|customer_id|product_id|   gender_vec|   country_vec|membership_level_vec|  category_vec|   device_vec|age|tenure_days| price|product_avg_rating|previous_visits|view_count|add_to_cart_count|review_count|purchase_count|total_interactions|total_time_spent|interaction_time_span_days|days_since_first_interaction|total_purchase_amount|avg_purchase_amount|avg_user_rating|has_purchased|\n",
      "+-----------+----------+-------------+--------------+--------------------+--------------+-------------+---+-----------+------+------------------+---------------+----------+-----------------+------------+--------------+------------------+----------------+--------------------------+----------------------------+---------------------+-------------------+---------------+-------------+\n",
      "|          2|       485|(3,[1],[1.0])|(10,[1],[1.0])|       (4,[1],[1.0])|(10,[8],[1.0])|(3,[0],[1.0])| 33|        192| 14.26|               3.1|              0|         1|                1|           0|             0|                 2|              12|      0.008333333333333333|           4.892766203703704|                  0.0|                0.0|            3.1|            0|\n",
      "|          4|       312|(3,[1],[1.0])|(10,[6],[1.0])|       (4,[0],[1.0])|(10,[1],[1.0])|(3,[0],[1.0])| 53|        823|  3.06|               4.3|              6|         1|                1|           0|             0|                 2|               1|      0.003472222222222222|           83.28679398148148|                  0.0|                0.0|            4.3|            0|\n",
      "|          4|       616|(3,[1],[1.0])|(10,[6],[1.0])|       (4,[0],[1.0])|(10,[6],[1.0])|(3,[1],[1.0])| 53|        823|210.71|               4.5|              4|         1|                0|           0|             1|                 1|              79|                       0.0|          168.47391203703702|               281.39|             281.39|            4.5|            1|\n",
      "|          5|       825|(3,[0],[1.0])|(10,[0],[1.0])|       (4,[0],[1.0])|(10,[0],[1.0])|(3,[1],[1.0])| 32|        513|146.62|               3.6|              4|         1|                1|           0|             1|                 2|             182|      0.004166666666666667|          270.01722222222224|               232.21|             232.21|            3.6|            1|\n",
      "|          6|       143|(3,[1],[1.0])|(10,[3],[1.0])|       (4,[0],[1.0])|(10,[0],[1.0])|(3,[1],[1.0])| 32|        214| 43.51|               3.5|              2|         1|                0|           0|             0|                 1|              13|                       0.0|          222.83483796296295|                  0.0|                0.0|            3.5|            0|\n",
      "+-----------+----------+-------------+--------------+--------------------+--------------+-------------+---+-----------+------+------------------+---------------+----------+-----------------+------------+--------------+------------------+----------------+--------------------------+----------------------------+---------------------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "    from pyspark.sql.functions import col, sum\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "\n",
    "\n",
    "    # Load the data from the previous notebook\n",
    "    input_table = \"ecommerce.customer_product_features\"\n",
    "    features_df = spark.read.parquet(\"spark-warehouse/ecommerce.db/customer_product_features\")\n",
    "    print(f\"Successfully loaded data from {input_table}\")\n",
    "    features_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5605394-5324-4e4e-99f3-f6fc9707cbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Identify Feature Columns\n",
    "\n",
    "We need to identify which columns are our already encoded categorical features (vectors ending in `_vec`) and which are the numerical features that require scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709cee02-a61f-42e6-9615-692cb0073287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Vector Columns: ['gender_vec', 'country_vec', 'membership_level_vec', 'category_vec', 'device_vec']\n",
      "Numerical Columns to Scale: ['age', 'tenure_days', 'price', 'product_avg_rating', 'previous_visits', 'view_count', 'add_to_cart_count', 'review_count', 'total_interactions', 'total_time_spent', 'interaction_time_span_days', 'days_since_first_interaction', 'avg_user_rating']\n",
      "Classification Target: has_purchased\n",
      "Regression Target: total_purchase_amount\n",
      "Columns generally excluded from direct feature assembly: ['customer_id', 'product_id', 'has_purchased', 'total_purchase_amount', 'avg_purchase_amount', 'purchase_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Get column names\n",
    "    all_cols = features_df.columns\n",
    "\n",
    "    # Identify categorical vector columns (ending with '_vec')\n",
    "    categorical_vec_cols = [c for c in all_cols if c.endswith('_vec')]\n",
    "    print(f\"Categorical Vector Columns: {categorical_vec_cols}\")\n",
    "\n",
    "    # Identify numerical columns for scaling\n",
    "    # Exclude IDs, the target 'has_purchased', and leakage risks for classification\n",
    "    numerical_cols_to_scale = [\n",
    "        'age', 'tenure_days', 'price', 'product_avg_rating', 'previous_visits',\n",
    "        'view_count', 'add_to_cart_count', 'review_count',\n",
    "        'total_interactions', 'total_time_spent',\n",
    "        'interaction_time_span_days', 'days_since_first_interaction',\n",
    "        'avg_user_rating'\n",
    "        # Excluded: 'purchase_count', 'total_purchase_amount', 'avg_purchase_amount' (leakage risk for classifier)\n",
    "    ]\n",
    "    print(f\"Numerical Columns to Scale: {numerical_cols_to_scale}\")\n",
    "\n",
    "    # Identify target columns\n",
    "    classification_target_col = \"has_purchased\"\n",
    "    regression_target_col = \"total_purchase_amount\"\n",
    "    print(f\"Classification Target: {classification_target_col}\")\n",
    "    print(f\"Regression Target: {regression_target_col}\")\n",
    "\n",
    "    # Columns to exclude from feature vector assembly (IDs, targets, potential leakage for specific tasks)\n",
    "    exclude_cols = ['customer_id', 'product_id', classification_target_col, regression_target_col, 'avg_purchase_amount', 'purchase_count']\n",
    "    print(f\"Columns generally excluded from direct feature assembly: {exclude_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab82073a-8eca-4528-a4f0-79487e275332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Numerical Feature Scaling\n",
    "\n",
    "Scaling numerical features is crucial because many ML algorithms are sensitive to the magnitude of feature values. Algorithms like Logistic Regression, SVMs, and PCA can perform poorly or converge slowly if features are on vastly different scales. Standardization (using `StandardScaler`) is a common technique that scales data to have zero mean and unit variance.\n",
    "\n",
    "We will:\n",
    "1. Assemble all numerical features needing scaling into a single vector.\n",
    "2. Apply `StandardScaler` to this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfee0eb-0d56-4cf5-b22f-03de8e86aa58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after Scaling:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- gender_vec: vector (nullable = true)\n",
      " |-- country_vec: vector (nullable = true)\n",
      " |-- membership_level_vec: vector (nullable = true)\n",
      " |-- category_vec: vector (nullable = true)\n",
      " |-- device_vec: vector (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- tenure_days: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_avg_rating: double (nullable = true)\n",
      " |-- previous_visits: long (nullable = true)\n",
      " |-- view_count: long (nullable = true)\n",
      " |-- add_to_cart_count: long (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- purchase_count: long (nullable = true)\n",
      " |-- total_interactions: long (nullable = true)\n",
      " |-- total_time_spent: long (nullable = true)\n",
      " |-- interaction_time_span_days: double (nullable = true)\n",
      " |-- days_since_first_interaction: double (nullable = true)\n",
      " |-- total_purchase_amount: double (nullable = true)\n",
      " |-- avg_purchase_amount: double (nullable = true)\n",
      " |-- avg_user_rating: double (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      " |-- unscaled_numerical_features: vector (nullable = true)\n",
      " |-- scaled_numerical_features: vector (nullable = true)\n",
      "\n",
      "Sample Data with Scaled Numerical Features:\n",
      "+-----------+----------+-------------------------+\n",
      "|customer_id|product_id|scaled_numerical_features|\n",
      "+-----------+----------+-------------------------+\n",
      "|          2|       485|     [0.24193548387096...|\n",
      "|          4|       312|     [0.56451612903225...|\n",
      "|          4|       616|     [0.56451612903225...|\n",
      "|          5|       825|     [0.22580645161290...|\n",
      "|          6|       143|     [0.22580645161290...|\n",
      "+-----------+----------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # 1. Assemble numerical features into an intermediate vector\n",
    "    num_vector_assembler = VectorAssembler(\n",
    "        inputCols=numerical_cols_to_scale,\n",
    "        outputCol=\"unscaled_numerical_features\",\n",
    "        handleInvalid=\"keep\" # 'keep' preserves rows with nulls, but StandardScaler might fail.\n",
    "                           # 'skip' would drop rows with nulls in these columns.\n",
    "                           # Assuming M2N1 handled nulls, 'keep' is okay.\n",
    "                           # If nulls were possible, use 'skip' or impute first.\n",
    "    )\n",
    "    df_with_unscaled_vec = num_vector_assembler.transform(features_df)\n",
    "\n",
    "# 2. Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(\n",
    "        inputCol=\"unscaled_numerical_features\",\n",
    "        outputCol=\"scaled_numerical_features\"\n",
    "        # min=0.0, max=1.0 are the defaults\n",
    "    )\n",
    "    scaler_model = scaler.fit(df_with_unscaled_vec)\n",
    "    scaled_df = scaler_model.transform(df_with_unscaled_vec)\n",
    "\n",
    "    print(\"Schema after Scaling:\")\n",
    "    scaled_df.printSchema()\n",
    "    print(\"Sample Data with Scaled Numerical Features:\")\n",
    "    scaled_df.select('customer_id', 'product_id', 'scaled_numerical_features').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6dde357-5a81-4467-9ad8-dbb6172481b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Prepare Classification Dataset\n",
    "\n",
    "For the classification task (predicting `has_purchased`), we need to assemble the final feature vector. This vector will include:\n",
    "- The `scaled_numerical_features` vector.\n",
    "- All the one-hot encoded categorical vectors (`_vec` columns).\n",
    "\n",
    "**Important:** We must **exclude** features that would cause target leakage. The columns `purchase_count`, `total_purchase_amount`, and `avg_purchase_amount` are potential leakage risks for predicting `has_purchased`. These should **not** be included in the input features for the classification model, even in their scaled form (they were excluded from `numerical_cols_to_scale` earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b472fc13-ff5e-419d-9edf-9cc6a3333428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns for Classification Features: ['scaled_numerical_features', 'gender_vec', 'country_vec', 'membership_level_vec', 'category_vec', 'device_vec']\n",
      "Classification Dataset Schema:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      "\n",
      "Sample Classification Data:\n",
      "+-----------+----------+--------------------+-------------+\n",
      "|customer_id|product_id|            features|has_purchased|\n",
      "+-----------+----------+--------------------+-------------+\n",
      "|          2|       485|(43,[0,1,2,3,6,7,...|            0|\n",
      "|          4|       312|(43,[0,1,2,3,4,6,...|            0|\n",
      "|          4|       616|(43,[0,1,2,3,4,7,...|            1|\n",
      "|          5|       825|(43,[0,1,2,3,4,6,...|            1|\n",
      "|          6|       143|(43,[0,1,2,3,4,7,...|            0|\n",
      "+-----------+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Define columns for the classification feature vector\n",
    "    # Includes scaled numerical vector and all categorical vectors\n",
    "    classification_feature_cols = [\"scaled_numerical_features\"] + categorical_vec_cols\n",
    "    print(f\"Columns for Classification Features: {classification_feature_cols}\")\n",
    "\n",
    "    # Assemble the final classification feature vector\n",
    "    classification_assembler = VectorAssembler(\n",
    "        inputCols=classification_feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\" # Consistent handling with scaler input\n",
    "    )\n",
    "    classification_prepared_df = classification_assembler.transform(scaled_df)\n",
    "\n",
    "    # Select final columns for the classification dataset\n",
    "    classification_final_df = classification_prepared_df.select(\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"features\",\n",
    "        classification_target_col # Target variable 'has_purchased'\n",
    "    )\n",
    "\n",
    "    print(\"Classification Dataset Schema:\")\n",
    "    classification_final_df.printSchema()\n",
    "    print(\"Sample Classification Data:\")\n",
    "    classification_final_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8332e48d-7dad-499b-b6ea-e58126f7d9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Prepare Regression Dataset\n",
    "\n",
    "For the regression task (predicting `total_purchase_amount`), we first need to filter the data to include only instances where a purchase actually occurred (`has_purchased == 1`).\n",
    "\n",
    "The feature vector for regression will include:\n",
    "- The `scaled_numerical_features` vector.\n",
    "- All the one-hot encoded categorical vectors (`_vec` columns).\n",
    "\n",
    "We must exclude the target variable (`total_purchase_amount`) and features directly derived from it (like `avg_purchase_amount`) from the input `features` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393b4ba0-a112-41b9-a669-8051c73210ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for regression (purchases only): 25238\n",
      "Columns for Regression Features: ['scaled_numerical_features', 'gender_vec', 'country_vec', 'membership_level_vec', 'category_vec', 'device_vec']\n",
      "Regression Dataset Schema:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- total_purchase_amount: double (nullable = true)\n",
      "\n",
      "Sample Regression Data:\n",
      "+-----------+----------+--------------------+---------------------+\n",
      "|customer_id|product_id|            features|total_purchase_amount|\n",
      "+-----------+----------+--------------------+---------------------+\n",
      "|          4|       616|(43,[0,1,2,3,4,7,...|               281.39|\n",
      "|          5|       825|(43,[0,1,2,3,4,6,...|               232.21|\n",
      "|          7|       584|(43,[0,1,2,3,4,7,...|               656.01|\n",
      "|          7|       615|(43,[0,1,2,3,4,7,...|               205.55|\n",
      "|          9|       340|(43,[0,1,2,3,4,7,...|               496.87|\n",
      "+-----------+----------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # 1. Filter data for instances where a purchase occurred\n",
    "    regression_filtered_df = scaled_df.filter(col(classification_target_col) == 1)\n",
    "    print(f\"Number of rows for regression (purchases only): {regression_filtered_df.count()}\")\n",
    "\n",
    "    # 2. Define columns for the regression feature vector\n",
    "    # These are the *same* inputs as classification in this setup, as potentially leaky features\n",
    "    # were already excluded during scaling definition or are targets themselves.\n",
    "    regression_feature_cols = [\"scaled_numerical_features\"] + categorical_vec_cols\n",
    "    print(f\"Columns for Regression Features: {regression_feature_cols}\") # Same as classification\n",
    "\n",
    "    # 3. Assemble the final regression feature vector\n",
    "    # We can reuse the classification_assembler definition if inputs are identical,\n",
    "    # but defining it again makes the step clearer.\n",
    "    regression_assembler = VectorAssembler(\n",
    "        inputCols=regression_feature_cols,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\" # Consistent handling\n",
    "    )\n",
    "    regression_prepared_df = regression_assembler.transform(regression_filtered_df)\n",
    "\n",
    "    # 4. Select final columns for the regression dataset\n",
    "    regression_final_df = regression_prepared_df.select(\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"features\",\n",
    "        regression_target_col # Target variable 'total_purchase_amount'\n",
    "    )\n",
    "\n",
    "    print(\"Regression Dataset Schema:\")\n",
    "    regression_final_df.printSchema()\n",
    "    print(\"Sample Regression Data:\")\n",
    "    regression_final_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c65e4717-1bfe-475d-a424-04629b87ac5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Save Outputs\n",
    "\n",
    "Save the prepared classification and regression datasets to new tables for use in the next notebook (Module 2, Notebook 3: PCA and Feature Selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5892516c-b902-43b8-aed5-5bc770f57164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving classification data to ecommerce.classification_scaled_features...\n",
      "Classification data saved.\n",
      "Saving regression data to ecommerce.regression_scaled_features...\n",
      "Regression data saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Define output table names\n",
    "    classification_output_table = \"ecommerce.classification_scaled_features\"\n",
    "    regression_output_table = \"ecommerce.regression_scaled_features\"\n",
    "\n",
    "    # Save classification data\n",
    "    print(f\"Saving classification data to {classification_output_table}...\")\n",
    "    classification_final_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .saveAsTable(f\"{classification_output_table}\")\n",
    "    print(\"Classification data saved.\")\n",
    "\n",
    "    # Save regression data\n",
    "    print(f\"Saving regression data to {regression_output_table}...\")\n",
    "    regression_final_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .saveAsTable(f\"{regression_output_table}\")\n",
    "    print(\"Regression data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "795ce925-d7c3-412c-818a-bc8243ee2f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Summary & Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded the feature-engineered data from M2N1.\n",
    "2. Identified numerical columns requiring scaling and categorical vector columns.\n",
    "3. Applied `StandardScaler` to the numerical features after verifying no nulls were present in those columns.\n",
    "4. Assembled distinct feature vectors for classification and regression tasks, carefully considering potential target leakage for the classification model.\n",
    "5. Saved the two prepared datasets (`classification_scaled_features` and `regression_scaled_features`) for the next stage.\n",
    "\n",
    "**Next:** In Module 2, Notebook 3, we will explore feature selection techniques (like Chi-Square selection) and Principal Component Analysis (PCA) using these scaled datasets to potentially reduce dimensionality and improve model performance and interpretability."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "module2_notebook2_numerical_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
