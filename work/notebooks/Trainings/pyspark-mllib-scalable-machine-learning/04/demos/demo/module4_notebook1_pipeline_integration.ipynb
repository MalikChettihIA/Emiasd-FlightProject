{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc375279-8734-4d27-ba3e-15b7960b52ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 4 Notebook 1: Pipeline Integration for Regression\n",
    "\n",
    "**Objective:** Consolidate the feature engineering, scaling, and model training steps for our linear regression (purchase amount prediction) task into a single, end-to-end PySpark ML `Pipeline`.\n",
    "\n",
    "**Recap:** In previous modules, we performed these steps individually:\n",
    "1. **M2N1:** Joined data, engineered customer journey features (counts, time, purchase history) and categorical features.\n",
    "2. **M2N2:** Scaled numerical features.\n",
    "3. **M2N3:** Assembled features, split data.\n",
    "4. **M3N2:** Trained a baseline `LinearRegression` model.\n",
    "5. **M3N3:** Tuned hyperparameters for `LinearRegression`.\n",
    "\n",
    "**Goal:** Build a `Pipeline` that takes the raw interaction, customer, and product data as input and outputs predictions using the optimized regression model found in M3N3. This demonstrates a more streamlined and production-ready approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9cd8d4a-0c90-49a7-be95-a45260125ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, avg, min, max, datediff, first, lit, when, expr, unix_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81021162-2b2a-4b2b-8775-cb484f5e3d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load and Prepare Raw Data\n",
    "\n",
    "We start by loading the raw data sources and replicating the aggregation logic from Module 2 Notebook 1 to create our base dataset for the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76ae67f-2930-4ab4-96ec-3f11cf27d28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 25238 records for regression pipeline.\nroot\n |-- age: integer (nullable = true)\n |-- tenure_days: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- previous_visits: long (nullable = true)\n |-- view_count: long (nullable = false)\n |-- add_to_cart_count: long (nullable = false)\n |-- review_count: long (nullable = false)\n |-- total_interactions: long (nullable = false)\n |-- total_time_spent: long (nullable = true)\n |-- interaction_time_span_days: double (nullable = false)\n |-- avg_user_rating: double (nullable = false)\n |-- gender: string (nullable = true)\n |-- country: string (nullable = true)\n |-- membership_level: string (nullable = true)\n |-- category: string (nullable = true)\n |-- total_purchase_amount: double (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "interactions_df = spark.table(\"ecommerce.interactions\")\n",
    "customers_df = spark.table(\"ecommerce.customers\")\n",
    "products_df = spark.table(\"ecommerce.products\")\n",
    "\n",
    "# --- Replicating Feature Engineering Logic from M2N1 --- \n",
    "\n",
    "# Calculate time differences and interaction counts\n",
    "window_spec = Window.partitionBy(\"customer_id\", \"product_id\").orderBy(\"timestamp\")\n",
    "\n",
    "interactions_with_time = interactions_df \\\n",
    "    .withColumn(\"first_interaction_time\", min(\"timestamp\").over(window_spec)) \\\n",
    "    .withColumn(\"last_interaction_time\", max(\"timestamp\").over(window_spec))\n",
    "\n",
    "# Aggregate interactions at customer-product level\n",
    "customer_product_agg = interactions_with_time \\\n",
    "    .groupBy(\"customer_id\", \"product_id\") \\\n",
    "    .agg(\n",
    "        count(when(col(\"interaction_type\") == \"view\", 1)).alias(\"view_count\"),\n",
    "        count(when(col(\"interaction_type\") == \"add_to_cart\", 1)).alias(\"add_to_cart_count\"),\n",
    "        count(when(col(\"interaction_type\") == \"review\", 1)).alias(\"review_count\"),\n",
    "        count(when(col(\"interaction_type\") == \"purchase\", 1)).alias(\"purchase_count\"),\n",
    "        count(\"interaction_type\").alias(\"total_interactions\"),\n",
    "        sum(\"time_spent_seconds\").alias(\"total_time_spent\"),\n",
    "        sum(when(col(\"interaction_type\") == \"purchase\", col(\"purchase_amount\")).otherwise(0)).alias(\"total_purchase_amount\"),\n",
    "        avg(when(col(\"interaction_type\") == \"review\", col(\"user_rating\"))).alias(\"avg_user_rating\"), # Needs imputation later\n",
    "        first(\"first_interaction_time\").alias(\"first_interaction_time\"),\n",
    "        first(\"last_interaction_time\").alias(\"last_interaction_time\"),\n",
    "        first(\"previous_visits\").alias(\"previous_visits\") # Assuming it's consistent per customer-product pair\n",
    "    )\n",
    "\n",
    "# Calculate derived time features \n",
    "customer_product_agg = customer_product_agg \\\n",
    "    .withColumn(\"interaction_time_span_seconds\", unix_timestamp(col(\"last_interaction_time\")) - unix_timestamp(col(\"first_interaction_time\"))) \\\n",
    "    .withColumn(\"interaction_time_span_days\", col(\"interaction_time_span_seconds\") / (60*60*24)) \\\n",
    "    .withColumn(\"avg_purchase_amount\", \n",
    "                when(col(\"purchase_count\") > 0, col(\"total_purchase_amount\") / col(\"purchase_count\"))\n",
    "                .otherwise(0.0))\n",
    "\n",
    "# --- Impute Missing Values (Simplified for Pipeline Demo) --- \n",
    "# In a real pipeline, imputation might be a separate stage (e.g., Imputer)\n",
    "# Here, we fill directly for simplicity, assuming logic from M2N2\n",
    "\n",
    "# Impute avg_user_rating (e.g., with global mean or a fixed value like 3.0)\n",
    "# Calculating global mean requires an extra step, let's use a fixed value for demo\n",
    "customer_product_agg = customer_product_agg.fillna(3.0, subset=[\"avg_user_rating\"])\n",
    "# Fill other potential nulls introduced by aggregation (e.g., counts, sums)\n",
    "numeric_cols_to_fill_zero = [\n",
    "    \"view_count\", \"add_to_cart_count\", \"review_count\", \"purchase_count\",\n",
    "    \"total_interactions\", \"total_time_spent\", \"total_purchase_amount\",\n",
    "    \"interaction_time_span_seconds\", \"interaction_time_span_days\",\n",
    "    \"avg_purchase_amount\", \"previous_visits\"\n",
    "]\n",
    "customer_product_agg = customer_product_agg.fillna(0, subset=numeric_cols_to_fill_zero)\n",
    "\n",
    "# --- Join with Customer and Product Data --- \n",
    "base_df = customer_product_agg \\\n",
    "    .join(customers_df, \"customer_id\") \\\n",
    "    .join(products_df, \"product_id\")\n",
    "\n",
    "# --- Filter for Regression Task --- \n",
    "# Keep only records where a purchase actually occurred for amount prediction\n",
    "regression_base_data = base_df.filter(col(\"purchase_count\") > 0)\n",
    "\n",
    "# --- Select Final Columns --- \n",
    "# Select features needed for the regression model (consistent with M2/M3)\n",
    "# Exclude the target variable itself ('total_purchase_amount') from features\n",
    "feature_columns = [\n",
    "    # Numerical (Original - to be scaled)\n",
    "    \"age\", \"tenure_days\", \"price\", \"avg_rating\", # product avg_rating\n",
    "    \"previous_visits\", \n",
    "    # Journey Features (Engineered - to be scaled)\n",
    "    \"view_count\", \"add_to_cart_count\", \"review_count\", \n",
    "    \"total_interactions\", \"total_time_spent\", \n",
    "    \"interaction_time_span_days\",\n",
    "    \"avg_user_rating\", # customer avg rating per product\n",
    "    # Categorical (Original - to be indexed/encoded)\n",
    "    \"gender\", \"country\", \"membership_level\", \"category\" # Assuming device requires complex handling not suitable for basic pipeline demo\n",
    "]\n",
    "target_column = \"total_purchase_amount\"\n",
    "\n",
    "regression_input_data = regression_base_data.select(feature_columns + [target_column])\n",
    "\n",
    "print(f\"Prepared {regression_input_data.count()} records for regression pipeline.\")\n",
    "regression_input_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4bab43-8c9b-45c5-bd94-3c6bd97fa051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Define Pipeline Stages\n",
    "\n",
    "Now, we define each step of our data processing and modeling workflow as a stage in the pipeline.\n",
    "\n",
    "1.  **Categorical Handling:** `StringIndexer` -> `OneHotEncoder`\n",
    "2.  **Numerical Handling:** `VectorAssembler` (raw numerical) -> `StandardScaler`\n",
    "3.  **Final Assembly:** `VectorAssembler` (scaled numerical + OHE categorical)\n",
    "4.  **Model:** `LinearRegression` (with best hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00441bb2-dc5b-45a2-91f2-3d42b7aabc40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 9 stages for the pipeline.\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns from our selected features\n",
    "categorical_cols = [\"gender\", \"country\", \"membership_level\", \"category\"]\n",
    "numerical_cols = [\n",
    "    \"age\", \"tenure_days\", \"price\", \"avg_rating\", \"previous_visits\",\n",
    "    \"view_count\", \"add_to_cart_count\", \"review_count\", \n",
    "    \"total_interactions\", \"total_time_spent\", \n",
    "    \"interaction_time_span_days\", \"avg_user_rating\"\n",
    "]\n",
    "\n",
    "# --- Stage 1: Categorical Feature Processing --- \n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\") \n",
    "    for col in categorical_cols\n",
    "]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{col}_index\" for col in categorical_cols],\n",
    "    outputCols=[f\"{col}_vec\" for col in categorical_cols],\n",
    "    dropLast=True # Common practice \n",
    ")\n",
    "\n",
    "# --- Stage 2: Numerical Feature Processing --- \n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCol=\"unscaled_numerical_features\",\n",
    "    handleInvalid=\"keep\" # Or use 'skip' or imputation\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_numerical_features\",\n",
    "    outputCol=\"scaled_numerical_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# --- Stage 3: Final Feature Assembly --- \n",
    "final_feature_cols = [\"scaled_numerical_features\"] + [f\"{col}_vec\" for col in categorical_cols]\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=final_feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# --- Stage 4: Model Definition --- \n",
    "# Use the best hyperparameters found in Module 3 Notebook 3\n",
    "# Example values - replace with actual best params from M3N3\n",
    "best_maxIter = 20 \n",
    "best_regParam = 0.01\n",
    "best_elasticNetParam = 0.0 # Corresponds to L2 regularization (Ridge)\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_column,\n",
    "    maxIter=best_maxIter,\n",
    "    regParam=best_regParam,\n",
    "    elasticNetParam=best_elasticNetParam\n",
    ")\n",
    "\n",
    "# Combine all stages into a list\n",
    "all_stages = indexers + [encoder, numerical_assembler, scaler, final_assembler, lr]\n",
    "print(f\"Defined {len(all_stages)} stages for the pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8fce743-966f-408c-8d83-364832a531e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Create and Train the Pipeline\n",
    "\n",
    "We assemble the stages into a `Pipeline` object and fit it to the training data. This process trains the indexers, learns the scaling parameters, and trains the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aeb4642-c634-4d67-8341-09504678752a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 20299\nTest data count: 4939\nFitting the pipeline...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a592d2cadd473f917750a28a12362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de46942f1c0425b8afff2e8162e67d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitting complete.\n"
     ]
    }
   ],
   "source": [
    "# Create the Pipeline\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = regression_input_data.randomSplit([0.8, 0.2], seed=SEED)\n",
    "\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "print(\"Fitting the pipeline...\")\n",
    "pipelineModel = pipeline.fit(train_data)\n",
    "print(\"Pipeline fitting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b194d399-17cf-4b3d-827f-f1588d965564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Apply Pipeline for Predictions\n",
    "\n",
    "Use the fitted `pipelineModel` to transform the test data. This applies all the learned transformations and generates predictions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78375af6-4191-4987-ab12-13560bcec037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>total_purchase_amount</th><th>prediction</th><th>features</th></tr></thead><tbody><tr><td>49.16</td><td>45.41873102566308</td><td>Map(vectorType -> sparse, length -> 39, indices -> List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 16, 25, 35), values -> List(-1.5085758549084674, -1.4524684713685045, -0.7679363155102512, -0.042036733454782794, 0.07336906942525259, -0.05422856352508102, -1.0263281838169567, 0.9966548401187559, -0.027705520508331027, -0.47281951575762976, 1.4149879965977856, 1.0, 1.0, 1.0, 1.0))</td></tr><tr><td>355.44</td><td>364.30572797598495</td><td>Map(vectorType -> sparse, length -> 39, indices -> List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 19, 26, 29), values -> List(-1.5085758549084674, -1.3927622473103776, 0.26979550322483287, -0.7532590280468658, -1.2939594695424854, -0.05422856352508102, 0.9669873940383505, -0.9993332198025627, -0.027705520508331027, 2.5199788725051966, -0.6248383701675808, 1.0, 1.0, 1.0, 1.0))</td></tr><tr><td>71.25</td><td>59.951812608821996</td><td>Map(vectorType -> sparse, length -> 39, indices -> List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 17, 26, 34), values -> List(-1.5085758549084674, -1.3600201244397918, -0.6691096674426527, -1.6422868962869699, 0.07336906942525259, -0.05422856352508102, -1.0263281838169567, -0.9993332198025627, -1.410987848419265, -0.6526194189284584, -0.6248383701675808, 1.0, 1.0, 1.0, 1.0))</td></tr><tr><td>121.38</td><td>131.88300002720143</td><td>Map(vectorType -> sparse, length -> 39, indices -> List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 23, 28, 34), values -> List(-1.5085758549084674, -1.3484640810737027, -0.31910729120745734, -1.286675748990928, -1.2939594695424854, -0.05422856352508102, 0.9669873940383505, -0.9993332198025627, -0.027705520508331027, -0.35391957978982364, -0.6248383701675808, 1.0, 1.0, 1.0, 1.0))</td></tr><tr><td>151.47</td><td>147.56074593390602</td><td>Map(vectorType -> sparse, length -> 39, indices -> List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 23, 27, 32), values -> List(-1.5085758549084674, -1.3388340449352951, -0.21778396992551943, -0.5754534543988449, -1.2939594695424854, -0.05422856352508102, -1.0263281838169567, 0.9966548401187559, -0.027705520508331027, -0.37421956885749785, -0.9920311188926247, 1.0, 1.0, 1.0, 1.0))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         49.16,
         45.41873102566308,
         {
          "indices": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           11,
           12,
           16,
           25,
           35
          ],
          "length": 39,
          "values": [
           -1.5085758549084674,
           -1.4524684713685045,
           -0.7679363155102512,
           -0.042036733454782794,
           0.07336906942525259,
           -0.05422856352508102,
           -1.0263281838169567,
           0.9966548401187559,
           -0.027705520508331027,
           -0.47281951575762976,
           1.4149879965977856,
           1.0,
           1.0,
           1.0,
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         355.44,
         364.30572797598495,
         {
          "indices": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           11,
           12,
           19,
           26,
           29
          ],
          "length": 39,
          "values": [
           -1.5085758549084674,
           -1.3927622473103776,
           0.26979550322483287,
           -0.7532590280468658,
           -1.2939594695424854,
           -0.05422856352508102,
           0.9669873940383505,
           -0.9993332198025627,
           -0.027705520508331027,
           2.5199788725051966,
           -0.6248383701675808,
           1.0,
           1.0,
           1.0,
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         71.25,
         59.951812608821996,
         {
          "indices": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           11,
           13,
           17,
           26,
           34
          ],
          "length": 39,
          "values": [
           -1.5085758549084674,
           -1.3600201244397918,
           -0.6691096674426527,
           -1.6422868962869699,
           0.07336906942525259,
           -0.05422856352508102,
           -1.0263281838169567,
           -0.9993332198025627,
           -1.410987848419265,
           -0.6526194189284584,
           -0.6248383701675808,
           1.0,
           1.0,
           1.0,
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         121.38,
         131.88300002720143,
         {
          "indices": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           11,
           12,
           23,
           28,
           34
          ],
          "length": 39,
          "values": [
           -1.5085758549084674,
           -1.3484640810737027,
           -0.31910729120745734,
           -1.286675748990928,
           -1.2939594695424854,
           -0.05422856352508102,
           0.9669873940383505,
           -0.9993332198025627,
           -0.027705520508331027,
           -0.35391957978982364,
           -0.6248383701675808,
           1.0,
           1.0,
           1.0,
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         151.47,
         147.56074593390602,
         {
          "indices": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           11,
           13,
           23,
           27,
           32
          ],
          "length": 39,
          "values": [
           -1.5085758549084674,
           -1.3388340449352951,
           -0.21778396992551943,
           -0.5754534543988449,
           -1.2939594695424854,
           -0.05422856352508102,
           -1.0263281838169567,
           0.9966548401187559,
           -0.027705520508331027,
           -0.37421956885749785,
           -0.9920311188926247,
           1.0,
           1.0,
           1.0,
           1.0
          ],
          "vectorType": "sparse"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "total_purchase_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{}}",
         "name": "prediction",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"scaled_numerical_features_0\"},{\"idx\":1,\"name\":\"scaled_numerical_features_1\"},{\"idx\":2,\"name\":\"scaled_numerical_features_2\"},{\"idx\":3,\"name\":\"scaled_numerical_features_3\"},{\"idx\":4,\"name\":\"scaled_numerical_features_4\"},{\"idx\":5,\"name\":\"scaled_numerical_features_5\"},{\"idx\":6,\"name\":\"scaled_numerical_features_6\"},{\"idx\":7,\"name\":\"scaled_numerical_features_7\"},{\"idx\":8,\"name\":\"scaled_numerical_features_8\"},{\"idx\":9,\"name\":\"scaled_numerical_features_9\"},{\"idx\":10,\"name\":\"scaled_numerical_features_10\"},{\"idx\":11,\"name\":\"scaled_numerical_features_11\"}],\"binary\":[{\"idx\":12,\"name\":\"gender_vec_F\"},{\"idx\":13,\"name\":\"gender_vec_M\"},{\"idx\":14,\"name\":\"gender_vec_Other\"},{\"idx\":15,\"name\":\"country_vec_US\"},{\"idx\":16,\"name\":\"country_vec_UK\"},{\"idx\":17,\"name\":\"country_vec_CA\"},{\"idx\":18,\"name\":\"country_vec_FR\"},{\"idx\":19,\"name\":\"country_vec_DE\"},{\"idx\":20,\"name\":\"country_vec_JP\"},{\"idx\":21,\"name\":\"country_vec_AU\"},{\"idx\":22,\"name\":\"country_vec_MX\"},{\"idx\":23,\"name\":\"country_vec_BR\"},{\"idx\":24,\"name\":\"country_vec_IN\"},{\"idx\":25,\"name\":\"membership_level_vec_Silver\"},{\"idx\":26,\"name\":\"membership_level_vec_Bronze\"},{\"idx\":27,\"name\":\"membership_level_vec_Gold\"},{\"idx\":28,\"name\":\"membership_level_vec_Platinum\"},{\"idx\":29,\"name\":\"category_vec_Automotive\"},{\"idx\":30,\"name\":\"category_vec_Sports\"},{\"idx\":31,\"name\":\"category_vec_Electronics\"},{\"idx\":32,\"name\":\"category_vec_Home & Kitchen\"},{\"idx\":33,\"name\":\"category_vec_Health\"},{\"idx\":34,\"name\":\"category_vec_Clothing\"},{\"idx\":35,\"name\":\"category_vec_Beauty\"},{\"idx\":36,\"name\":\"category_vec_Toys\"},{\"idx\":37,\"name\":\"category_vec_Books\"},{\"idx\":38,\"name\":\"category_vec_Grocery\"}]},\"num_attrs\":39}}",
         "name": "features",
         "type": "{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "print(\"Generating predictions on test data...\")\n",
    "predictions = pipelineModel.transform(test_data)\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(target_column, \"prediction\", \"features\").limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a4a745-324d-464b-b0e0-01b2928dd4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Evaluate the Pipeline Model\n",
    "\n",
    "Evaluate the performance of the model trained within the pipeline using standard regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7beef4f1-1f20-4ecc-a19f-a15e5f0a2339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Model Performance on Test Data:\n  Root Mean Squared Error (RMSE): 20.51\n  Mean Absolute Error (MAE): 12.35\n  R-squared (R²): 0.9941\nThese metrics should be very close to the results obtained for the tuned model in M3N3.\n"
     ]
    }
   ],
   "source": [
    "# Create an evaluator\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"Pipeline Model Performance on Test Data:\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"  R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "# Compare with M3N3 results (mention if they are similar/identical)\n",
    "print(\"These metrics should be very close to the results obtained for the tuned model in M3N3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cd2741e-ba40-4b40-8849-482c7bfda978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Save Pipeline Model with MLflow\n",
    "\n",
    "Finally, we save the trained `PipelineModel` using MLflow. This packages the entire pipeline (including all preprocessing stages and the trained model) for easy loading and deployment later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ca98fd-da4b-47c4-a41e-3f5232c5fa35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run to log pipeline model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/21 01:18:19 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f6ba78b24d491eac75b644fdfbc449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/21 01:18:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/232784118800640/2a473aa8df5f45209a4dd93b22d7fc63/artifacts/ecommerce_regression_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74e96020af342209dc0234641039dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/21 01:18:52 INFO mlflow.tracking._tracking_service.client: 🏃 View run stylish-wren-332 at: adb-2187061022924749.9.azuredatabricks.net/ml/experiments/232784118800640/runs/2a473aa8df5f45209a4dd93b22d7fc63.\n2025/04/21 01:18:52 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: adb-2187061022924749.9.azuredatabricks.net/ml/experiments/232784118800640.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline model saved successfully under MLflow Run ID: 2a473aa8df5f45209a4dd93b22d7fc63\nArtifact Path: 'ecommerce_regression_model'\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run to log the model\n",
    "print(\"Starting MLflow run to log pipeline model...\")\n",
    "with mlflow.start_run() as run:\n",
    "    # Log the pipeline model\n",
    "    mlflow.spark.log_model(pipelineModel, \"ecommerce_regression_model\")\n",
    "    \n",
    "    # Get the run ID for reference in the next notebook\n",
    "    run_id = run.info.run_id\n",
    "    print(f'Pipeline model saved successfully under MLflow Run ID: {run_id}')\n",
    "    print(\"Artifact Path: 'ecommerce_regression_model'\")\n",
    "\n",
    "# You can optionally save the run_id to a file or variable for M4N2 \n",
    "# For now, we just print it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "563223f1-9a24-4719-930c-44d29f32a0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we successfully:\n",
    "*   Loaded raw data and replicated the necessary feature engineering for the regression task.\n",
    "*   Defined all preprocessing steps (indexing, encoding, scaling) and the regression model as stages.\n",
    "*   Assembled these stages into a single PySpark `Pipeline`.\n",
    "*   Trained the entire pipeline using `.fit()` on the training data.\n",
    "*   Applied the fitted pipeline using `.transform()` to generate predictions on the test data.\n",
    "*   Evaluated the model's performance, confirming consistency with previous results.\n",
    "\n",
    "**Benefits Demonstrated:**\n",
    "*   **Streamlined Workflow:** Combines multiple steps into one object.\n",
    "*   **Consistency:** Ensures the same transformations are applied during training and prediction.\n",
    "*   **Reduced Complexity:** Simplifies the process of applying the model to new data.\n",
    "*   **Deployment Ready:** The `pipelineModel` encapsulates the entire workflow, making it easier to save, load, and deploy (as we'll see in the next notebook).\n",
    "\n",
    "**Next Steps:** In Module 4 Notebook 2, we will focus on saving this `pipelineModel` and using it for distributed inference on new data."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "module4_notebook1_pipeline_integration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}