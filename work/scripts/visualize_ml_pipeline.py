#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Visualization script for ML Pipeline Results (K-Fold CV + Hold-out Test)

This script visualizes comprehensive metrics generated by the MLPipeline:
- Cross-validation fold metrics
- CV summary statistics
- Hold-out test set performance
- Best hyperparameters from grid search

Usage:
    python visualize_ml_pipeline.py <metrics_directory>

Example:
    python visualize_ml_pipeline.py /output/exp1_rf_pca/metrics
"""

import sys
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

def load_ml_pipeline_metrics(metrics_dir):
    """Load all ML Pipeline metrics CSV files"""
    metrics_dir = Path(metrics_dir)
    metrics = {}

    # Load CV fold metrics
    cv_folds_file = metrics_dir / "cv_fold_metrics.csv"
    if cv_folds_file.exists():
        metrics['cv_folds'] = pd.read_csv(cv_folds_file)
        print(f" Loaded: {cv_folds_file}")

    # Load CV summary (mean � std)
    cv_summary_file = metrics_dir / "cv_summary.csv"
    if cv_summary_file.exists():
        metrics['cv_summary'] = pd.read_csv(cv_summary_file)
        print(f" Loaded: {cv_summary_file}")

    # Load hold-out test metrics
    holdout_file = metrics_dir / "holdout_test_metrics.csv"
    if holdout_file.exists():
        metrics['holdout'] = pd.read_csv(holdout_file)
        print(f" Loaded: {holdout_file}")

    # Load best hyperparameters
    hp_file = metrics_dir / "best_hyperparameters.csv"
    if hp_file.exists():
        metrics['hyperparameters'] = pd.read_csv(hp_file)
        print(f" Loaded: {hp_file}")

    return metrics

def plot_cv_folds_detailed(metrics, output_dir):
    """Plot detailed metrics across CV folds"""
    if 'cv_folds' not in metrics:
        print("� No CV folds data found")
        return

    df = metrics['cv_folds']
    metric_cols = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()

    for idx, metric in enumerate(metric_cols):
        ax = axes[idx]

        # Line plot with markers
        ax.plot(df['fold'], df[metric] * 100, marker='o', linewidth=2,
                markersize=8, color='#3498db', alpha=0.8, label='Fold metric')

        # Add mean line
        mean_val = df[metric].mean() * 100
        std_val = df[metric].std() * 100
        ax.axhline(y=mean_val, color='#e74c3c', linestyle='--',
                   linewidth=2, alpha=0.7, label=f'Mean: {mean_val:.2f}%')

        # Add std dev band
        ax.fill_between(df['fold'], mean_val - std_val, mean_val + std_val,
                        alpha=0.2, color='#e74c3c', label=f'�1�: {std_val:.2f}%')

        # Styling
        ax.set_xlabel('Fold Number', fontweight='bold', fontsize=11)
        ax.set_ylabel(f'{metric.replace("_", " ").title()} (%)', fontweight='bold', fontsize=11)
        ax.set_title(f'{metric.replace("_", " ").title()} Across CV Folds',
                     fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)
        ax.set_xticks(df['fold'])
        ax.set_ylim([0, 105])

    # Remove extra subplot
    fig.delaxes(axes[5])

    plt.tight_layout()
    output_file = output_dir / "cv_folds_detailed.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f" Saved: {output_file}")
    plt.close()

def plot_cv_vs_holdout_comparison(metrics, output_dir):
    """Plot CV average vs hold-out test metrics comparison"""
    if 'cv_summary' not in metrics or 'holdout' not in metrics:
        print("� Missing CV summary or hold-out data")
        return

    cv_df = metrics['cv_summary']
    holdout_df = metrics['holdout']

    # Prepare data - only classification metrics
    main_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    cv_data = []
    holdout_data = []
    cv_std = []
    labels = []

    for metric in main_metrics:
        cv_row = cv_df[cv_df['metric'] == metric]
        holdout_row = holdout_df[holdout_df['metric'] == metric]

        if not cv_row.empty and not holdout_row.empty:
            cv_data.append(cv_row['mean'].values[0] * 100)
            cv_std.append(cv_row['std'].values[0] * 100)
            holdout_data.append(holdout_row['value'].values[0] * 100)
            labels.append(metric.replace('_', '\n').title())

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Plot 1: Bar chart comparison
    ax1 = axes[0]
    x = np.arange(len(labels))
    width = 0.35

    bars1 = ax1.bar(x - width/2, cv_data, width, yerr=cv_std,
                    label='CV Mean � Std', alpha=0.8, color='#2ecc71',
                    capsize=5, error_kw={'linewidth': 2, 'elinewidth': 2})
    bars2 = ax1.bar(x + width/2, holdout_data, width,
                    label='Hold-out Test', alpha=0.8, color='#3498db')

    # Add value labels
    for i, bar in enumerate(bars1):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + cv_std[i] + 1,
                f'{cv_data[i]:.1f}%',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    for i, bar in enumerate(bars2):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{holdout_data[i]:.1f}%',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    ax1.set_xlabel('Metric', fontweight='bold', fontsize=12)
    ax1.set_ylabel('Score (%)', fontweight='bold', fontsize=12)
    ax1.set_title('Cross-Validation vs Hold-out Test Performance',
                  fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(labels, fontsize=10)
    ax1.legend(fontsize=11, loc='lower right')
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim([0, 105])

    # Plot 2: Generalization gap analysis
    ax2 = axes[1]
    gaps = [cv - ho for cv, ho in zip(cv_data, holdout_data)]
    colors = ['#e74c3c' if abs(gap) > 5 else '#2ecc71' if abs(gap) < 2 else '#f39c12'
              for gap in gaps]

    bars = ax2.barh(labels, gaps, color=colors, alpha=0.8, edgecolor='black', linewidth=1)

    # Add value labels
    for i, (bar, gap) in enumerate(zip(bars, gaps)):
        width = bar.get_width()
        ax2.text(width, bar.get_y() + bar.get_height()/2,
                f' {gap:.2f}%' if width >= 0 else f'{gap:.2f}% ',
                ha='left' if width >= 0 else 'right',
                va='center', fontsize=9, fontweight='bold')

    ax2.set_xlabel('Gap (CV - Hold-out) [%]', fontweight='bold', fontsize=12)
    ax2.set_title('Generalization Gap Analysis', fontsize=14, fontweight='bold')
    ax2.axvline(x=0, color='black', linestyle='-', linewidth=1)
    ax2.axvline(x=2, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='�2% threshold')
    ax2.axvline(x=-2, color='orange', linestyle='--', linewidth=1, alpha=0.5)
    ax2.axvline(x=5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='�5% threshold')
    ax2.axvline(x=-5, color='red', linestyle='--', linewidth=1, alpha=0.5)
    ax2.legend(fontsize=9, loc='best')
    ax2.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    output_file = output_dir / "cv_vs_holdout_comparison.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f" Saved: {output_file}")
    plt.close()

def plot_box_plot_stability(metrics, output_dir):
    """Box plot showing distribution and stability of metrics across folds"""
    if 'cv_folds' not in metrics:
        return

    df = metrics['cv_folds']
    metric_cols = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    # Prepare data
    data_to_plot = [df[col].values * 100 for col in metric_cols]

    fig, ax = plt.subplots(figsize=(12, 7))

    bp = ax.boxplot(data_to_plot,
                    labels=[m.replace('_', '\n').title() for m in metric_cols],
                    patch_artist=True,
                    notch=True,
                    showmeans=True,
                    meanprops={'marker': 'D', 'markerfacecolor': 'red',
                              'markeredgecolor': 'red', 'markersize': 8},
                    medianprops={'linewidth': 2, 'color': 'darkblue'},
                    boxprops={'linewidth': 1.5},
                    whiskerprops={'linewidth': 1.5},
                    capprops={'linewidth': 1.5})

    # Color boxes with gradient
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(metric_cols)))
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax.set_ylabel('Score (%)', fontweight='bold', fontsize=12)
    ax.set_title('Distribution of Metrics Across CV Folds (Stability Analysis)',
                 fontsize=14, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim([0, 105])

    # Add legend
    from matplotlib.lines import Line2D
    from matplotlib.patches import Patch
    legend_elements = [
        Line2D([0], [0], color='darkblue', linewidth=2, label='Median'),
        Line2D([0], [0], marker='D', color='w', markerfacecolor='red',
               markersize=8, label='Mean'),
        Patch(facecolor='gray', alpha=0.3, label='IQR (25%-75%)')
    ]
    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)

    # Add stability assessment
    max_std = df[metric_cols].std().max() * 100
    if max_std < 1:
        stability = "Excellent"
        color = '#2ecc71'
    elif max_std < 2:
        stability = "Good"
        color = '#f39c12'
    else:
        stability = "Moderate"
        color = '#e74c3c'

    textstr = f'Stability: {stability}\nMax StdDev: {max_std:.2f}%'
    props = dict(boxstyle='round', facecolor=color, alpha=0.3, edgecolor=color, linewidth=2)
    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=11,
            verticalalignment='top', bbox=props, fontweight='bold')

    plt.tight_layout()
    output_file = output_dir / "cv_stability_boxplot.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f" Saved: {output_file}")
    plt.close()

def plot_confusion_matrix_holdout(metrics, output_dir):
    """Plot confusion matrix from hold-out test set"""
    if 'holdout' not in metrics:
        print("� No hold-out data found")
        return

    holdout_df = metrics['holdout']

    # Extract confusion matrix values
    try:
        tp = int(holdout_df[holdout_df['metric'] == 'true_positives']['value'].values[0])
        tn = int(holdout_df[holdout_df['metric'] == 'true_negatives']['value'].values[0])
        fp = int(holdout_df[holdout_df['metric'] == 'false_positives']['value'].values[0])
        fn = int(holdout_df[holdout_df['metric'] == 'false_negatives']['value'].values[0])
    except:
        print("� Could not extract confusion matrix values")
        return

    cm = np.array([[tn, fp], [fn, tp]])
    total = cm.sum()

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

    # Plot 1: Confusion Matrix (counts)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted\nNegative (0)', 'Predicted\nPositive (1)'],
                yticklabels=['Actual\nNegative (0)', 'Actual\nPositive (1)'],
                ax=ax1, cbar_kws={'label': 'Count'},
                annot_kws={'fontsize': 16, 'fontweight': 'bold'})

    ax1.set_title('Confusion Matrix - Hold-out Test (Counts)',
                 fontsize=14, fontweight='bold', pad=15)

    # Add percentage annotations
    for i in range(2):
        for j in range(2):
            percentage = cm[i, j] / total * 100
            ax1.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                   ha='center', va='center', fontsize=12, color='gray')

    # Plot 2: Normalized Confusion Matrix (percentages)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    sns.heatmap(cm_norm, annot=True, fmt='.1f', cmap='RdYlGn',
                xticklabels=['Predicted\nNegative (0)', 'Predicted\nPositive (1)'],
                yticklabels=['Actual\nNegative (0)', 'Actual\nPositive (1)'],
                ax=ax2, cbar_kws={'label': 'Percentage (%)'},
                annot_kws={'fontsize': 16, 'fontweight': 'bold'})

    ax2.set_title('Normalized Confusion Matrix (Row %)',
                 fontsize=14, fontweight='bold', pad=15)

    # Add performance metrics as text
    accuracy = (tp + tn) / total * 100
    precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    textstr = '\n'.join([
        'Performance:',
        f'Accuracy:  {accuracy:.2f}%',
        f'Precision: {precision:.2f}%',
        f'Recall:    {recall:.2f}%',
        f'F1-Score:  {f1:.2f}%'
    ])
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5, edgecolor='black', linewidth=2)
    ax2.text(1.35, 0.5, textstr, transform=ax2.transAxes, fontsize=11,
            verticalalignment='center', bbox=props, fontfamily='monospace')

    plt.tight_layout()
    output_file = output_dir / "confusion_matrix_holdout.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f" Saved: {output_file}")
    plt.close()

def plot_metrics_radar(metrics, output_dir):
    """Plot radar chart comparing CV and hold-out metrics"""
    if 'cv_summary' not in metrics or 'holdout' not in metrics:
        return

    cv_df = metrics['cv_summary']
    holdout_df = metrics['holdout']

    # Select metrics for radar
    radar_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

    cv_values = []
    holdout_values = []
    categories = []

    for metric in radar_metrics:
        cv_row = cv_df[cv_df['metric'] == metric]
        holdout_row = holdout_df[holdout_df['metric'] == metric]

        if not cv_row.empty and not holdout_row.empty:
            cv_values.append(cv_row['mean'].values[0])
            holdout_values.append(holdout_row['value'].values[0])
            categories.append(metric.replace('_', ' ').title())

    if not categories:
        return

    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

    # Number of variables
    N = len(categories)

    # Compute angle for each axis
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    # Plot data (close the loop)
    cv_plot = cv_values + cv_values[:1]
    holdout_plot = holdout_values + holdout_values[:1]

    ax.plot(angles, cv_plot, 'o-', linewidth=3, label='CV Mean',
            color='#2ecc71', markersize=10)
    ax.fill(angles, cv_plot, alpha=0.25, color='#2ecc71')

    ax.plot(angles, holdout_plot, 'o-', linewidth=3, label='Hold-out Test',
            color='#3498db', markersize=10)
    ax.fill(angles, holdout_plot, alpha=0.25, color='#3498db')

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, size=12, fontweight='bold')
    ax.set_ylim(0, 1)
    ax.set_title('Model Performance Radar Chart',
                 size=16, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
    ax.grid(True, linewidth=1.5, alpha=0.5)

    # Add concentric circles labels
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'], fontsize=10)

    plt.tight_layout()
    output_file = output_dir / "metrics_radar.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f" Saved: {output_file}")
    plt.close()

def plot_hyperparameters_summary(metrics, output_dir):
    """Plot best hyperparameters from grid search"""
    if 'hyperparameters' not in metrics:
        print("� No hyperparameters data found (Grid Search was likely disabled)")
        return

    hp_df = metrics['hyperparameters']

    fig, ax = plt.subplots(figsize=(10, max(6, len(hp_df) * 0.5)))

    # Create horizontal bar chart
    y_pos = np.arange(len(hp_df))

    # Format values for display
    hp_df['display_value'] = hp_df['value'].astype(str)

    # Create bars (just for visual representation, all same length)
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(hp_df)))
    ax.barh(y_pos, [1] * len(hp_df), color=colors, alpha=0.7,
            edgecolor='black', linewidth=1.5)

    ax.set_yticks(y_pos)
    ax.set_yticklabels(hp_df['parameter'], fontsize=11, fontweight='bold')
    ax.set_xlim([0, 1.5])
    ax.set_xticks([])  # Remove x-axis ticks

    # Add value labels
    for i, (_, row) in enumerate(hp_df.iterrows()):
        ax.text(0.5, i, f"  {row['display_value']}",
               va='center', ha='center', fontsize=12,
               fontweight='bold', color='white',
               bbox=dict(boxstyle='round,pad=0.5', facecolor='black', alpha=0.7))

    ax.set_title('Best Hyperparameters from Grid Search',
                 fontsize=14, fontweight='bold', pad=15)
    ax.set_xlabel('')

    # Add note
    note = "These are the optimal hyperparameters found through grid search\nbased on cross-validation performance"
    ax.text(0.5, -0.15, note, transform=ax.transAxes,
           fontsize=10, ha='center', style='italic', color='gray')

    plt.tight_layout()
    output_file = output_dir / "best_hyperparameters.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f" Saved: {output_file}")
    plt.close()

def generate_comprehensive_report(metrics, output_dir):
    """Generate comprehensive text summary report"""
    report = []
    report.append("=" * 80)
    report.append("ML PIPELINE COMPREHENSIVE PERFORMANCE REPORT")
    report.append("=" * 80)
    report.append("")

    # Hyperparameters (if available)
    if 'hyperparameters' in metrics:
        hp_df = metrics['hyperparameters']
        report.append("Best Hyperparameters (Grid Search):")
        report.append("-" * 80)
        for _, row in hp_df.iterrows():
            report.append(f"  {row['parameter']:30s}: {row['value']}")
        report.append("")

    # Cross-Validation Summary
    if 'cv_summary' in metrics:
        cv_df = metrics['cv_summary']
        report.append("Cross-Validation Results (K-Fold):")
        report.append("-" * 80)
        report.append(f"{'Metric':<20s} {'Mean':>12s} {'Std Dev':>12s} {'95% CI':>20s}")
        report.append("-" * 80)
        for _, row in cv_df.iterrows():
            mean_pct = row['mean'] * 100
            std_pct = row['std'] * 100
            ci_lower = mean_pct - 1.96 * std_pct
            ci_upper = mean_pct + 1.96 * std_pct
            report.append(f"{row['metric']:<20s} {mean_pct:>11.2f}% "
                         f"�{std_pct:>10.2f}% "
                         f"[{ci_lower:>6.2f}%, {ci_upper:>6.2f}%]")
        report.append("")

    # Per-Fold Results
    if 'cv_folds' in metrics:
        df_folds = metrics['cv_folds']
        num_folds = len(df_folds)
        report.append(f"Per-Fold Results ({num_folds} folds):")
        report.append("-" * 80)
        report.append(f"{'Fold':>6s} {'Accuracy':>12s} {'Precision':>12s} "
                     f"{'Recall':>12s} {'F1-Score':>12s} {'AUC-ROC':>12s}")
        report.append("-" * 80)

        for _, row in df_folds.iterrows():
            report.append(f"{int(row['fold']):>6d} "
                         f"{row['accuracy']*100:>11.2f}% "
                         f"{row['precision']*100:>11.2f}% "
                         f"{row['recall']*100:>11.2f}% "
                         f"{row['f1_score']*100:>11.2f}% "
                         f"{row['auc_roc']:>12.4f}")
        report.append("-" * 80)
        report.append("")

    # Hold-out Test Results
    if 'holdout' in metrics:
        holdout_df = metrics['holdout']
        report.append("Hold-out Test Set Results:")
        report.append("-" * 80)

        # Performance metrics
        metric_rows = holdout_df[~holdout_df['metric'].str.contains('positive|negative')]
        for _, row in metric_rows.iterrows():
            value = float(row['value'])
            if row['metric'] in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'auc_pr']:
                report.append(f"  {row['metric']:20s}: {value*100:>8.2f}%")
            else:
                report.append(f"  {row['metric']:20s}: {value:>8.4f}")
        report.append("")

        # Confusion Matrix
        report.append("Confusion Matrix:")
        report.append("-" * 80)
        try:
            tp = int(holdout_df[holdout_df['metric'] == 'true_positives']['value'].values[0])
            tn = int(holdout_df[holdout_df['metric'] == 'true_negatives']['value'].values[0])
            fp = int(holdout_df[holdout_df['metric'] == 'false_positives']['value'].values[0])
            fn = int(holdout_df[holdout_df['metric'] == 'false_negatives']['value'].values[0])

            report.append(f"  True Positives (TP)  : {tp:>10,d}")
            report.append(f"  True Negatives (TN)  : {tn:>10,d}")
            report.append(f"  False Positives (FP) : {fp:>10,d}")
            report.append(f"  False Negatives (FN) : {fn:>10,d}")
            report.append("")

            total = tp + tn + fp + fn
            report.append(f"  Total Predictions    : {total:>10,d}")
            report.append("")
        except:
            report.append("  [Could not extract confusion matrix values]")
            report.append("")

    # Generalization Analysis
    if 'cv_summary' in metrics and 'holdout' in metrics:
        cv_df = metrics['cv_summary']
        holdout_df = metrics['holdout']

        report.append("Generalization Analysis (CV vs Hold-out):")
        report.append("-" * 80)
        report.append(f"{'Metric':<20s} {'CV Mean':>12s} {'Hold-out':>12s} {'Gap':>12s}")
        report.append("-" * 80)

        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']:
            cv_row = cv_df[cv_df['metric'] == metric]
            ho_row = holdout_df[holdout_df['metric'] == metric]

            if not cv_row.empty and not ho_row.empty:
                cv_val = cv_row['mean'].values[0] * 100
                ho_val = ho_row['value'].values[0] * 100
                gap = cv_val - ho_val

                gap_status = "" if abs(gap) < 2 else "�" if abs(gap) < 5 else ""
                report.append(f"  {metric:<18s} {cv_val:>11.2f}% {ho_val:>11.2f}% "
                             f"{gap:>+10.2f}% {gap_status}")

        report.append("")
        report.append("  Gap Legend:")
        report.append("     Excellent generalization (gap < 2%)")
        report.append("    � Good generalization (2% d gap < 5%)")
        report.append("     Potential overfitting (gap e 5%)")
        report.append("")

    # Stability Assessment
    if 'cv_folds' in metrics:
        df_folds = metrics['cv_folds']
        report.append("Model Stability Assessment:")
        report.append("-" * 80)

        for metric in ['accuracy', 'f1_score', 'auc_roc']:
            std = df_folds[metric].std() * 100
            report.append(f"  {metric.replace('_', ' ').title():20s} StdDev: {std:.2f}%")

        max_std = df_folds[['accuracy', 'f1_score', 'auc_roc']].std().max() * 100

        report.append("")
        if max_std < 1:
            report.append("   Excellent stability - Very consistent across folds")
        elif max_std < 2:
            report.append("   Good stability - Reliable performance")
        elif max_std < 5:
            report.append("  � Moderate stability - Some variance across folds")
        else:
            report.append("   Poor stability - High variance, consider more data")
        report.append("")

    report.append("=" * 80)

    # Write to file
    output_file = output_dir / "ml_pipeline_report.txt"
    with open(output_file, 'w') as f:
        f.write('\n'.join(report))

    print(f" Saved: {output_file}")

    # Also print to console
    print("\n" + '\n'.join(report))

def main():
    if len(sys.argv) < 2:
        print("Usage: python visualize_ml_pipeline.py <metrics_directory>")
        print("Example: python visualize_ml_pipeline.py /output/exp1_rf_pca/metrics")
        sys.exit(1)

    metrics_dir = Path(sys.argv[1])

    if not metrics_dir.exists():
        print(f" Error: Directory not found: {metrics_dir}")
        sys.exit(1)

    print("=" * 80)
    print("ML Pipeline - Comprehensive Metrics Visualization")
    print("=" * 80)
    print(f"\nMetrics directory: {metrics_dir}\n")

    # Load metrics
    print("Loading metrics...")
    metrics = load_ml_pipeline_metrics(metrics_dir)

    if not metrics:
        print(" No metrics files found!")
        print("\nExpected files:")
        print("  - cv_fold_metrics.csv")
        print("  - cv_summary.csv")
        print("  - holdout_test_metrics.csv")
        print("  - best_hyperparameters.csv (optional)")
        sys.exit(1)

    print(f"\n Loaded {len(metrics)} metric files\n")

    # Create output directory for plots
    output_dir = metrics_dir / "plots-ml-pipeline"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Saving plots to: {output_dir}\n")

    # Generate visualizations
    print("Generating visualizations...")
    plot_cv_folds_detailed(metrics, output_dir)
    plot_cv_vs_holdout_comparison(metrics, output_dir)
    plot_box_plot_stability(metrics, output_dir)
    plot_confusion_matrix_holdout(metrics, output_dir)
    plot_metrics_radar(metrics, output_dir)
    plot_hyperparameters_summary(metrics, output_dir)

    # Generate comprehensive report
    print("\nGenerating comprehensive report...")
    generate_comprehensive_report(metrics, output_dir)

    print("\n" + "=" * 80)
    print(" Visualization complete!")
    print(f" All results saved in: {output_dir}")
    print("=" * 80)
    print("\nGenerated files:")
    print("  - cv_folds_detailed.png          : Metrics across CV folds")
    print("  - cv_vs_holdout_comparison.png   : CV vs Hold-out comparison")
    print("  - cv_stability_boxplot.png       : Stability analysis (box plots)")
    print("  - confusion_matrix_holdout.png   : Confusion matrix (counts & normalized)")
    print("  - metrics_radar.png              : Performance radar chart")
    print("  - best_hyperparameters.png       : Best hyperparameters (if available)")
    print("  - ml_pipeline_report.txt         : Comprehensive text report")
    print()

if __name__ == "__main__":
    main()
