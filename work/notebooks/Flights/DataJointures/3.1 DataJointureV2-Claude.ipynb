{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e159c5ae-37c7-43b7-a800-6349093ad707",
   "metadata": {},
   "source": [
    "# Data Jointure V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e54813-1f4c-4ba8-ae6f-bd3f4571fb6a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b9013a-88ff-4c24-8b38-c69e9114ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar\n",
      "Finished download of Emiasd-Flight-Data-Analysis.jar\n",
      "Using cached version of Emiasd-Flight-Data-Analysis.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/jovyan/work/apps/Emiasd-Flight-Data-Analysis.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee6509b-16ae-49fc-b829-db9aed8c8820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "args = Array(jupyter)\n",
       "spark = org.apache.spark.sql.SparkSession@9ae9e27\n",
       "session = org.apache.spark.sql.SparkSession@9ae9e27\n",
       "configuration = AppConfiguration(local,CommonConfig(42,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output/d...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "AppConfiguration(local,CommonConfig(42,DataConfig(/home/jovyan/work/data,FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Flights/201201*.csv),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/Weather/20101*.txt),FileConfig(/home/jovyan/work/data/FLIGHT-3Y/wban_airport_timezone.csv)),OutputConfig(/home/jovyan/work/output,FileConfig(/home/jovyan/work/output/d..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import com.flightdelay.config.{AppConfiguration, ConfigurationLoader, ExperimentConfig}\n",
    "import com.flightdelay.data.loaders.FlightDataLoader\n",
    "\n",
    "//Env Configuration\n",
    "val args: Array[String] = Array(\"jupyter\")\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(sc.getConf)\n",
    "  .getOrCreate()\n",
    "\n",
    "// Rendre la session Spark implicite\n",
    "implicit val session = spark\n",
    "implicit val configuration: AppConfiguration = ConfigurationLoader.loadConfiguration(args)\n",
    "implicit val experiment: ExperimentConfig = configuration.experiments(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dd5ff-6a58-4ec9-bd15-b0d629e468c6",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96dc104-1631-4d57-8b1a-6b6aeb896805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Flight DF Count: ,461369)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flightDFPath = /home/jovyan/work/output/common/data/processed_flights.parquet\n",
       "flightDF = [FL_DATE: date, OP_CARRIER_AIRLINE_ID: int ... 135 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[FL_DATE: date, OP_CARRIER_AIRLINE_ID: int ... 135 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDFPath = s\"${configuration.common.output.basePath}/common/data/processed_flights.parquet\"\n",
    "val flightDF = spark.read.parquet(flightDFPath)\n",
    "\n",
    "println(\"Flight DF Count: \", flightDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5c18ea-ea85-47b8-8afe-d07f131ff152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER_AIRLINE_ID: integer (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_ID: integer (nullable = true)\n",
      " |-- DEST_AIRPORT_ID: integer (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- ARR_DELAY_NEW: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- D4: integer (nullable = true)\n",
      " |-- D3: integer (nullable = true)\n",
      " |-- D1: integer (nullable = true)\n",
      " |-- D2_15: integer (nullable = true)\n",
      " |-- D2_30: integer (nullable = true)\n",
      " |-- D2_45: integer (nullable = true)\n",
      " |-- D2_60: integer (nullable = true)\n",
      " |-- D2_90: integer (nullable = true)\n",
      " |-- ORIGIN_WBAN: string (nullable = true)\n",
      " |-- ORIGIN_TIMEZONE: integer (nullable = true)\n",
      " |-- DEST_WBAN: string (nullable = true)\n",
      " |-- DEST_TIMEZONE: integer (nullable = true)\n",
      " |-- UTC_CRS_DEP_TIME: string (nullable = true)\n",
      " |-- UTC_ARR_TIME: string (nullable = true)\n",
      " |-- UTC_ARR_DATE: date (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_DATE: date (nullable = true)\n",
      " |-- feature_arrival_hour: integer (nullable = true)\n",
      " |-- feature_utc_arrival_hour: integer (nullable = true)\n",
      " |-- feature_utc_arrival_hour_rounded: string (nullable = true)\n",
      " |-- feature_utc_arrival_date: string (nullable = true)\n",
      " |-- feature_arrival_time_period: string (nullable = true)\n",
      " |-- feature_crosses_midnight_local: integer (nullable = true)\n",
      " |-- feature_crosses_midnight_utc: integer (nullable = true)\n",
      " |-- feature_flight_days_span: integer (nullable = true)\n",
      " |-- feature_timezone_difference: integer (nullable = true)\n",
      " |-- feature_flies_eastward: integer (nullable = true)\n",
      " |-- feature_flies_westward: integer (nullable = true)\n",
      " |-- feature_departure_minute: integer (nullable = true)\n",
      " |-- feature_arrival_minutes_total: double (nullable = true)\n",
      " |-- feature_crosses_midnight: integer (nullable = true)\n",
      " |-- feature_flight_day_of_week: integer (nullable = true)\n",
      " |-- feature_departure_hour_decimal: double (nullable = true)\n",
      " |-- feature_flight_timestamp: timestamp (nullable = true)\n",
      " |-- feature_utc_departure_hour: integer (nullable = true)\n",
      " |-- feature_flight_quarter_name: string (nullable = true)\n",
      " |-- feature_utc_departure_hour_decimal: double (nullable = true)\n",
      " |-- feature_departure_hour: integer (nullable = true)\n",
      " |-- feature_minutes_since_midnight: double (nullable = true)\n",
      " |-- feature_departure_quarter_day: integer (nullable = true)\n",
      " |-- feature_flight_month: integer (nullable = true)\n",
      " |-- feature_utc_departure_hour_rounded: long (nullable = true)\n",
      " |-- feature_arrival_minute: integer (nullable = true)\n",
      " |-- feature_flight_year: integer (nullable = true)\n",
      " |-- feature_flight_day_of_year: integer (nullable = true)\n",
      " |-- feature_arrival_time: integer (nullable = true)\n",
      " |-- feature_flight_quarter: integer (nullable = true)\n",
      " |-- feature_arrival_hour_decimal: double (nullable = true)\n",
      " |-- feature_flight_week_of_year: integer (nullable = true)\n",
      " |-- feature_flight_day_of_month: integer (nullable = true)\n",
      " |-- feature_departure_quarter_name: string (nullable = true)\n",
      " |-- feature_departure_time_period: string (nullable = true)\n",
      " |-- feature_departure_hour_rounded: long (nullable = true)\n",
      " |-- feature_departure_minutes_total: double (nullable = true)\n",
      " |-- feature_arrival_hour_rounded: string (nullable = true)\n",
      " |-- feature_arrival_date: string (nullable = true)\n",
      " |-- feature_distance_score: double (nullable = true)\n",
      " |-- feature_distance_category: string (nullable = true)\n",
      " |-- feature_carrier_hash: integer (nullable = true)\n",
      " |-- feature_route_id: string (nullable = true)\n",
      " |-- feature_is_likely_domestic: integer (nullable = true)\n",
      " |-- feature_is_roundtrip_candidate: integer (nullable = true)\n",
      " |-- feature_flight_unique_id: string (nullable = true)\n",
      " |-- feature_is_summer: integer (nullable = true)\n",
      " |-- feature_is_monday: integer (nullable = true)\n",
      " |-- feature_is_evening_rush: integer (nullable = true)\n",
      " |-- feature_is_month_start: integer (nullable = true)\n",
      " |-- feature_is_extended_weekend: integer (nullable = true)\n",
      " |-- feature_is_winter: integer (nullable = true)\n",
      " |-- feature_is_friday: integer (nullable = true)\n",
      " |-- feature_is_spring: integer (nullable = true)\n",
      " |-- feature_is_weekend: integer (nullable = true)\n",
      " |-- feature_is_holiday_season: integer (nullable = true)\n",
      " |-- feature_is_fall: integer (nullable = true)\n",
      " |-- feature_is_early_morning: integer (nullable = true)\n",
      " |-- feature_is_morning_rush: integer (nullable = true)\n",
      " |-- feature_is_business_hours: integer (nullable = true)\n",
      " |-- feature_is_night_flight: integer (nullable = true)\n",
      " |-- feature_is_month_end: integer (nullable = true)\n",
      " |-- feature_origin_complexity_score: double (nullable = true)\n",
      " |-- feature_is_westbound: integer (nullable = true)\n",
      " |-- feature_dest_complexity_score: double (nullable = true)\n",
      " |-- feature_dest_is_major_hub: integer (nullable = true)\n",
      " |-- feature_is_eastbound: integer (nullable = true)\n",
      " |-- feature_timezone_diff_proxy: integer (nullable = true)\n",
      " |-- feature_origin_is_major_hub: integer (nullable = true)\n",
      " |-- feature_is_hub_to_hub: integer (nullable = true)\n",
      " |-- feature_flights_on_route: long (nullable = true)\n",
      " |-- feature_carrier_flight_count: long (nullable = true)\n",
      " |-- feature_origin_airport_traffic: long (nullable = true)\n",
      " |-- feature_route_popularity_score: string (nullable = true)\n",
      " |-- feature_carrier_size_category: string (nullable = true)\n",
      " |-- AIRCRAFT_ID: string (nullable = true)\n",
      " |-- PREV_AIRCRAFT_ARR_DELAY: double (nullable = true)\n",
      " |-- feature_turnaround_buffer: double (nullable = true)\n",
      " |-- IS_PREV_AIRCRAFT_LATE: integer (nullable = true)\n",
      " |-- feature_is_tight_turnaround: integer (nullable = true)\n",
      " |-- feature_is_very_tight_turnaround: integer (nullable = true)\n",
      " |-- feature_turnaround_category: string (nullable = true)\n",
      " |-- label_arr_delay_filled: double (nullable = true)\n",
      " |-- label_nas_delay_filled: double (nullable = true)\n",
      " |-- label_weather_delay_filled: double (nullable = true)\n",
      " |-- label_weather_delay_was_missing: integer (nullable = true)\n",
      " |-- label_nas_delay_was_missing: integer (nullable = true)\n",
      " |-- label_arr_delay_was_missing: integer (nullable = true)\n",
      " |-- label_is_early: integer (nullable = true)\n",
      " |-- label_total_weather_nas_delay: double (nullable = true)\n",
      " |-- label_is_delayed_15min: integer (nullable = true)\n",
      " |-- label_has_nas_delay: integer (nullable = true)\n",
      " |-- label_is_delayed_30min: integer (nullable = true)\n",
      " |-- label_is_delayed_60min: integer (nullable = true)\n",
      " |-- label_is_delayed_45min: integer (nullable = true)\n",
      " |-- label_is_delayed_90min: integer (nullable = true)\n",
      " |-- label_has_weather_delay: integer (nullable = true)\n",
      " |-- label_is_on_time: integer (nullable = true)\n",
      " |-- label_has_any_weather_nas_delay: integer (nullable = true)\n",
      " |-- feature_avg_delay: double (nullable = true)\n",
      " |-- feature_num_previous_flights: long (nullable = true)\n",
      " |-- feature_stddev_delay: double (nullable = true)\n",
      " |-- feature_max_delay: double (nullable = true)\n",
      " |-- feature_min_delay: double (nullable = true)\n",
      " |-- feature_proportion_delayed_30min: double (nullable = true)\n",
      " |-- feature_proportion_delayed_60min: double (nullable = true)\n",
      " |-- feature_proportion_delayed_15min: double (nullable = true)\n",
      " |-- feature_proportion_delayed_45min: double (nullable = true)\n",
      " |-- feature_proportion_delayed_90min: double (nullable = true)\n",
      " |-- UTC_FL_DATE: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccf286a-46b7-4aaf-881d-048ad866fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Weather DF Count: ,1414384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weatherDFPath = /home/jovyan/work/output/common/data/processed_weather.parquet\n",
       "weatherDF = [WBAN: string, Date: date ... 92 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[WBAN: string, Date: date ... 92 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weatherDFPath = s\"${configuration.common.output.basePath}/common/data/processed_weather.parquet\"\n",
    "val weatherDF = spark.read.parquet(weatherDFPath)\n",
    "\n",
    "println(\"Weather DF Count: \", weatherDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117a9b2a-12dd-421d-bd53-266bf364b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- WBAN: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StationType: integer (nullable = true)\n",
      " |-- SkyCondition: string (nullable = true)\n",
      " |-- SkyConditionFlag: string (nullable = true)\n",
      " |-- Visibility: double (nullable = true)\n",
      " |-- VisibilityFlag: string (nullable = true)\n",
      " |-- WeatherType: string (nullable = true)\n",
      " |-- WeatherTypeFlag: string (nullable = true)\n",
      " |-- DryBulbFarenheit: double (nullable = true)\n",
      " |-- DryBulbFarenheitFlag: string (nullable = true)\n",
      " |-- DryBulbCelsius: double (nullable = true)\n",
      " |-- DryBulbCelsiusFlag: string (nullable = true)\n",
      " |-- WetBulbFarenheit: double (nullable = true)\n",
      " |-- WetBulbFarenheitFlag: string (nullable = true)\n",
      " |-- WetBulbCelsius: double (nullable = true)\n",
      " |-- WetBulbCelsiusFlag: string (nullable = true)\n",
      " |-- DewPointFarenheit: double (nullable = true)\n",
      " |-- DewPointFarenheitFlag: string (nullable = true)\n",
      " |-- DewPointCelsius: double (nullable = true)\n",
      " |-- DewPointCelsiusFlag: string (nullable = true)\n",
      " |-- RelativeHumidity: double (nullable = true)\n",
      " |-- RelativeHumidityFlag: string (nullable = true)\n",
      " |-- WindSpeed: double (nullable = true)\n",
      " |-- WindSpeedFlag: string (nullable = true)\n",
      " |-- WindDirection: double (nullable = true)\n",
      " |-- WindDirectionFlag: string (nullable = true)\n",
      " |-- ValueForWindCharacter: integer (nullable = true)\n",
      " |-- ValueForWindCharacterFlag: string (nullable = true)\n",
      " |-- StationPressure: double (nullable = true)\n",
      " |-- StationPressureFlag: string (nullable = true)\n",
      " |-- PressureTendency: integer (nullable = true)\n",
      " |-- PressureTendencyFlag: string (nullable = true)\n",
      " |-- PressureChange: double (nullable = true)\n",
      " |-- PressureChangeFlag: string (nullable = true)\n",
      " |-- SeaLevelPressure: double (nullable = true)\n",
      " |-- SeaLevelPressureFlag: string (nullable = true)\n",
      " |-- RecordType: string (nullable = true)\n",
      " |-- RecordTypeFlag: string (nullable = true)\n",
      " |-- HourlyPrecip: double (nullable = true)\n",
      " |-- HourlyPrecipFlag: string (nullable = true)\n",
      " |-- Altimeter: double (nullable = true)\n",
      " |-- AltimeterFlag: string (nullable = true)\n",
      " |-- feature_most_critical_sky: string (nullable = true)\n",
      " |-- feature_num_cloud_layers: integer (nullable = true)\n",
      " |-- feature_cloud_risk_score: double (nullable = true)\n",
      " |-- feature_has_overcast: integer (nullable = true)\n",
      " |-- feature_has_broken: integer (nullable = true)\n",
      " |-- feature_has_obscured: integer (nullable = true)\n",
      " |-- feature_is_clear: integer (nullable = true)\n",
      " |-- feature_lowest_cloud_height: integer (nullable = true)\n",
      " |-- feature_ceiling: integer (nullable = true)\n",
      " |-- feature_has_low_ceiling: integer (nullable = true)\n",
      " |-- feature_visibility_miles: double (nullable = true)\n",
      " |-- feature_visibility_km: double (nullable = true)\n",
      " |-- feature_visibility_category: string (nullable = true)\n",
      " |-- feature_visibility_risk_score: double (nullable = true)\n",
      " |-- feature_is_low_visibility: integer (nullable = true)\n",
      " |-- feature_is_very_low_visibility: integer (nullable = true)\n",
      " |-- feature_visibility_normalized: double (nullable = true)\n",
      " |-- feature_visibility_inverse: double (nullable = true)\n",
      " |-- feature_weather_severity_index: double (nullable = true)\n",
      " |-- feature_is_vfr_conditions: integer (nullable = true)\n",
      " |-- feature_is_ifr_conditions: integer (nullable = true)\n",
      " |-- feature_requires_cat_ii: integer (nullable = true)\n",
      " |-- feature_operations_risk_level: integer (nullable = true)\n",
      " |-- feature_flight_category: string (nullable = true)\n",
      " |-- feature_flight_category_ordinal: integer (nullable = true)\n",
      " |-- intensity_heavy: integer (nullable = true)\n",
      " |-- intensity_light: integer (nullable = true)\n",
      " |-- weather_intensity: string (nullable = true)\n",
      " |-- feature_precipitation_intensity: integer (nullable = true)\n",
      " |-- has_thunderstorm: integer (nullable = true)\n",
      " |-- has_freezing_precip: integer (nullable = true)\n",
      " |-- has_freezing: integer (nullable = true)\n",
      " |-- has_precipitation: integer (nullable = true)\n",
      " |-- has_obscuration: integer (nullable = true)\n",
      " |-- has_visible_moisture: integer (nullable = true)\n",
      " |-- has_hazardous: integer (nullable = true)\n",
      " |-- has_rain: integer (nullable = true)\n",
      " |-- has_snow: integer (nullable = true)\n",
      " |-- has_hail: integer (nullable = true)\n",
      " |-- extracted_codes: string (nullable = true)\n",
      " |-- weather_hazard_level: integer (nullable = true)\n",
      " |-- Icing_Risk_Flag: integer (nullable = true)\n",
      " |-- Icing_Risk_Level: integer (nullable = true)\n",
      " |-- press_trend_sign: integer (nullable = true)\n",
      " |-- press_change_raw: double (nullable = true)\n",
      " |-- press_change_abs: double (nullable = true)\n",
      " |-- feature_pressure_variation_risk: integer (nullable = true)\n",
      " |-- feature_pressure_bucket: string (nullable = true)\n",
      " |-- feature_pressure_visibility_combo: double (nullable = true)\n",
      " |-- feature_pressure_vis_combo_bin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bc400-6556-45e1-82d3-10fe8b4dd8a2",
   "metadata": {},
   "source": [
    "## Claude - RDD Implemntation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba17c46f-a501-47f1-ae24-deadb2bb205d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "ename": "Unknown Error",
     "evalue": "<console>:453: error: value repartitionAndSortWithinPartitions is not a member of org.apache.spark.rdd.RDD[((String, String, String), T)]\n         mappedRDD.repartitionAndSortWithinPartitions(partitioner)\n                   ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession, Row}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{Partitioner}\n",
    "import java.sql.Date\n",
    "import scala.collection.mutable\n",
    "\n",
    "// ============================================\n",
    "// PARTITIONNEUR (inchangé)\n",
    "// ============================================\n",
    "\n",
    "class AirportDatePartitioner(val numPartitions: Int) extends Partitioner with Serializable {\n",
    "  \n",
    "  override def getPartition(key: Any): Int = key match {\n",
    "    case (wban: String, date: String, tag: String) =>\n",
    "      val hash = wban.hashCode + date.hashCode\n",
    "      Math.abs(hash) % numPartitions\n",
    "    case _ =>\n",
    "      throw new IllegalArgumentException(s\"Unexpected key type: ${key.getClass}\")\n",
    "  }\n",
    "  \n",
    "  override def equals(other: Any): Boolean = other match {\n",
    "    case p: AirportDatePartitioner => p.numPartitions == numPartitions\n",
    "    case _ => false\n",
    "  }\n",
    "  \n",
    "  override def hashCode: Int = numPartitions\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// UTILS (inchangé)\n",
    "// ============================================\n",
    "\n",
    "object JoinUtils extends Serializable {\n",
    "  \n",
    "  def needsPreviousDayData(timeStr: String): Boolean = {\n",
    "    try {\n",
    "      if (timeStr == null) return false\n",
    "      \n",
    "      val hour = if (timeStr.contains(\":\")) {\n",
    "        timeStr.split(\":\")(0).toInt\n",
    "      } else if (timeStr.length >= 2) {\n",
    "        timeStr.take(2).toInt\n",
    "      } else {\n",
    "        return false\n",
    "      }\n",
    "      hour < 12\n",
    "    } catch {\n",
    "      case _: Exception => false\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  def addDays(date: Date, days: Int): Date = {\n",
    "    val cal = java.util.Calendar.getInstance()\n",
    "    cal.setTime(date)\n",
    "    cal.add(java.util.Calendar.DAY_OF_MONTH, days)\n",
    "    new Date(cal.getTimeInMillis)\n",
    "  }\n",
    "  \n",
    "  def parseWeatherTimestamp(weatherRow: Row): Long = {\n",
    "    try {\n",
    "      val date = weatherRow.getAs[Date](\"Date\")\n",
    "      val time = weatherRow.getAs[String](\"Time\")\n",
    "      \n",
    "      if (date == null || time == null) return 0L\n",
    "      \n",
    "      val hourMinute = if (time.contains(\":\")) {\n",
    "        time.split(\":\")\n",
    "      } else if (time.length == 4) {\n",
    "        Array(time.take(2), time.drop(2))\n",
    "      } else {\n",
    "        return 0L\n",
    "      }\n",
    "      \n",
    "      val cal = java.util.Calendar.getInstance()\n",
    "      cal.setTime(date)\n",
    "      cal.set(java.util.Calendar.HOUR_OF_DAY, hourMinute(0).toInt)\n",
    "      cal.set(java.util.Calendar.MINUTE, hourMinute(1).toInt)\n",
    "      cal.set(java.util.Calendar.SECOND, 0)\n",
    "      cal.set(java.util.Calendar.MILLISECOND, 0)\n",
    "      \n",
    "      cal.getTimeInMillis\n",
    "    } catch {\n",
    "      case _: Exception => 0L\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  def parseFlightTimestamp(date: Date, time: String): Long = {\n",
    "    try {\n",
    "      if (date == null || time == null) return 0L\n",
    "      \n",
    "      val hourMinute = if (time.contains(\":\")) {\n",
    "        time.split(\":\")\n",
    "      } else if (time.length == 4) {\n",
    "        Array(time.take(2), time.drop(2))\n",
    "      } else {\n",
    "        return 0L\n",
    "      }\n",
    "      \n",
    "      val cal = java.util.Calendar.getInstance()\n",
    "      cal.setTime(date)\n",
    "      cal.set(java.util.Calendar.HOUR_OF_DAY, hourMinute(0).toInt)\n",
    "      cal.set(java.util.Calendar.MINUTE, hourMinute(1).toInt)\n",
    "      cal.set(java.util.Calendar.SECOND, 0)\n",
    "      cal.set(java.util.Calendar.MILLISECOND, 0)\n",
    "      \n",
    "      cal.getTimeInMillis\n",
    "    } catch {\n",
    "      case _: Exception => 0L\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  def extract12HourlyObservations(\n",
    "    weatherBuffer: mutable.TreeMap[Long, Row],\n",
    "    scheduledTimestamp: Long\n",
    "  ): Array[Row] = {\n",
    "    \n",
    "    val observations = new Array[Row](12)\n",
    "    \n",
    "    for (i <- 0 until 12) {\n",
    "      val targetTimestamp = scheduledTimestamp - (i * 3600000L)\n",
    "      val closestEntry = weatherBuffer.to(targetTimestamp).lastOption\n",
    "      \n",
    "      closestEntry match {\n",
    "        case Some((_, weatherRow)) => observations(i) = weatherRow\n",
    "        case None => observations(i) = null\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    observations\n",
    "  }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// CRÉATION DE SCHÉMAS POUR LES ROWS JOINTES\n",
    "// ============================================\n",
    "\n",
    "object SchemaUtils {\n",
    "  \n",
    "  def createOriginJoinedSchema(flightSchema: StructType): StructType = {\n",
    "    // Ajouter un champ pour les observations météo d'origine\n",
    "    flightSchema.add(\"origin_weather_observations\", \n",
    "      ArrayType(StructType(Seq(\n",
    "        StructField(\"weather_data\", StringType, nullable = true)\n",
    "      )))\n",
    "    )\n",
    "  }\n",
    "  \n",
    "  def createFinalJoinedSchema(originJoinedSchema: StructType): StructType = {\n",
    "    // Ajouter un champ pour les observations météo de destination\n",
    "    originJoinedSchema.add(\"dest_weather_observations\",\n",
    "      ArrayType(StructType(Seq(\n",
    "        StructField(\"weather_data\", StringType, nullable = true)\n",
    "      )))\n",
    "    )\n",
    "  }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// PHASE MAP - Origin (avec identifiant unique)\n",
    "// ============================================\n",
    "\n",
    "def mapPhaseOrigin(flightDF: DataFrame, weatherDF: DataFrame): RDD[((String, String, String), (Row, String))] = {\n",
    "  \n",
    "  val weatherMapped = weatherDF.rdd.map { row =>\n",
    "    val wban = row.getAs[String](\"WBAN\")\n",
    "    val date = row.getAs[Date](\"Date\").toString\n",
    "    ((wban, date, \"OT\"), (row, \"weather\"))\n",
    "  }\n",
    "  \n",
    "  // IMPORTANT: Ajouter un identifiant unique à chaque vol\n",
    "  val flightMapped = flightDF.rdd.flatMap { row =>\n",
    "    val originWban = row.getAs[String](\"ORIGIN_WBAN\")\n",
    "    val flightDate = row.getAs[Date](\"UTC_FL_DATE\")\n",
    "    val depTime = row.getAs[String](\"UTC_CRS_DEP_TIME\")\n",
    "    \n",
    "    // Créer un ID unique basé sur les colonnes du vol\n",
    "    val flightId = s\"${row.getAs[String](\"feature_flight_unique_id\")}\"\n",
    "    \n",
    "    val results = mutable.ArrayBuffer[((String, String, String), (Row, String))]()\n",
    "    \n",
    "    results += (((originWban, flightDate.toString, \"FT\"), (row, flightId)))\n",
    "    \n",
    "    if (JoinUtils.needsPreviousDayData(depTime)) {\n",
    "      val prevDate = JoinUtils.addDays(flightDate, -1)\n",
    "      results += (((originWban, prevDate.toString, \"FT\"), (row, flightId)))\n",
    "    }\n",
    "    \n",
    "    results\n",
    "  }\n",
    "  \n",
    "  weatherMapped.union(flightMapped)\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// PHASE REDUCE - Origin (avec déduplication)\n",
    "// ============================================\n",
    "\n",
    "def reducePhaseOrigin(sortedRDD: RDD[((String, String, String), (Row, String))]): RDD[(Row, StructType)] = {\n",
    "  \n",
    "  sortedRDD.mapPartitions { partition =>\n",
    "    \n",
    "    val results = mutable.ArrayBuffer[(Row, StructType)]()\n",
    "    \n",
    "    val grouped = partition.toSeq.groupBy { case ((wban, date, _), _) =>\n",
    "      (wban, date)\n",
    "    }\n",
    "    \n",
    "    grouped.foreach { case ((wban, date), records) =>\n",
    "      \n",
    "      val weatherBuffer = mutable.TreeMap[Long, Row]()\n",
    "      val flightMap = mutable.Map[String, Row]()  // Map pour déduplication\n",
    "      \n",
    "      records.foreach { case ((_, _, tag), (row, idOrType)) =>\n",
    "        if (tag == \"OT\") {\n",
    "          val timestamp = JoinUtils.parseWeatherTimestamp(row)\n",
    "          if (timestamp > 0) {\n",
    "            weatherBuffer.put(timestamp, row)\n",
    "          }\n",
    "        } else {\n",
    "          // Stocker le vol avec son ID unique\n",
    "          flightMap.put(idOrType, row)\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      // Traiter chaque vol unique\n",
    "      flightMap.values.foreach { flightRow =>\n",
    "        val flightDate = flightRow.getAs[Date](\"UTC_FL_DATE\")\n",
    "        val depTime = flightRow.getAs[String](\"UTC_CRS_DEP_TIME\")\n",
    "        val depTimestamp = JoinUtils.parseFlightTimestamp(flightDate, depTime)\n",
    "        \n",
    "        val weatherObservations = JoinUtils.extract12HourlyObservations(\n",
    "          weatherBuffer, \n",
    "          depTimestamp\n",
    "        )\n",
    "        \n",
    "        // Créer une Row avec schéma\n",
    "        val flightSeq = flightRow.toSeq\n",
    "        val weatherSeq = weatherObservations.toSeq\n",
    "        \n",
    "        // Conserver le schéma original\n",
    "        val originalSchema = flightRow.schema\n",
    "        \n",
    "        // Créer la nouvelle Row avec les données\n",
    "        val joinedSeq = flightSeq :+ weatherSeq\n",
    "        val joinedRow = Row.fromSeq(joinedSeq)\n",
    "        \n",
    "        results += ((joinedRow, originalSchema))\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    results.iterator\n",
    "  }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// PHASE MAP - Destination (avec accès par index)\n",
    "// ============================================\n",
    "\n",
    "def mapPhaseDestination(\n",
    "  firstJoinResultRDD: RDD[(Row, StructType)], \n",
    "  weatherDF: DataFrame\n",
    "): RDD[((String, String, String), (Row, StructType, String))] = {\n",
    "  \n",
    "  val weatherMapped = weatherDF.rdd.map { row =>\n",
    "    val wban = row.getAs[String](\"WBAN\")\n",
    "    val date = row.getAs[Date](\"Date\").toString\n",
    "    ((wban, date, \"OT\"), (row, weatherDF.schema, \"weather\"))\n",
    "  }\n",
    "  \n",
    "  val joinedMapped = firstJoinResultRDD.flatMap { case (row, schema) =>\n",
    "    // Utiliser l'index pour accéder aux champs\n",
    "    val destWbanIdx = schema.fieldIndex(\"DEST_WBAN\")\n",
    "    val arrDateIdx = schema.fieldIndex(\"UTC_ARR_DATE\")\n",
    "    val arrTimeIdx = schema.fieldIndex(\"UTC_ARR_TIME\")\n",
    "    val flightIdIdx = schema.fieldIndex(\"feature_flight_unique_id\")\n",
    "    \n",
    "    val destWban = row.getString(destWbanIdx)\n",
    "    val arrDate = row.getDate(arrDateIdx)\n",
    "    val arrTime = row.getString(arrTimeIdx)\n",
    "    val flightId = row.getString(flightIdIdx)\n",
    "    \n",
    "    val results = mutable.ArrayBuffer[((String, String, String), (Row, StructType, String))]()\n",
    "    \n",
    "    results += (((destWban, arrDate.toString, \"FT\"), (row, schema, flightId)))\n",
    "    \n",
    "    if (JoinUtils.needsPreviousDayData(arrTime)) {\n",
    "      val prevDate = JoinUtils.addDays(arrDate, -1)\n",
    "      results += (((destWban, prevDate.toString, \"FT\"), (row, schema, flightId)))\n",
    "    }\n",
    "    \n",
    "    results\n",
    "  }\n",
    "  \n",
    "  weatherMapped.union(joinedMapped)\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// PHASE REDUCE - Destination (avec déduplication)\n",
    "// ============================================\n",
    "\n",
    "def reducePhaseDestination(\n",
    "  sortedRDD: RDD[((String, String, String), (Row, StructType, String))]\n",
    "): RDD[(Row, StructType)] = {\n",
    "  \n",
    "  sortedRDD.mapPartitions { partition =>\n",
    "    \n",
    "    val results = mutable.ArrayBuffer[(Row, StructType)]()\n",
    "    \n",
    "    val grouped = partition.toSeq.groupBy { case ((wban, date, _), _) =>\n",
    "      (wban, date)\n",
    "    }\n",
    "    \n",
    "    grouped.foreach { case ((wban, date), records) =>\n",
    "      \n",
    "      val weatherBuffer = mutable.TreeMap[Long, Row]()\n",
    "      val flightMap = mutable.Map[String, (Row, StructType)]()\n",
    "      \n",
    "      records.foreach { case ((_, _, tag), (row, schema, idOrType)) =>\n",
    "        if (tag == \"OT\") {\n",
    "          val timestamp = JoinUtils.parseWeatherTimestamp(row)\n",
    "          if (timestamp > 0) {\n",
    "            weatherBuffer.put(timestamp, row)\n",
    "          }\n",
    "        } else {\n",
    "          flightMap.put(idOrType, (row, schema))\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      flightMap.values.foreach { case (joinedRow, schema) =>\n",
    "        val arrDateIdx = schema.fieldIndex(\"UTC_ARR_DATE\")\n",
    "        val arrTimeIdx = schema.fieldIndex(\"UTC_ARR_TIME\")\n",
    "        \n",
    "        val arrDate = joinedRow.getDate(arrDateIdx)\n",
    "        val arrTime = joinedRow.getString(arrTimeIdx)\n",
    "        val arrTimestamp = JoinUtils.parseFlightTimestamp(arrDate, arrTime)\n",
    "        \n",
    "        val weatherObservations = JoinUtils.extract12HourlyObservations(\n",
    "          weatherBuffer, \n",
    "          arrTimestamp\n",
    "        )\n",
    "        \n",
    "        val currentSeq = joinedRow.toSeq\n",
    "        val destWeatherSeq = weatherObservations.toSeq\n",
    "        \n",
    "        val finalSeq = currentSeq :+ destWeatherSeq\n",
    "        val finalRow = Row.fromSeq(finalSeq)\n",
    "        \n",
    "        results += ((finalRow, schema))\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    results.iterator\n",
    "  }\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// PARTITION ET SORT (inchangé)\n",
    "// ============================================\n",
    "\n",
    "def partitionAndSort[T](\n",
    "  mappedRDD: RDD[((String, String, String), T)],\n",
    "  numPartitions: Int\n",
    "): RDD[((String, String, String), T)] = {\n",
    "  \n",
    "  val partitioner = new AirportDatePartitioner(numPartitions)\n",
    "  \n",
    "  implicit val keyOrdering: Ordering[(String, String, String)] = \n",
    "    new Ordering[(String, String, String)] {\n",
    "      def compare(x: (String, String, String), y: (String, String, String)): Int = {\n",
    "        val cmpWban = x._1.compareTo(y._1)\n",
    "        if (cmpWban != 0) return cmpWban\n",
    "        \n",
    "        val cmpDate = x._2.compareTo(y._2)\n",
    "        if (cmpDate != 0) return cmpDate\n",
    "        \n",
    "        x._3.compareTo(y._3)\n",
    "      }\n",
    "    }\n",
    "  \n",
    "  mappedRDD.repartitionAndSortWithinPartitions(partitioner)\n",
    "}\n",
    "\n",
    "// ============================================\n",
    "// FONCTION PRINCIPALE\n",
    "// ============================================\n",
    "\n",
    "def performImprovedJoin(\n",
    "  flightDF: DataFrame,\n",
    "  weatherDF: DataFrame,\n",
    "  numPartitions: Int = 200\n",
    "): RDD[(Row, StructType)] = {\n",
    "  \n",
    "  println(\"=== PREMIÈRE JOINTURE (ORIGIN AIRPORT) ===\")\n",
    "  println(\"Phase MAP...\")\n",
    "  val mappedOrigin = mapPhaseOrigin(flightDF, weatherDF)\n",
    "  \n",
    "  println(\"Phase PARTITION + SORT...\")\n",
    "  val sortedOrigin = partitionAndSort(mappedOrigin, numPartitions)\n",
    "  \n",
    "  println(\"Phase REDUCE...\")\n",
    "  val firstJoinResult = reducePhaseOrigin(sortedOrigin)\n",
    "  firstJoinResult.cache()\n",
    "  val count1 = firstJoinResult.count()\n",
    "  println(s\"Nombre de vols après première jointure: $count1\")\n",
    "  \n",
    "  println(\"\\n=== DEUXIÈME JOINTURE (DESTINATION AIRPORT) ===\")\n",
    "  println(\"Phase MAP...\")\n",
    "  val mappedDest = mapPhaseDestination(firstJoinResult, weatherDF)\n",
    "  \n",
    "  println(\"Phase PARTITION + SORT...\")\n",
    "  val sortedDest = partitionAndSort(mappedDest, numPartitions)\n",
    "  \n",
    "  println(\"Phase REDUCE...\")\n",
    "  val finalResult = reducePhaseDestination(sortedDest)\n",
    "  val count2 = finalResult.count()\n",
    "  println(s\"Nombre de vols dans résultat final: $count2\")\n",
    "  \n",
    "  finalResult\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d09273-eaeb-44a3-aa32-5a69905d4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREMIÈRE JOINTURE (ORIGIN AIRPORT) ===\n",
      "Phase MAP...\n",
      "Phase PARTITION + SORT...\n",
      "Phase REDUCE...\n",
      "Nombre de vols après première jointure: 574553\n",
      "\n",
      "=== DEUXIÈME JOINTURE (DESTINATION AIRPORT) ===\n",
      "Phase MAP...\n",
      "Phase PARTITION + SORT...\n",
      "Phase REDUCE...\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 183 in stage 11.0 failed 1 times, most recent failure: Lost task 183.0 in stage 11.0 (TID 265) (jupyter-spark executor driver): org.apache.spark.SparkUnsupportedOperationException: fieldIndex on a Row without schema is undefined.\n\tat org.apache.spark.sql.errors.DataTypeErrors$.fieldIndexOnRowWithoutSchemaError(DataTypeErrors.scala:269)\n\tat org.apache.spark.sql.Row.fieldIndex(Row.scala:381)\n\tat org.apache.spark.sql.Row.fieldIndex$(Row.scala:380)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.fieldIndex(rows.scala:27)\n\tat org.apache.spark.sql.Row.getAs(Row.scala:372)\n\tat org.apache.spark.sql.Row.getAs$(Row.scala:372)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getAs(rows.scala:27)\n\tat $line58.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.$anonfun$mapPhaseDestination$2(<console>:237)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 183 in stage 11.0 failed 1 times, most recent failure: Lost task 183.0 in stage 11.0 (TID 265) (jupyter-spark executor driver): org.apache.spark.SparkUnsupportedOperationException: fieldIndex on a Row without schema is undefined.",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.fieldIndexOnRowWithoutSchemaError(DataTypeErrors.scala:269)",
      "\tat org.apache.spark.sql.Row.fieldIndex(Row.scala:381)",
      "\tat org.apache.spark.sql.Row.fieldIndex$(Row.scala:380)",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.fieldIndex(rows.scala:27)",
      "\tat org.apache.spark.sql.Row.getAs(Row.scala:372)",
      "\tat org.apache.spark.sql.Row.getAs$(Row.scala:372)",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getAs(rows.scala:27)",
      "\tat $anonfun$mapPhaseDestination$2(<console>:237)",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)",
      "  at org.apache.spark.rdd.RDD.count(RDD.scala:1296)",
      "  at performImprovedJoin(<console>:410)",
      "  ... 47 elided",
      "Caused by: org.apache.spark.SparkUnsupportedOperationException: fieldIndex on a Row without schema is undefined.",
      "  at org.apache.spark.sql.errors.DataTypeErrors$.fieldIndexOnRowWithoutSchemaError(DataTypeErrors.scala:269)",
      "  at org.apache.spark.sql.Row.fieldIndex(Row.scala:381)",
      "  at org.apache.spark.sql.Row.fieldIndex$(Row.scala:380)",
      "  at org.apache.spark.sql.catalyst.expressions.GenericRow.fieldIndex(rows.scala:27)",
      "  at org.apache.spark.sql.Row.getAs(Row.scala:372)",
      "  at org.apache.spark.sql.Row.getAs$(Row.scala:372)",
      "  at org.apache.spark.sql.catalyst.expressions.GenericRow.getAs(rows.scala:27)",
      "  at $anonfun$mapPhaseDestination$2(<console>:237)",
      "  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)",
      "  ... 3 more"
     ]
    }
   ],
   "source": [
    "// Mesurer le temps\n",
    "val startTime = System.currentTimeMillis()\n",
    "\n",
    "// Effectuer la jointure\n",
    "val result = performImprovedJoin(\n",
    "  flightDF, \n",
    "  weatherDF, \n",
    "  numPartitions = 200\n",
    ")\n",
    "\n",
    "// Forcer l'exécution\n",
    "val count = result.count()\n",
    "\n",
    "val endTime = System.currentTimeMillis()\n",
    "val duration = (endTime - startTime) / 1000.0\n",
    "\n",
    "println(s\"\\n=== RÉSULTATS ===\")\n",
    "println(s\"Nombre total de vols traités: $count\")\n",
    "println(s\"Temps d'exécution: $duration secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcac082-137a-4b92-b722-590d54ede21a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_scala - Scala",
   "language": "scala",
   "name": "apache_toree_scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
