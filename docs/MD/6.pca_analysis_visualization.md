# PCA Analysis and Visualization Guide

## Overview

Le projet utilise l'analyse en composantes principales (PCA) pour la réduction de dimensionnalité des features. Cette documentation explique comment utiliser le `PCAFeatureExtractor`, sauvegarder les résultats d'analyse et générer des visualisations complètes.

## Architecture PCA

### Composants Scala

- **`PCAFeatureExtractor`** : Classe principale pour l'extraction PCA avec sélection automatique basée sur la variance
- **`VarianceAnalysis`** : Case class contenant les métriques d'analyse de variance
- **Méthodes de sauvegarde** : Export CSV pour visualisation Python

### Stratégies de Sélection

Le `PCAFeatureExtractor` supporte deux stratégies :

1. **Variance-based** : Sélection automatique pour atteindre un seuil de variance (ex: 95%)
2. **K-based** : Nombre fixe de composantes

## Utilisation du PCAFeatureExtractor

### Example 1: Sélection Basée sur la Variance

```scala
import com.flightdelay.features.pca.PCAFeatureExtractor

// Créer un extracteur PCA avec seuil de variance à 95%
val pca = new PCAFeatureExtractor(
  inputCol = "features",
  outputCol = "pcaFeatures",
  varianceThreshold = Some(0.95)
)

// Fit et transform sur les données d'entraînement
val (model, trainTransformed, analysis) = pca.fitTransform(trainData)

// Afficher le rapport d'analyse
pca.printVarianceReport(analysis)

// Sauvegarder les données pour visualisation
val metricsPath = "/output/metrics/pca_analysis"
pca.saveVarianceAnalysis(analysis, s"$metricsPath/pca_variance.csv")
pca.savePCAProjections(trainTransformed, s"$metricsPath/pca_projections.csv",
                       labelCol = Some("label"))

// Transformer les données de test
val testTransformed = pca.transform(model, testData)
```

### Example 2: Nombre Fixe de Composantes

```scala
// Créer un extracteur PCA avec K=10 composantes
val pca = new PCAFeatureExtractor(
  inputCol = "features",
  outputCol = "pcaFeatures",
  k = 10,
  varianceThreshold = None
)

val (model, transformed, analysis) = pca.fitTransform(trainData)
```

### Example 3: Builder Pattern

```scala
val pca = PCAFeatureExtractor.builder()
  .withInputCol("scaledFeatures")
  .withOutputCol("pcaFeatures")
  .withVarianceThreshold(0.99)
  .build()
```

### Example 4: Factory Methods

```scala
// Variance-based
val pca1 = PCAFeatureExtractor.varianceBased(threshold = 0.95)

// K-based
val pca2 = PCAFeatureExtractor.fixedK(k = 15)

// Exploration only (no transformation)
val analysis = PCAFeatureExtractor.exploreVariance(data, maxK = 50)
```

## Fichiers Générés

Après l'analyse PCA, les données sont sauvegardées dans :

```
/output/metrics/pca_analysis/
├── pca_variance.csv           # Variance expliquée par chaque composante
├── pca_projections.csv        # Projections 2D (PC1, PC2) avec labels
└── plots/
    ├── scree_plot.png         # Variance individuelle par composante
    ├── cumulative_variance.png # Variance cumulée
    ├── variance_analysis.png   # Vue combinée (scree + cumulée)
    ├── biplot.png             # Projection 2D colorée par classe
    ├── variance_heatmap.png    # Heatmap des top composantes
    └── pca_report.txt         # Rapport texte détaillé
```

## Structure des Fichiers CSV

### 1. `pca_variance.csv`

```csv
component,explained_variance,cumulative_variance,cumulative_variance_pct
1,0.234567,0.234567,23.46
2,0.156789,0.391356,39.14
3,0.098765,0.490121,49.01
...
```

**Colonnes :**
- **component** : Numéro de la composante principale (PC1, PC2, ...)
- **explained_variance** : Variance expliquée par cette composante seule
- **cumulative_variance** : Variance cumulée jusqu'à cette composante
- **cumulative_variance_pct** : Variance cumulée en pourcentage

### 2. `pca_projections.csv`

```csv
pc1,pc2,label
-2.345,1.234,0.0
3.456,-0.987,1.0
-1.234,2.345,0.0
...
```

**Colonnes :**
- **pc1** : Projection sur la première composante principale
- **pc2** : Projection sur la deuxième composante principale
- **label** : Label de classe (0.0 = à l'heure, 1.0 = retardé)

## Installation des Dépendances Python

```bash
pip install pandas matplotlib seaborn numpy
```

Ou avec conda :

```bash
conda install pandas matplotlib seaborn numpy
```

## Utilisation du Script de Visualisation

### Commande de Base

```bash
python work/scripts/visualize_pca.py /output/metrics/pca_analysis
```

### Exemple Complet

```bash
# Depuis la racine du projet
cd /Users/malikchettih/Projects/Emiasd-Projects/Emiasd-FlightProject

# Générer les visualisations PCA
python ./work/scripts/visualize_pca.py ./work/output/metrics/pca_analysis
```

## Visualisations Générées

### 1. **scree_plot.png**
- Graphique en barres montrant la variance expliquée par chaque composante
- Utile pour identifier le "coude" (elbow point)
- Aide à déterminer le nombre optimal de composantes

### 2. **cumulative_variance.png**
- Courbe de variance cumulée
- Lignes de seuil à 80%, 90%, 95%, 99%
- Indique le nombre de composantes nécessaires pour atteindre chaque seuil
- Annotations automatiques des composantes critiques

### 3. **variance_analysis.png**
- Vue combinée : scree plot (gauche) + variance cumulée (droite)
- Limite au top 20 composantes pour le scree plot
- Vue d'ensemble complète de l'analyse de variance

### 4. **biplot.png**
- Projection des données sur les 2 premières composantes (PC1, PC2)
- Couleurs différentes pour chaque classe (retardé vs à l'heure)
- Permet d'évaluer la séparabilité des classes dans l'espace PCA
- Maximum 5000 échantillons pour performance

### 5. **variance_heatmap.png**
- Heatmap verticale des top 20 composantes
- Visualisation rapide de l'importance relative
- Valeurs annotées pour précision

### 6. **pca_report.txt**
- Rapport texte détaillé avec :
  - Nombre total de composantes
  - Variance totale expliquée
  - Composantes nécessaires pour 80%, 90%, 95%, 99% variance
  - Top 10 composantes avec variance individuelle et cumulée
  - Statistiques de projection (range PC1, PC2)
  - Distribution des classes

## Interprétation des Résultats

### Scree Plot (Elbow Method)

Le **scree plot** aide à identifier le point de diminution marginale de l'information :

- **Elbow Point** : Point où la courbe commence à "s'aplatir"
- **Avant le coude** : Composantes importantes avec forte variance
- **Après le coude** : Composantes moins informatives

**Exemple :**
```
Si le coude est à PC10, utiliser 10-15 composantes est optimal
```

### Variance Cumulée

La **variance cumulée** indique le pourcentage d'information conservée :

- **95% variance** : Standard pour la plupart des applications
- **99% variance** : Pour applications nécessitant haute précision
- **80-90% variance** : Pour réduction dimensionnelle agressive

**Règles empiriques :**
- **Machine Learning** : 90-95% est généralement suffisant
- **Visualisation** : 70-80% peut suffire
- **Reconstruction précise** : 95-99% recommandé

### Biplot

Le **biplot** révèle la structure des données dans l'espace PCA :

- **Clusters séparés** : Classes bien séparables → PCA efficace
- **Clusters superposés** : Classes difficiles à séparer → considérer d'autres features
- **Outliers** : Points isolés peuvent indiquer des anomalies

### Réduction de Dimensionnalité

**Scénario 1 : Features originales = 100, Variance cible = 95%**

```
Si 20 composantes atteignent 95% variance :
  - Réduction : 100 → 20 (80% de réduction)
  - Information conservée : 95%
  - Gain : Moins de paramètres, entraînement plus rapide, moins d'overfitting
```

**Scénario 2 : Features originales = 50, Variance cible = 99%**

```
Si 40 composantes atteignent 99% variance :
  - Réduction : 50 → 40 (20% de réduction)
  - Information conservée : 99%
  - Conclusion : PCA peu bénéfique, considérer feature selection
```

## Best Practices

### 1. Toujours Standardiser Avant PCA

```scala
import org.apache.spark.ml.feature.StandardScaler

val scaler = new StandardScaler()
  .setInputCol("rawFeatures")
  .setOutputCol("scaledFeatures")
  .setWithMean(true)
  .setWithStd(true)

val scalerModel = scaler.fit(trainData)
val scaledTrain = scalerModel.transform(trainData)

// Puis appliquer PCA sur features standardisées
val pca = new PCAFeatureExtractor(inputCol = "scaledFeatures")
```

**Pourquoi ?**
- PCA est sensible à l'échelle des features
- Features avec grandes valeurs dominent la variance
- Standardisation assure une contribution équitable

### 2. Fit sur Train, Transform sur Train+Test

```scala
// ✓ CORRECT
val (model, trainPCA, analysis) = pca.fitTransform(trainData)
val testPCA = pca.transform(model, testData)

// ✗ INCORRECT (data leakage)
val (model, allPCA, _) = pca.fitTransform(trainData.union(testData))
```

### 3. Sauvegarder le Modèle PCA

```scala
// Le PCAModel peut être sauvegardé et rechargé
model.write.overwrite().save("/output/models/pca_model")

// Recharger plus tard
import org.apache.spark.ml.feature.PCAModel
val loadedModel = PCAModel.load("/output/models/pca_model")
```

### 4. Analyser Avant de Décider

```scala
// Exploration initiale avec beaucoup de composantes
val analysis = PCAFeatureExtractor.exploreVariance(trainData, maxK = 50)
pca.printVarianceReport(analysis)

// Identifier le seuil optimal visuellement
// Puis réentraîner avec K optimal
val optimalK = 15 // Basé sur l'analyse
val pcaFinal = PCAFeatureExtractor.fixedK(k = optimalK)
```

## Intégration dans le Pipeline

### Pipeline Complet avec PCA

```scala
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{StandardScaler, VectorAssembler}
import com.flightdelay.features.pca.PCAFeatureExtractor

// 1. Assembly des features
val assembler = new VectorAssembler()
  .setInputCols(featureColumns)
  .setOutputCol("rawFeatures")

// 2. Standardisation
val scaler = new StandardScaler()
  .setInputCol("rawFeatures")
  .setOutputCol("scaledFeatures")
  .setWithMean(true)
  .setWithStd(true)

// 3. PCA
val pca = new PCAFeatureExtractor(
  inputCol = "scaledFeatures",
  outputCol = "pcaFeatures",
  varianceThreshold = Some(0.95)
)

// Option 1 : Pipeline manuel avec analyse
val scalerModel = scaler.fit(assembledData)
val scaledData = scalerModel.transform(assembledData)

val (pcaModel, pcaData, analysis) = pca.fitTransform(scaledData)
pca.saveVarianceAnalysis(analysis, metricsPath + "/pca_variance.csv")

// Option 2 : Pipeline Spark ML complet
val pipeline = new Pipeline()
  .setStages(Array(assembler, scaler))

val preparedData = pipeline.fit(trainData).transform(trainData)
val (pcaModel, pcaFeatures, _) = pca.fitTransform(preparedData)
```

## Dépannage

### Erreur : "No PCA data files found"
- Vérifier que les CSV ont été générés
- Vérifier le chemin du répertoire
- S'assurer que `saveVarianceAnalysis()` a été appelé

### Erreur : "Dimension mismatch"
- Vérifier que le même scaler est utilisé pour train et test
- S'assurer que les mêmes features sont présentes
- Vérifier l'ordre des colonnes

### PCA ne réduit pas beaucoup la dimensionnalité
- Features peuvent déjà être décorrélées
- Considérer feature selection au lieu de PCA
- Vérifier si standardisation a été appliquée

### Visualisations vides ou erreurs
- Vérifier que pandas, matplotlib, seaborn sont installés
- S'assurer que les CSV contiennent des données
- Vérifier les permissions d'écriture

## Exemple Complet : Workflow PCA

```scala
import com.flightdelay.features.pca.PCAFeatureExtractor
import org.apache.spark.ml.feature.StandardScaler

// Configuration
val metricsPath = "/output/metrics/pca_analysis"
val varianceTarget = 0.95

println("=== PCA Analysis Workflow ===")

// 1. Standardisation (obligatoire)
println("\n[Step 1] Feature Standardization")
val scaler = new StandardScaler()
  .setInputCol("features")
  .setOutputCol("scaledFeatures")
  .setWithMean(true)
  .setWithStd(true)

val scalerModel = scaler.fit(trainData)
val scaledTrain = scalerModel.transform(trainData)
val scaledTest = scalerModel.transform(testData)

// 2. PCA avec sélection automatique
println(s"\n[Step 2] PCA with ${varianceTarget * 100}% variance target")
val pca = PCAFeatureExtractor.varianceBased(
  threshold = varianceTarget,
  inputCol = "scaledFeatures",
  outputCol = "pcaFeatures"
)

// 3. Fit et analyse
println("\n[Step 3] Fitting PCA model")
val (pcaModel, trainPCA, analysis) = pca.fitTransform(scaledTrain)

// 4. Affichage et sauvegarde
println("\n[Step 4] Variance Analysis")
pca.printVarianceReport(analysis)

pca.saveVarianceAnalysis(analysis, s"$metricsPath/pca_variance.csv")
pca.savePCAProjections(trainPCA, s"$metricsPath/pca_projections.csv",
                       labelCol = Some("label"))

// 5. Transformation test
println("\n[Step 5] Transforming test data")
val testPCA = pca.transform(pcaModel, scaledTest)

// 6. Génération visualisations (en Python)
println("\n[Step 6] Generate visualizations")
println(s"Run: python work/scripts/visualize_pca.py $metricsPath")

println("\n=== PCA Workflow Complete ===")
println(s"Components selected: ${analysis.numComponents}")
println(s"Variance explained: ${analysis.totalVarianceExplained * 100:.2f}%")
println(s"Dimensionality reduction: ${analysis.originalDimension} → ${analysis.numComponents}")
```

## Références

- **Spark MLlib PCA**: https://spark.apache.org/docs/latest/ml-features.html#pca
- **PCA Theory**: Jolliffe, I.T. (2002). Principal Component Analysis
- **Elbow Method**: Thorndike, R.L. (1953). "Who belongs in the family?"

## Notes Techniques

- **Complexité** : O(n × p²) où n = samples, p = features
- **Mémoire** : Matrice de covariance p × p doit tenir en mémoire driver
- **Recommandation** : Pour p > 10000, considérer des alternatives (Randomized PCA, Incremental PCA)
- **Spark** : PCA Spark utilise SVD (Singular Value Decomposition) sous le capot
- **Persistence** : Les modèles PCA sont sérialisables et peuvent être sauvegardés
