services:
  # ============================================================================
  # Spark Master
  # ============================================================================
  spark-master:
    image: bitnami/spark:3.5.3
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_DAEMON_MEMORY=2g
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ../work/notebooks:/notebooks
      - ../work/data:/data
      - ../work/apps:/apps
      - ../work/output:/output
    networks:
      - spark-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/8080' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  # ============================================================================
  # Spark Worker 1
  # ============================================================================
  spark-worker-1:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-1
    hostname: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=6G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_DAEMON_MEMORY=2g
      - SPARK_LOCAL_DIRS=/tmp/spark-local
    ports:
      - "8081:8081"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../work/data:/data
      - ../work/apps:/apps
      - ../work/output:/output
      - ../work/tmp/spark-tmp-1:/tmp/spark-local
    networks:
      - spark-network
  # ============================================================================
  # Spark Worker 2
  # ============================================================================
  spark-worker-2:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-2
    hostname: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=6G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_DAEMON_MEMORY=2g
      - SPARK_LOCAL_DIRS=/tmp/spark-local
    ports:
      - "8082:8082"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../work/data:/data
      - ../work/apps:/apps
      - ../work/output:/output
      - ../work/tmp/spark-tmp-2:/tmp/spark-local
    networks:
      - spark-network
  # ============================================================================
  # Spark Worker 3
  # ============================================================================
  spark-worker-3:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-3
    hostname: spark-worker-3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=6G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8083
      - SPARK_DAEMON_MEMORY=2g
      - SPARK_LOCAL_DIRS=/tmp/spark-local
    ports:
      - "8083:8083"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../work/data:/data
      - ../work/apps:/apps
      - ../work/output:/output
      - ../work/tmp/spark-tmp-3:/tmp/spark-local
    networks:
      - spark-network
  # ============================================================================
  # Spark Worker 4
  # ============================================================================
  spark-worker-4:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-4
    hostname: spark-worker-4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=6G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8084
      - SPARK_DAEMON_MEMORY=2g
      - SPARK_LOCAL_DIRS=/tmp/spark-local
    ports:
      - "8084:8084"
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ../work/data:/data
      - ../work/apps:/apps
      - ../work/output:/output
      - ../work/tmp/spark-tmp-4:/tmp/spark-local
    networks:
      - spark-network
  # ============================================================================
  # Spark Submit
  # ============================================================================
  spark-submit:
    image: bitnami/spark:3.5.3
    container_name: spark-submit
    hostname: spark-submit
    environment:
      - SPARK_MODE=submit
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HOME=/opt/bitnami/spark
      - JAVA_HOME=/opt/bitnami/java
    volumes:
      - ../work/data:/data
      - ../work/apps:/apps
      - ../work/output:/output
      - ../work/scripts:/scripts
      - ../work/libs:/libs
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    # Maintien le container actif sans dÃ©marrer de service Spark
    command: tail -f /dev/null
    restart: unless-stopped
  # ============================================================================
  # Jupyter - Notebooks
  # ============================================================================
  jupyter:
    build:
      context: ..
      dockerfile: docker/Dockerfile.jupyter
    container_name: jupyter-spark
    hostname: jupyter-spark
    ports:
      - "8888:8888"
      - "4040-4050:4040-4050"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HOME=/usr/local/spark
      - JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-arm64
      - PYSPARK_DRIVER_PYTHON=jupyter
      - PYSPARK_DRIVER_PYTHON_OPTS=lab
      - SPARK_DRIVER_MEMORY=3g
      - SPARK_EXECUTOR_MEMORY=4g
      - SPARK_EXECUTOR_CORES=2
      - SPARK_DRIVER_MAXRESULTSIZE=2g
    volumes:
      - ../work:/home/jovyan/work
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    restart: unless-stopped

  # ============================================================================
  # MLFlow Tracking Server
  # ============================================================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.4.0
    container_name: mlflow-server
    ports:
      - "5555:5000"
    volumes:
      # SQLite database persistence
      - ../work/mlflow:/mlflow
      # Artifacts storage
      - ../work/mlflow/artifacts:/mlflow/artifacts
    environment:
      # Backend store (SQLite)
      - BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      # Artifact store (local filesystem)
      - ARTIFACT_ROOT=/mlflow/artifacts
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - spark-network

networks:
  spark-network:
    driver: bridge