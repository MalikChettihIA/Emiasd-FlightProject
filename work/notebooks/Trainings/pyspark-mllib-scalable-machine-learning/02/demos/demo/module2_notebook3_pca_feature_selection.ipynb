{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152f0a39-1dbd-4e54-96c3-c72689ac9f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 2 Notebook 3: PCA and Feature Selection\n",
    "\n",
    "**Objective:** Demonstrate PCA and Chi-Square feature selection on the scaled datasets, select the final feature set using Chi-Square (per design decision), and split the data into training/testing sets.\n",
    "\n",
    "**Inputs:**\n",
    "1. `ecommerce.classification_scaled_features` (from M2N2 with MinMaxScaler)\n",
    "2. `ecommerce.regression_scaled_features` (from M2N2 with MinMaxScaler)\n",
    "\n",
    "**Outputs:**\n",
    "1. `ecommerce.classification_train_features`\n",
    "2. `ecommerce.classification_test_features`\n",
    "3. `ecommerce.regression_train_features`\n",
    "4. `ecommerce.regression_test_features`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65eb38f0-82b7-4502-a346-b83672666f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup & Load Data\n",
    "\n",
    "Load the scaled feature tables from the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671d95ee-78f7-43b1-9211-7337f2bb5e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from ecommerce.classification_scaled_features and ecommerce.regression_scaled_features\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA, ChiSqSelector, VectorSlicer, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Cr√©er une SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MonApplication\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    classification_df = spark.read.parquet(f\"spark-warehouse/ecommerce.db/classification_scaled_features\")\n",
    "    regression_df = spark.read.parquet(f\"spark-warehouse/ecommerce.db/regression_scaled_features\")\n",
    "    print(f\"Successfully loaded data from {classification_input_table} and {regression_input_table}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tables: {e}\")\n",
    "    print(\"Please ensure Module 2 Notebook 2 was run successfully with MinMaxScaler.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6910c842-0b9d-4a1f-b4e3-7c83ac40bd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Classification Input Data:\n",
      "+-----------+----------+--------------------+-------------+\n",
      "|customer_id|product_id|            features|has_purchased|\n",
      "+-----------+----------+--------------------+-------------+\n",
      "|          2|       485|(43,[0,1,2,3,6,7,...|            0|\n",
      "|          4|       312|(43,[0,1,2,3,4,6,...|            0|\n",
      "|          4|       616|(43,[0,1,2,3,4,7,...|            1|\n",
      "|          5|       825|(43,[0,1,2,3,4,6,...|            1|\n",
      "|          6|       143|(43,[0,1,2,3,4,7,...|            0|\n",
      "+-----------+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample data for classification\n",
    "print(\"Sample Classification Input Data:\")\n",
    "classification_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "654a20eb-c2a2-4ad4-b1e0-45a31c3416e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. PCA Demonstration\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms features into a smaller set of principal components, capturing the most variance. While powerful, it can make features harder to interpret directly.\n",
    "\n",
    "We'll demonstrate applying PCA but won't use its results for our final model pipeline, sticking to ChiSqSelector for better interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf75ee6-219e-4ede-9e24-7aeacaba923e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting PCA with k=10...\n",
      "\n",
      "Data after PCA Transformation:\n",
      "+-----------+----------+--------------------+\n",
      "|customer_id|product_id|         pcaFeatures|\n",
      "+-----------+----------+--------------------+\n",
      "|          2|       485|[0.74976916095425...|\n",
      "|          4|       312|[0.73520409210458...|\n",
      "|          4|       616|[0.66944346981817...|\n",
      "|          5|       825|[-0.7729857783717...|\n",
      "|          6|       143|[0.67368060065962...|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Explained Variance per component: [0.12195263026849008, 0.11017128129711976, 0.07874457921957756, 0.06297266098746845, 0.060305930370820056, 0.04806744810650913, 0.03444931623220214, 0.03409640759744138, 0.033225320949748, 0.02940129278425248]\n",
      "Total Variance Explained by 10 components: 0.6134\n"
     ]
    }
   ],
   "source": [
    "# Configure PCA\n",
    "num_pca_components = 10 # Choose a small number of components for demonstration\n",
    "pca = PCA(\n",
    "    k=num_pca_components, \n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"pcaFeatures\"\n",
    ")\n",
    "\n",
    "# Fit PCA model to the classification data\n",
    "print(f\"\\nFitting PCA with k={num_pca_components}...\")\n",
    "pca_model = pca.fit(classification_df)\n",
    "\n",
    "# Transform the data \n",
    "pca_df = pca_model.transform(classification_df)\n",
    "print(\"\\nData after PCA Transformation:\")\n",
    "pca_df.select('customer_id', 'product_id', 'pcaFeatures').show(5)\n",
    "\n",
    "# Show explained variance\n",
    "explained_variance = pca_model.explainedVariance\n",
    "print(f\"Explained Variance per component: {list(explained_variance)}\") # Cast to list for cleaner print\n",
    "print(f\"Total Variance Explained by {num_pca_components} components: {sum(explained_variance):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc4e62e0-6378-4eef-8c75-55ff8b115fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, PCA reduces the number of features. \n",
    "\n",
    "The 'explained variance' tells us how much of the original data's spread is captured by each new principal component. We can see that our first 10 components together capture approximately 61% of the total variance from the original 43 features. This highlights PCA's ability to condense information, though often more components are analyzed in practice to capture a higher percentage (like 90%+).\n",
    "\n",
    "However, for our course, we prioritize interpretability and will proceed with Chi-Square Feature Selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "546d89c8-d95f-458c-af6b-784599179fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Feature Selection (Chi-Square)\n",
    "\n",
    "The Chi-Square test is designed for categorical features. We will first isolate the one-hot encoded (OHE) categorical parts of our feature vector using `VectorSlicer`, apply `ChiSqSelector` to only those features, and then combine the selected categorical features with the original scaled numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d017b03a-ad04-46e8-ade0-8a504d8205bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Feature Indices (0-based): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Categorical Feature Indices (0-based): [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]\n"
     ]
    }
   ],
   "source": [
    "# Indices based on M2N2 assembly order: 13 numerical + 3 gender + 4 membership + 3 device + 10 country + 10 category = 43 total\n",
    "numerical_indices = list(range(0, 13))\n",
    "categorical_indices = list(range(13, 43))\n",
    "\n",
    "print(f\"Numerical Feature Indices (0-based): {numerical_indices}\")\n",
    "print(f\"Categorical Feature Indices (0-based): {categorical_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b5c005-7fb4-4309-b4b1-ab09730a433b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after slicing OHE features:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      " |-- ohe_features: vector (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|        ohe_features|\n",
      "+--------------------+\n",
      "|(30,[1,4,14,25,27...|\n",
      "|(30,[1,9,13,18,27...|\n",
      "|(30,[1,9,13,23,28...|\n",
      "|(30,[0,3,13,17,28...|\n",
      "|(30,[1,6,13,17,28...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slice out the OHE categorical features\n",
    "ohe_slicer = VectorSlicer(inputCol=\"features\", outputCol=\"ohe_features\", indices=categorical_indices)\n",
    "classification_df_sliced_ohe = ohe_slicer.transform(classification_df)\n",
    "\n",
    "print(\"Schema after slicing OHE features:\")\n",
    "classification_df_sliced_ohe.printSchema()\n",
    "classification_df_sliced_ohe.select(\"ohe_features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61322326-34b7-4913-b7ce-95bf0dd9ebe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting ChiSqSelector on OHE features (fpr=0.05)...\n",
      "Schema after selecting OHE features:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      " |-- ohe_features: vector (nullable = true)\n",
      " |-- selected_ohe_features: vector (nullable = true)\n",
      "\n",
      "+---------------------+\n",
      "|selected_ohe_features|\n",
      "+---------------------+\n",
      "|      (14,[12],[1.0])|\n",
      "| (14,[1,5],[1.0,1.0])|\n",
      "| (14,[1,10],[1.0,1...|\n",
      "| (14,[1,4],[1.0,1.0])|\n",
      "| (14,[0,1,4],[1.0,...|\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure ChiSqSelector for the OHE features\n",
    "fpr_threshold = 0.05\n",
    "chisq_selector = ChiSqSelector(\n",
    "    selectorType=\"fpr\", \n",
    "    fpr=fpr_threshold, \n",
    "    featuresCol=\"ohe_features\", \n",
    "    outputCol=\"selected_ohe_features\", \n",
    "    labelCol=\"has_purchased\"\n",
    ")\n",
    "\n",
    "# Fit ChiSqSelector model\n",
    "print(f\"\\nFitting ChiSqSelector on OHE features (fpr={fpr_threshold})...\")\n",
    "chisq_model = chisq_selector.fit(classification_df_sliced_ohe)\n",
    "\n",
    "# Transform to get selected OHE features\n",
    "classification_selected_ohe = chisq_model.transform(classification_df_sliced_ohe)\n",
    "\n",
    "print(\"Schema after selecting OHE features:\")\n",
    "classification_selected_ohe.printSchema()\n",
    "classification_selected_ohe.select(\"selected_ohe_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6383e9-b6db-445e-b66a-791e801bfa29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after slicing numerical features:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      " |-- ohe_features: vector (nullable = true)\n",
      " |-- selected_ohe_features: vector (nullable = true)\n",
      " |-- numerical_features_scaled: vector (nullable = true)\n",
      "\n",
      "+-------------------------+\n",
      "|numerical_features_scaled|\n",
      "+-------------------------+\n",
      "|     (13,[0,1,2,3,6,7,...|\n",
      "|     (13,[0,1,2,3,4,6,...|\n",
      "|     (13,[0,1,2,3,4,7,...|\n",
      "|     (13,[0,1,2,3,4,6,...|\n",
      "|     (13,[0,1,2,3,4,7,...|\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slice out the scaled numerical features from the original vector\n",
    "num_slicer = VectorSlicer(inputCol=\"features\", outputCol=\"numerical_features_scaled\", indices=numerical_indices)\n",
    "# Apply to the df that now has selected_ohe_features\n",
    "classification_final_parts = num_slicer.transform(classification_selected_ohe)\n",
    "\n",
    "print(\"Schema after slicing numerical features:\")\n",
    "classification_final_parts.printSchema()\n",
    "classification_final_parts.select(\"numerical_features_scaled\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c24c6d1-0c21-422f-a78e-06b76861af77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after final assembly:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      " |-- ohe_features: vector (nullable = true)\n",
      " |-- selected_ohe_features: vector (nullable = true)\n",
      " |-- numerical_features_scaled: vector (nullable = true)\n",
      " |-- final_features: vector (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|      final_features|\n",
      "+--------------------+\n",
      "|(27,[0,1,2,3,6,7,...|\n",
      "|(27,[0,1,2,3,4,6,...|\n",
      "|(27,[0,1,2,3,4,7,...|\n",
      "|(27,[0,1,2,3,4,6,...|\n",
      "|(27,[0,1,2,3,4,7,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assemble the final feature vector: scaled numerical + selected OHE\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"numerical_features_scaled\", \"selected_ohe_features\"], \n",
    "    outputCol=\"final_features\"\n",
    ")\n",
    "classification_assembled = final_assembler.transform(classification_final_parts)\n",
    "\n",
    "print(\"Schema after final assembly:\")\n",
    "classification_assembled.printSchema()\n",
    "classification_assembled.select(\"final_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b817b94-b8f6-4674-a554-937142e62ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Prepare Final Datasets using Selected Features\n",
    "\n",
    "Select the final columns for the classification dataset and apply the same transformations to the regression dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09994a07-abaa-4a60-a90d-4d687cbd8711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classification Dataset Schema:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- has_purchased: integer (nullable = true)\n",
      "\n",
      "+-----------+----------+--------------------+-------------+\n",
      "|customer_id|product_id|            features|has_purchased|\n",
      "+-----------+----------+--------------------+-------------+\n",
      "|          2|       485|(27,[0,1,2,3,6,7,...|            0|\n",
      "|          4|       312|(27,[0,1,2,3,4,6,...|            0|\n",
      "|          4|       616|(27,[0,1,2,3,4,7,...|            1|\n",
      "|          5|       825|(27,[0,1,2,3,4,6,...|            1|\n",
      "|          6|       143|(27,[0,1,2,3,4,7,...|            0|\n",
      "+-----------+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select final columns for classification, renaming final_features -> features\n",
    "final_classification_df = classification_assembled.select(\n",
    "    \"customer_id\", \n",
    "    \"product_id\", \n",
    "    col(\"final_features\").alias(\"features\"), \n",
    "    \"has_purchased\"\n",
    ")\n",
    "\n",
    "print(\"Final Classification Dataset Schema:\")\n",
    "final_classification_df.printSchema()\n",
    "final_classification_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b96662-32ac-4d24-b1c3-cf7d7f5d027e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing regression data...\n",
      "Final Regression Dataset Schema:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- total_purchase_amount: double (nullable = true)\n",
      "\n",
      "+-----------+----------+--------------------+---------------------+\n",
      "|customer_id|product_id|            features|total_purchase_amount|\n",
      "+-----------+----------+--------------------+---------------------+\n",
      "|          4|       616|(27,[0,1,2,3,4,7,...|               281.39|\n",
      "|          5|       825|(27,[0,1,2,3,4,6,...|               232.21|\n",
      "|          7|       584|(27,[0,1,2,3,4,7,...|               656.01|\n",
      "|          7|       615|(27,[0,1,2,3,4,7,...|               205.55|\n",
      "|          9|       340|(27,[0,1,2,3,4,7,...|               496.87|\n",
      "+-----------+----------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the same transformations to the regression data\n",
    "print(\"\\nProcessing regression data...\")\n",
    "# 1. Slice OHE\n",
    "regression_df_sliced_ohe = ohe_slicer.transform(regression_df)\n",
    "# 2. Select OHE using the *fitted* chisq_model\n",
    "regression_selected_ohe = chisq_model.transform(regression_df_sliced_ohe)\n",
    "# 3. Slice Numerical\n",
    "regression_final_parts = num_slicer.transform(regression_selected_ohe)\n",
    "# 4. Assemble Final\n",
    "regression_assembled = final_assembler.transform(regression_final_parts)\n",
    "\n",
    "# Select final columns for regression\n",
    "final_regression_df = regression_assembled.select(\n",
    "    \"customer_id\", \n",
    "    \"product_id\", \n",
    "    col(\"final_features\").alias(\"features\"), \n",
    "    \"total_purchase_amount\"\n",
    ")\n",
    "\n",
    "print(\"Final Regression Dataset Schema:\")\n",
    "final_regression_df.printSchema()\n",
    "final_regression_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e36aa57-b197-4cd9-b235-b383e005b847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Split Data (Train/Test)\n",
    "\n",
    "Split both the final classification and regression datasets into training and testing sets for model evaluation in the next module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd21d497-01c0-4541-a266-f2ce5938c44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting classification data (80.0% train / 20.0% test)...\n",
      "Splitting regression data (80.0% train / 20.0% test)...\n"
     ]
    }
   ],
   "source": [
    "# Define split ratio and seed\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "seed = 42\n",
    "\n",
    "# Split classification data\n",
    "print(f\"\\nSplitting classification data ({train_ratio*100}% train / {test_ratio*100}% test)...\")\n",
    "class_train_df, class_test_df = final_classification_df.randomSplit([train_ratio, test_ratio], seed=seed)\n",
    "\n",
    "# Split regression data\n",
    "print(f\"Splitting regression data ({train_ratio*100}% train / {test_ratio*100}% test)...\")\n",
    "reg_train_df, reg_test_df = final_regression_df.randomSplit([train_ratio, test_ratio], seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a350f34f-e9d5-4f1e-8a05-1379f93719bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Counts after Splitting:\n",
      "Classification Train Count: 40174\n",
      "Classification Test Count:  9793\n",
      "Regression Train Count:   20286\n",
      "Regression Test Count:    4952\n"
     ]
    }
   ],
   "source": [
    "# Show counts of the splits\n",
    "print(\"\\nDataset Counts after Splitting:\")\n",
    "print(f\"Classification Train Count: {class_train_df.count()}\")\n",
    "print(f\"Classification Test Count:  {class_test_df.count()}\")\n",
    "print(f\"Regression Train Count:   {reg_train_df.count()}\")\n",
    "print(f\"Regression Test Count:    {reg_test_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971818f7-6088-45dc-a626-a07053ce75d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Save Outputs\n",
    "\n",
    "Save the training and testing datasets as Delta tables for use in Module 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'ecommerce' database\n"
     ]
    }
   ],
   "source": [
    "# Create database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommerce\")\n",
    "spark.sql(\"USE ecommerce\")\n",
    "\n",
    "\n",
    "print(\"Using 'ecommerce' database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ebe4f9-d792-4ea6-92d7-ef8d262945c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving split datasets as Delta tables...\n",
      "Successfully saved training and testing datasets.\n"
     ]
    }
   ],
   "source": [
    "# Define output table names\n",
    "class_train_table = \"ecommerce.classification_train_features\"\n",
    "class_test_table = \"ecommerce.classification_test_features\"\n",
    "reg_train_table = \"ecommerce.regression_train_features\"\n",
    "reg_test_table = \"ecommerce.regression_test_features\"\n",
    "\n",
    "# Save tables\n",
    "print(\"\\nSaving split datasets as Delta tables...\")\n",
    "class_train_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(f\"{class_train_table}\")\n",
    "class_test_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(f\"{class_test_table}\")\n",
    "reg_train_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(f\"{reg_train_table}\")\n",
    "reg_test_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(f\"{reg_test_table}\")\n",
    "\n",
    "print(\"Successfully saved training and testing datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e150289-6d94-41f4-a8bd-c6e5d443a386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Summary & Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded the scaled feature data from M2N2.\n",
    "2. Briefly demonstrated PCA for dimensionality reduction.\n",
    "3. Isolated the OHE categorical features using `VectorSlicer`.\n",
    "4. Applied Chi-Square feature selection (using `fpr`) *only* to the OHE features.\n",
    "5. Isolated the scaled numerical features using `VectorSlicer`.\n",
    "6. Recombined the selected OHE features and the scaled numerical features using `VectorAssembler`.\n",
    "7. Applied the same transformations to the regression dataset.\n",
    "8. Split both datasets into training and testing sets.\n",
    "9. Saved the final train/test datasets as Delta tables.\n",
    "\n",
    "**Next:** Module 3 will utilize these prepared datasets to train and evaluate machine learning models.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "module2_notebook3_pca_feature_selection",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
